# American Politics: Pre-Written Examples {#ap-pre}

This chapter provides a variety of shorter, themed responses to possible American Politics comprehensive exams. In the actual exams, you will likely be stitching together these shorter pieces as part of a larger effort. You can see an example of that [in a later chapter](#ap-real), where an actual exam and answers are fully replicated.


## Citizens as Political Beings

Questions about how citizens develop their basic political beliefs, perceive political issues, and participate in the political process are at the heart of the study of American political behavior. This paper reviews how scholars have attempted to address these questions. The paper begins with the first sociological approaches to political behavior, follows through to the theories of political cognition which gained supremacy following the theoretical and empirical failures of the sociological approach, and finally ends with a review of the Zaller’s (1992) model of political belief formation and Delli Carpini and Keeter’s (1996) examination of the stratification of political information in the American public. Following that review of the most established findings on political citizenship, I review recent lines of scholarship that have begun to call into question a long held belief in American behavioral research, that Americans are generally ‘ideologically innocent’ (Converse, 1964). Finally, while the essay mainly draws from the behavioral approach to American politics, in the closing section the connection from behavior and ideology to institutional scholarship is made clear with examples from presidential (Skowronek, 1997) and legislative (Lee, 2009) studies.

### Learning Politics: The Socialization Approach

Following the second world war, political scientists were attempting to understand why America and Great Britain were different than the Germany. How did the German people allow their country to be taken over politically by the Nazi party? The obvious answer for many was that there was something different about the American family, and that children were socialized by the family to value democracy. Political scientists, borrowing from theories of sociology, set out to try and test this theory. Fred Greenstein’s (1960) study "The Benevolent Leader: Children's Images of Political Authority," and Jennings and Niemi’s (1968) "Transmission of Political Values from Parent to Child," typify what is now known as the socialization approach to political behavior. One positive aspect to the socialization approach is that it provides very predictions - your political attitudes should follow how you are socialized - first by parents, and then by friends, so by the time you reach adulthood, we would expect that political grounding will determine your own views. 

The socialization of political behavior was the primary theoretical framework for much of the scholarly work in the 1960s, but soon ran into both empirical and theoretical problems. Empirically, data became much harder to generate. Whereas Greenstein (1960) was able to simply begin interviewing elementary school aged children, in the era of Institutional Review Boards, the ethical implications of that type of research render it fairly impossible. Moreover, it soon became clear the evidence did not support the theoretical predictions. Jennings and Niemi (1968) in particular exposed the gap between theory and empirics: children did not simply adopt their parent’s political views.  In addition to these empirical problems, the theory of socialization had a substantial challenge in that it expects that by adulthood our preferences (in this case political preferences) are fixed. This expectation was undermined however, with increasing evidence that people maintained a life-long openness, and socialization continued throughout adulthood, as individuals continued to try to fit-in with their work, neighborhood, and other social environments. 

By the 1970’s, theories of socialization were still a going concern. But by the 1980’s political scientists were looking back on the behavioralist approaches and finding them lacking. A new theoretical lens, rational choice, was beginning its dominance in political science (Aldrich, 1976; Downs, 1957). Downs (1957) sets out the basic rational choice model and uses it to try and understand voter turnout, arguing that because each individual vote has so little value, it is rational for voters to have little political knowledge, and to not seek out more. The theory of rational choice was soon exposed as having its own theoretical end-point problems (Green & Shapiro, 1996; Waldo, 2017) – namely the expectation that rational people will not vote, which faced considerable empirical challenges. Still, rational choice variants such as prospect theory (Kahneman & Tversky, 1984; Kahneman, 2003) continued to be a dominant theoretical lens for political scientists interested in political behavior (Hammond & Bonneau, 2009; Shepsle, 2006; Simon, 1990).

While socialization would never again gain favor as a pure theoretical approach to political behavior, it never completely disappeared either. Sears and Valentino (1997) use elements of socialization to examine a much narrower question then either Greenstein (1960) or Jennings and Niemi (1968). Sears and Valentino (1997) use the context of a presidential election and find that adolescents are socialized, but by exogenous events rather than the family. Empirically this makes sense, as generational shifts in political thinking can be seen in the aftermath of large exogenous events, such as the Great Depression impacting the life-long economic views toward thrift for those who lived through it, or how the terror of 9/11 shifted the political views on patriotism and war for that generation. 

Ronald Inglehart and Paul Abramson(1994) provide more evidence for this generational effect in political beliefs, using Inglehart’s theory of post-materialism (Inglehart, 1990; Inglehart & Norris, 2017). Using a vast amount of survey data from across Europe and the United States, the pair makes a grand generational argument that there is a cohort effect within generations that is affected by the environmental conditions the cohort experienced as children. In Inglehart’s theory, as a society becomes more prosperous, and thus less likely to be worried about base survival needs, their political views as adults will tend to be post-materialist. This theory is not really a socialization theory, but is useful because it shows how socialization information can be used profitably in the context of political science. Inglehart’s work also tends to undermine the purely rational choice approach to political behavior, as it suggests that individuals’ choices are determined by events outside of an immediate response to a given situation. Inglehart’s theory and evidence point to the importance of understanding political culture, insofar as it impacts political belief formation.   

### Political Cognition

Whereas political socialization asked how people formed their beliefs as they approached adulthood, the question of how American’s think about politics was still left unanswered. This became the central question motivating the scholars studying political cognition. In this field, the field of political science has been closely following the ideas best set out by Converse (1964). This theoretical line operates from a base assumption that voters have clear ideological positions (Downs, 1957), but update the earlier theories to better understand how voters make decisions on candidates and political issues. These theorists believed that for the vast majority of issues, voters can use ideology and partisan identification (Green, Palmquist, & Schickler, 2002) as a shortcut to determine their views fairly quickly even when (Lupia, 1994). Converse undermines that assumption, and studies how American’s use and conceive of ideology, finding that on the whole, citizens do not clearly grasp ideology, nor do they use it effectively to form preferences. Instead, Converse argues that while elites have something resembling a consistent ideology, most American’s are not consistent in ideology, a feature of the electorate he famously named an “ideological innocence.”

With Converse finding that most Americans don’t have ideological principles from which their political beliefs follow, where those beliefs originate? This is where other political scientists pick up the argument. Feldman (1988), for example, tests an alternate theory that in the absence of ideology, people use value systems to form beliefs. Again though, the empirical evidence did not support the value theory. 
Lodge and Hamill (1986) provide the model that in many ways supplanted the sociological tradition. They borrow heavily from theories in psychology (Kahneman & Tversky, 1984) and make the basic assumption that people are “cognitive misers” who operate in a very complex political environment with too much information to process fully, and so they want to make decisions as simply and easily as possible. Lodge and Hamill theorize that people use partisan schemas to handle most political decisions, with these schemas making complex political information easier to process, a form of heuristic thinking. The problem for these schemas becomes confirmation bias, as people tend then to dismiss information that does not fit easily with their already held notions, while simultaneously adopting information that confirms those same notions. A second outcome of this theory is the problem of heuristic processing, which occurs as people make errors and begin to fill in information that was never actually there. These problems leave the possibility of belief change somewhat unexplainable, short of massive exogenous shocks to the schema through overwhelming new information. 

Recent advances in measurement and a Bayesian view of belief updating suggest that the threats of backlash to non-conforming information is overblown, and that when confronted with politically persuasive messages, the receivers of that information update their views in the direction of the persuasion. In other words, “information designed to persuade can and does change minds” and those changes are positive, small, homogenous, and durable (Coppock, 2016, p. x).   However, partisan schema theories continue to be explored in the literature, and Lodge and Hamill’s contribution has been long lasting. Their work also shows the beginning of a shift in methodology in political science, as they use experimental research methods, as opposed to the pure survey research that had dominated the field to that point.
Arthur Lupia uses his (1994) "Shortcuts versus Encyclopedias: Information and Voting Behavior in California Insurance Reform Elections" to address some of the problems with partisan schema theories. Lupia demonstrates that low information voters don’t have to be perfectly informed information depositories (encyclopedias) in order to mimic the political decisions of their better-informed counterparts. Instead, the low-information voters use information shortcuts, such as third-party endorsements of political positions and candidates, to form their opinions. This allows them to mimic the votes of the better-informed voters. 

Notions of information processing and heuristics has become a well-established theory in American political behavior. It probably provides a more realistic reflection of how American’s actually behave and operate in a complex political environment. Some problems remain, however, including problems of bias confirmation. Taber and Lodge (2006) address this problem with their model of motivated skepticism that helps explain when and why citizens are biased-information processors. Using a Bayesian-inspired information processing frame, they find evidence that those citizens who are the most politically sophisticated and with the strongest levels of prior belief are most subject to errors induced by confirmation and disconfirmation bias. The authors key argument is that most people are simply unaware of the strength of their own prior beliefs, and that these priors dictate to a large degree how citizen’s process information. Bolsen, Druckman, and Cook (2014) add to this literature, by examining the motivation process in how people form their political opinions. Their finding is that people are indeed biased processors of information, particularly in the context of in-party and out-party endorsements. The authors predict their finding “will be troubling to people who worry that partisan motivated reasoning leads to lower quality opinions due to dogmatism and inflexibility.” 

### Understanding Mass Opinion

In some ways, John Zaller can be seen as modernizing and updating the The American Voter (Campbell, Converse, Miller, & Stokes, 1960). Zaller’s (1992) book The Nature and Origins of Mass Opinion stands a classic in the field, and is centrally concerned with examining how citizens use mass media information to form political preferences. Zaller gives a good frame for understanding mass political opinion, and on balance the evidence which followed him tends to support his theory. 

Zaller provides a theory that allows for competing considerations to be held by a person at any given time as they confront political choices. He shows that elites do provide information that voters use to construct their own opinions, but that this process is mediated. Voters have a political awareness which filters elite (here, mass media) information in terms of issue salience and consistency. In other words, argues Zaller, voters do not have any single, true, political preference. Rather, voters have multiple political considerations, which he structures in his “Receive-Accept-Sample” model of voter preference. Voters must first receive information, that is they must be made aware of it. Next, the voter must accept (or reject) that information based on its consistency with their prior beliefs. Finally, the voter samples from the most recent information they’ve been made aware of, with the information nearest in time given highest preference. 
Zaller uses his model to show that for the most part American’s use of elite cues in the form of political discourse, which they are exposed to by mass media, to form their own political opinions (Prior, 2013). Like Converse, Zaller disabuses political science of the belief that American’s possess consistent political ideology that forms the basis for their political beliefs. But Zaller permanently improved the model by, first, allowing for more sophisticated understanding of what elements the public does use to form political opinions; and second, by illustrating the primary cue for formation of political beliefs originates with exposure to elite discourse on political matters through the mass media. Finally, one of the lasting impacts of Zaller’s work has to raise fundamental questions about what political scientists are really measuring when they survey people’s attitudes (Prior, 2009). Given the importance of recency considerations for voter behavior (Panagopoulos, 2011), we should question the validity of opinion survey measures, as recent political events stand a good chance of having skewed respondent’s reported beliefs and opinions. Zaller’s model gives a good explanation for why people aren’t consistent in reported opinions over time. Political opinion formation is a dynamic process, and exposure and recency matter. Zaller’s book was impactful upon its release, and has continued to shape the study of American political behavior since. 

### Political Knowledge and Citizen Competence

Also following and updating the early Michigan studies (Campbell et al., 1960; Converse, 1964) is Michael Delli Carpini and Scott Keeter’s (1996) What Americans Know about Politics and Why it Matters. The primary question confronted in the work is whether Americans have enough factual information to be able to participate meaningfully in the American democracy. The authors take a view contrary to the political cognition literature, in that they make a strong normative argument that heuristics are not good enough, and that voters need a strong background in meaningful facts in order to properly participate in a democracy. 

The main point of the book is that the distribution of meaningful political information is uneven, particularly along dimensions of race and socio-economic status (SES). They find that if a voter is a middle-aged or older white male from the upper half of the SES distribution, that person stands a fairly good chance of having the appropriate basis of political information to participate politically. However, if the voter is, for example, a black woman living in the inner-city and from the lower SES distribution, the likelihood is that she does not possess enough relevant political information to be able to participate politically. 

Delli Carpini and Keeter make a strong argument that there are institutional hurdles that are skewing political participation. Their normative statement is that if we are to thrive in a democracy, we must begin to address the structural elements which produce unequal distributions of political information. They do an excellent job of documenting the process by which they came to their conclusions, and make a convincing argument for why we should be concerned about what people know. 

### Relating Changing American Behavior to Institutions

Recall that the most important early studies of American political behavior came out of the University of Michigan election studies beginning in 1956 (Campbell et al., 1960). Much of how we still conceive of political behavior is influenced by those initial studies. One of the main findings on public ideological alignment is that Americans generally do not understand the ideological differences that are important to political elites (Converse, 1964). The publics inattention is very high even in moments of great political conflict, their opinions are inconsistent, and do not cohere ideologically. On the whole, Converse concluded that Americans are ‘ideologically innocent.’
However, recent research suggests that the ideological innocence is transforming as more Americans become ideologically sorted, with opinion surveys finding that non-elites are more ideologically aligned and constrained (coherent). Baldassari and Gelman (2008) use National Election Study (NES) data from 1974 to 2004 and find that the correlation between issue attitudes and party identification had grown significantly. In other words, we learn a lot about a voter’s stance on policy issues such as abortion, gay marriage, and social welfare programs simply by knowing whether identified as a Democrat or a Republican. 

But was the finding by Baldassari and Gelman (2008) just a short-term trend? Recent evidence suggests a longer-term trend is in play, as shown by Martin Wattenberg (2019) in his conference paper at the 2019 American Political Science Association’s national conference. Wattenberg finds that the positive correlation trend has only increased in the decade since Baldassari and Gelman investigated it. The percentage of Americans (Wattenberg, 2019, p. 1)“with well-developed belief systems based on a clear understanding of public policy choices has increased substantially” even since 2000, and this increase “accounts for virtually all of the increase in respondents whose partisanship matches their ideology.” Wattenberg (p. 8) traces the start of this increasing ideological coherence to the presidential campaigns of Ronald Reagan, whose presidency was the first (at least in the American National Election Survey era) to “promote a clear agenda that represented a major shift in the course of public policy” issues such as tax and social welfare reductions alongside increased military funding. 

Two other research trends support Wattenberg’s belief that we are in something in a weird moment in American political behavior, though both supports come from the American political institutionalist camp in presidential and senatorial studies. First, is presidential scholarship that shows presidential ‘paradigms’ tend to dictate presidential leadership and success, rather than innate political skill (Skowronek, 1997, 2008). In Skowronek’s telling, we are still in the Reagan era because presidential candidates still hew closely to the policy issue patterns developed by Reagan. For example, both of our most recent presidents overtly compared themselves to Reagan during their campaigns. For example, Senator Obama in 2008 made the comparison himself during the campaign (Murray, 2008, para. 3): 
"I don't want to present myself as some sort of singular figure...I think Ronald Reagan changed the trajectory of America in a way that Richard Nixon did not and in a way that Bill Clinton did not. He put us on a fundamentally different path because the country was ready for it."

As a Democrat candidate still engaged in a primary fight, Obama was not campaigning for conservative values. But Obama was a political actor in a still resilient political environment shaped by Reagan, and so while he opposed the political order, he was not able to cast aside the vision of leadership offered by Reagan. The Reagan paradigm is what Skowrownek (2008) would call “institutionally thick,” and sets the terms of engagement for candidates from both parties, and Wattenberg is likely right to frame the increasingly ideological public as beginning with President Reagan.

Further evidence for the findings of Wattenberg (2019) and Baldassari and Gelman (2008) comes from Frances Lee (2009, 2016) in her studies of elite polarization among politicians. Lee notes that we are living in a period of intense electoral competition that skews what is normally expected in terms of ideological polarization. Most of American political history was dominated by one party or the other, and only in the modern era is the partisan competition so balanced that the party in power must be constantly attending to the next election. Tied closely the electoral connection (Mayhew, 1974) but leveraged at the institutional level, Lee sees ideological polarization at the elite level is really partisan competition for power, rather than policy positions. This helps explain why issues can flip between parties in a relatively short period of time, such as Democrats moving from dismissing candidate Mitt Romney for his hawkish views on Russia in 2012 (Oppel, 2012) to embracing the view that Russia presents a credible threat to American democracy. On the same issue, traditionally hawkish, anti-Russian Republicans have seemed to lack urgency (Senate Democrats, 2019) to confront Russian threats (Sanger & Edmondson, 2019). The issue realignment at the institutional level is a rational response for politicians who increasingly operate in a nationalized political environment. 

### Affective Ideology

To return to political behavior, those institutional demands must be met by a coinciding increase in ideological coherence in the populace, a trend that appears supported by the evidence reviewed earlier (Baldassarri & Gelman, 2008; Wattenberg, 2019). Which came first is unclear, but a least one line of research indicates that the issue and partisan polarization is more akin to a social identity than mere policy differences. In Partisan Hearts and Minds (Green et al., 2002, p. 13) partisanship is defined as a type of social identification: a “psychological process of self-categorization and group evaluation.” People identify as a Democrat or a Republican in the same way they identify as belonging to a religious denomination or ethnic group. Instead of the ‘warmth’ indicators used by many partisanship researchers in surveys, the authors argue that “people ask themselves two questions: What kinds of social groups come to mind as I think about Democrats, Republicans, and Independents? What assemblage of groups (if any) best describe me?” (Green et al., 2002, p. 8). This a rejection of the rational choice version of partisanship which dominated the academic view of partisanship at the time(Green & Shapiro, 1996).

How a person answers this question to themselves produces a very stable partisanship. The authors see partisanship as a relatively non-dynamic phenomenon. That is, rather than elections producing a great deal of party choice change among voters, most people already have a partisan identification (at least to themselves) and that identification is rather unlikely to change in the short-term span of an election cycle. Elections are more of a cause for cheerleading one’s own team, and less a competition between two (or more) choices of individual politicians: “Elections are also forums for intergroup competition. Individuals who identify with these groups are drawn into this competition. Their interest and level of emotional engagement increase as they embrace the team as their own. Although not irresistible, the desire to see one’s team prevail powerfully influences the probability of casting a vote for the candidate of one’s party” (Green et al., 2002, p. 202).

This willingness to see past rivaling individual politicians and engage with politics as social identification is why partisanship matters – it matters because it affects electoral politics. The authors seek to provide empirical support for their theory not just in American politics through the case of the 2000 presidential election, but in comparative international contexts as well, with evidence from the United Kingdom, Canada, and Germany.  This is an important claim from the authors, who position their theory of partisanship not as an American phenomenon, but a human one. The increasing salience of partisanship as a heuristic device for deciding policy stands goes beyond the political. While partisanship is traditionally seen in issue-based terms, there is evidence that partisanship has affective impacts as well (Iyengar, Lelkes, Levendusky, Malhotra, & Westwood, 2019, p. 129): “Ordinary Americans increasingly dislike and distrust those from the other party” and this animus is leading “Democrats and Republicans both say that the other party's members are hypocritical, selfish, and closed-minded, and they are unwilling to socialize across party lines.”

Partisan Hearts and Minds makes a clear methodological contribution as well, despite containing relatively sparse statistical analysis than other texts in the genre. Because of their underlying critique that party identification has been poorly measured in much of the partisanship research, the authors present and defend a way of accounting for measurement error (p. 231-234). Once models of partisanship allow for measurement error, the authors argue, party identification is revealed as a very stable pattern over multiple decades.
While in this work they use mean-corrected panel analysis, this is just one example of how to correct for measurement error. Psychologists have long recognized that measurement error interferes with causal inference (Nesselroade & Baltes, 1979). Green, Palmquist, and Schickler (2002) make good use of this insight to address pooled measurement error, which still remains relatively unaddressed in much of political science research. The next steps needed can already be seen in modern techniques and rapid development still taking place in developmental psychology (Deboeck, Nicholson, Kouros, Little, & Garber, 2015), which uses derivatives of both inter- and intra-measurement error. Political science has not yet developed even the tools to collect the tools needed to build datasets capable of being tested in this way. Once (if) they are, however, further insights into the stability of partisanship over time, and its vulnerability to political events including elections, could be gleaned. As it is, the authors make a good case for including measurement error in partisanship studies, and more broadly political science.

### Conclusion

The evidence available suggests that the democratic theorists have been mistaken to assume that the average voter is in possession of highly structured, sophisticated ideological frameworks with which to navigate their duties as a citizen. Instead, voters live in a highly complex political world, and tend to only pay attention when highly relevant, current information which has direct impact on their world is provided. Zaller (1992) and Delli Carpini and Keeter (1996)continue to provide relevant theoretical lenses with which to examine the American voter and how they construct a political worldview. Modernization has vastly increased the amount of information available but it is unclear whether that information is salient enough for the average voter in order to constitute valuable information. Given the evidence that most voters continue to operate as politically naïve, if not outright ignorant, one must begin to question the normative assumption that a well-informed public is necessary for a democracy to survive. 

However, early evidence is emerging that political knowledge is increasing, at least among some issue domains and partisan axes. While some scholars worry that the American voters’ lack of basic political information threatens democracy, other have held that people do not necessarily need to be in complete command of political knowledge in order to operate effectively as a voter (Shannon, McGee, & Jones, 2019) because they use informational shortcuts (Lupia, 1994). Perhaps, given the centuries of relative democratic stability in the American context, it is time to give serious consideration to the idea that in fact the democratic experiment does not require a sophisticated electorate. While a democracy may be improved by one, there seems to be enough evidence at this point to at least conclude that the necessity of politically sophisticated and knowledgeable publics is not a prima facie requirement.

## Recent Classics in American Politics

### Introduction

Does wealth inequality matter to running a democracy (Bartels, 2010)? What is the ‘correct’ vote for a voter to make - and do they make it even in the face of poor information and even disinformation (Lau & Redlawsk, 2006)? What comes next in the battle for American voter minds in a socially and technologically networked world that is increasingly vulnerable to artificial intelligence(Radford, Wu, Clark, et al., 2019)? Disinformation spreads through networks - how should we understand how the social networks of Americans affect how they vote ((Sinclair, 2012)? Finally, why should we be concerned about what American voters know in the first place (Delli Carpini & Keeter, 1996)?

In this essay, I review four political science books that are likely to be considered future classics in the field, and shape how I think about the questions facing political science broadly. In addition, I will identify a technology article covering advances textual prediction via machine learning, which will profoundly affect American politics soon and will be a classic turning point studies by American politics scholars for many years in the future.

### Bartels: Unequal Democracy

A long line of American political science scholarship is concerned with the effect of economic equality on democratic outcomes. Schattschneider (1960) famously noted that claims that the pluralist foundations of the US constitution protected against certain well-off groups having outsized political effects were not convincing: “The flaw in the pluralist heaven is that the heavenly chorus sings with a strong upper-class accent." Since then, many scholars have found wealth and business interests have a biasing effect in political outcomes (Gilens, 2012; Gilens & Page, 2014; Hall & Wayman, 1990; Schlozman, 1984). 

In Bartels Unequal Democracy (2010), the central puzzle is: Why does income inequality in the US continue to expand, even some eighty years after “an expanding welfare state” (p. 31) sought to close the gap? Bartels answer lies in connecting the realms of politics and economic inequality, two areas which he contends only “perfunctory attention” (p. 35) has been paid. Economic equality is important because it “poses a crucial challenge to America’s democratic ideals,” (p. 32) and Bartels roots this belief in great political thinkers from two thousand years of political writings, from Aristotle (Lintott, 1992) to De Tocqueville (1969) to Krugman (1995).

On the whole, given his starting position that economic inequality matters, Bartels is successful in showing how politics and policy drive increases in economic policies. He does so with clear theoretical propositions in the first half of the book and then turns to specific case studies of tax cuts, the estate tax, minimum wage legislation, and the political economy during the Great Recession. He concludes the book with a great deal of pessimism – inequality undermines democracy, and there is little hope for different outcomes. Even when presidential elections seem to provide political energy for policy changes to reduce inequality (such as the 2008 election of President Obama), the outcomes of the continued growth of inequality show that partisans of either major party have little to celebrate about.
In chapter three, Bartels disposes of the class argument that social policy preferences trump economic ones for voters – the poorest Americans are no more conservative on social issues then they were in the pre-war period. Bartels finds instead that American voters are economically myopic, and that Republican presidential candidates have been able, through luck or skill, to preside over unusually good cyclical economic peaks during election years. Here, Bartels is unable to offer a compelling causal argument, though only because is limited to a correlative argument, as with only 16 post-war elections inferring causal links is quite difficult. Even so, the patterns he uncovers are fascinating and useful.

Bartels is careful in selecting case studies that uniformly support his argument, and he investigates the Bush tax cuts (chapter 5), the Estate Tax Repeal (chapter 6), the minimum wage policy (chapter 7), and finally the Great Recession (chapter 9).  Here, neither party fares well in Bartel’s estimation, with the bulk of economic policy benefits accruing to the wealthy, while the middle class is awarded in paltry ways, and the poor are left worse off – all while politicians and parties escape political consequence from the poor. The book closes with Bartels examining how his findings impact normative democratic theory (we should be worried) and how we might close the widening gulf between rich and poor (we shouldn’t be optimistic it can be done). However, a more optimistic reader, before giving up, might examine the lengthy, complex argument Bartels is relying on for such pessimistic consequences, before giving up.
Although I pick at Unequal Democracy a bit more in the next section, Bartels provides a critical contribution to understanding modern American political behavior. Wealth and income inequality continue to be a motivating force in American politics (Boix, 2010), and the issue is particularly salient in two frontrunners in the Democrat primary, Elizabeth Warren and Bernie Sanders. 
Alternate views of this problem are available. A recent review concludes that highly disparate levels of wealth and democracy are compatible (Scheve & Stasavage, 2017) because 1) democracies cleave along important dimensions other than wealth, 2) some voters who are on the losing side of wealth inequality care more about those other cleavages, and 3) the wealthiest citizens may capture important critical parts of the democratic process, thus preventing policies that promote wealth redistribution. 

To Scheve and Stasavage’s second point, wealth distribution is not clearly connected to voting outcomes. Determining how people vote is perhaps the deepest literature in American political science, and a methodological turn towards experimental research in this area since the early 2000’s has continued to build the findings. Of recent works, I find work by Lau & Redlawsk (2006) compelling, as they investigate and undermine academic concepts of ‘correct’ and ‘incorrect’ voting. 

### Lau & Redlawsk: How Voters Decide

Taking on Bartels critically is not easy; he takes on a huge subject and does so well. As Page (2009, p. 148) notes, “anyone who quibbles with his interpretations or suggests he has left important questions unanswered is likely to appear ungenerous, even churlish.”  With that risk in mind, there is some reason to quibble with Bartel's general conception of voter decision making, as compared to the relatively more sophisticated treatment used by Richard Lau and David Redlawsk (2006) in their book *How Voters Decide: Information Processing in Election Campaigns*. 

Lau and Redlawsk (2006) use a unique method to test the idea of ‘incorrect voting’ that is at the heart of Bartel's research question. Voters are economically myopic in Bartel's view, and so have voted incorrectly and against their own economic interests, which would in the aggregate be with Democratic presidents. Lau and Redlawsk use a clever experimental design to test for incorrect voting. They expose their experimental subjects to scrolling information about hypothetical election candidates, allowing the subjects to click on the information they want to know more about. This is done under a time-constrained environment, and at the end of the timed exposure subjects are asked to vote for one of the candidates. In the next phase, the same subjects are then allowed to delve into all the candidate information they want, without time constraint, and given the chance to vote again. ‘Correct voting’ in this context is when a respondent chooses to vote for the same candidate in both phases, while ‘incorrect voting’ is when a study subject switches their vote from phase one to phase two. Lau and Redlawsk find that approximately 30% of the subjects cast incorrect votes.

Applying the incorrect voting findings of Lau and Redlawsk to Bartels findings that voters are myopic reveals a consistent theme through Bartel’s work, one which Page (2009, p. 148) characterizes as “blaming the victims” and the “chief defect of Bartel’s book.” This is not to say that Bartels sets out with any pejorative attempt to devalue those on the wrong side of the Gini curve – quite the opposite. But Bartel’s conception of what ‘correct’ voting entails is rooted in a classic rational choice approach, which places perhaps impossible expectations on voters to process incredibly complex economic information in full, consider outcomes for competing policy alternatives, and then vote ‘correctly.’ In reality, as shown by Law and Redlawsk, voters are more boundedly rational – they do not have complete information, nor the time to process it if they did. Bartels takes 363 pages, decades of academic expertise, and sophisticated methodology to come to the conclusions he does, and then asks the reader to wonder why people vote against their own economic interests as he sees them. What is amazing is that voters are ever able to make correct decisions given the volume of economic policy information – and disinformation – they are required to consider. As Lau and Redlawsk show, individuals must use psychological decision-making strategies, and those strategies are what allows for a substantial portion of the public to vote correctly in the first place.

### Speaking of Disinformation: The GPT-2 Natural Language Model

A consistent theme in American political scholarship is that Americans are not all that knowledgable about politics(Campbell, Converse, Miller, & Stokes, 1960; Delli Carpini & Keeter, 1996; Lodge & Hamill, 1986; Lodge & Taber, 2013; Zaller, 1991). However, little if any of this scholarship has accounted for the modern ability for actors to effectively deploy intentional disinformation. The 2016 presidential election was famously interfered with by Russian actors (Permanent Select Committee on Intelligence, 2019), and the intervening three years has seen an explosion in machine learning work that is likely to play a part in future elections. Perhaps the most important of these developments has been the ability to quickly generate fake news stories that are indistinguishable from the real thing. At the forefront of these developments is the “GPT-2” text prediction algorithm.

Simultaneously released as a blog post on the OpenAI site (Radford, Wu, Clark, et al., 2019), the scholarly article (Radford, Wu, Child, et al., 2019) announcing the development of the ‘GPT-2’ natural language prediction model will be seen as a turning point in political speech. The GPT-2 model, and the paper announcing it, are bound to be turning points in politics, broadly drawn, and more specifically the political and policy implications of artificial intelligence methods. The authors directly address the political and policy implications of their own technology, stating that along with synthetic audio and so-called ‘deep fake’ videos, “technologies are reducing the cost of generating fake content and waging disinformation campaigns…politicians may want to consider introducing penalties for the misuse of such systems.” 

The authors decided to address their concerns about the potential misuse of such systems by publicly releasing truncated models in a staged strategy. Though the full model has 1.5B (billion) parameters, the authors decided to first release only the 114M (million) parameter model in February 2019; a 344M model in May 2019; and a 774M model in late August 2019. 
Reviewing the technical accomplishments of the GPT-2 language transformer is beyond the scope of this essay. However, it should suffice to note that the algorithm surpasses previous attempts in language prediction models, and in some cases approaches human-level capabilities. The model was trained on over 8 million English-language webpages, with the goal of simply predicting the next word in a sentence. A simple goal, but the output of the algorithm is convincing enough that 83% found it indistinguishable from New York Times articles (Kreps & McCain, 2019).

Scare tactics around technological innovation is nothing new, and political scientists are already primed to consider the effects of big data and artificial intelligence on political outcomes (Brady, 2019). Easy enough to claim an algorithm is dangerous to politics - what does it look like in practice and why would it be dangerous to an informed electorate? Consider the following ‘fake news’ I generated for the purpose of this essay using the GPT-2 774M model. Bolded text is the input I used as a seed, and what the algorithm generated follows in normal text:

Today, three University of Utah professors, Phillip Singer, Juliet Carlisle, and Matthew Burbank, published groundbreaking work supporting the case that Donald Trump is the most successful president in American history. In that study, the authors argue, “Trump's presidency has been highly successful because he has been unusually willing to challenge the norms of traditional American political and economic institutions, including media and traditional power bases, and make big decisions.”

The New York Times quoted the authors' conclusion, “Mr. Trump's victory was an electoral shock delivered to an establishment that had been too comfortable to challenge him or take a stand against a movement that was increasingly winning national elections.”

The article continues, “Yet the shock and anger that Mr. Trump's surprise victory has unleashed on Americans is a harbinger of the challenges to which he was bound — if, of course, he could survive them. For a populist leader who pledged to put a stop to big government and hold America 'on a golden platter,' the election of Mr. Trump has provided a challenge to institutions, big business, and popular norms in ways that they have never been challenged before.”

A bit too cutesy? Consider the following as a perhaps more likely type of misinformation we might see in the 2020 election campaign. Again, I generated this in under ten seconds, and only for the purpose of this essay (bolded is me, unbolded is algorithmically generated):

Presidential candidate Elizabeth Warren stumbled in an interview Tuesday morning, raising concerns about her ability to govern. Speaking with MSNBC's Andrea Mitchell Tuesday morning, Warren seemed to contradict herself, arguing she was not looking for a job, but rather a chance to build a "dream team" of people who can help transform Washington with her.

"What I am looking for is a team, not a job," Warren said. "I'm looking to build a dream team. To build a team that can get things done in an administration that has to be run with the knowledge and understanding that all of us who are members of Congress and federal officials, there is a reason we called it a team. It's not because I'm looking for a job. There isn't an opportunity for me to get a job, and there's no reason this administration should be a team. I'm looking for a team."

That statement seemed to go over some heads on Twitter, with many questioning whether Warren was even qualified to be the President at all. At one point she claimed "there are no openings" in the White House staff.


To be clear, I am not arguing that the GPT-2 paper is significant because it is the final word in this type of technology. In fact, the ability to generate large amounts of misinformation that is for all intents and purposes indistinguishable from authentic news will only become easier. Importantly, the two examples above were generated with a version of GPT-2 that is only approximately half as powerful as the full 1.5B parameter model. This is not the end of fake news and disinformation for political purposes, but the beginning. The American electorate (or at least some substantial portion of it) was misled by foreign powers in the 2016 presidential election by the intentional use of memes (Permanent Select Committee on Intelligence, 2019). How will they react to conceivable disinformation generated like the above? 

Despite the intent of the GPT-2 team to stage releases of the model so that the unintended effects could be judged, such a strategy inherently relies on the belief that either; 1)  no one else can do the same work, or 2) those who are capable of re-creating the technology would also restrain themselves. Those premises are both doubtful, as shown by the full release of “OpenGPT-2” by two graduate students at Brown University (Gokaslan & Cohen, 2019) in a post titled “OpenGPT-2: We Replicated GPT-2 Because You Can Too” the same week the OpenAI authors released the watered-down model. Moreover, in their release of the 774M model, the OpenAI authors note that they have spoken to five groups that had already replicated GPT-2. These are only the groups willing to publicly admit they are developing the capability - what of the Internet Research Agency (IRA), a front for Russian intelligence agencies intent on destabilizing American elections (Permanent Select Committee on Intelligence, 2019)? With an ever more networked citizenry comes increased risk that those networks can be turned against democratic ideals.

### Betsy Sinclair: The Social Citizen

In her book The Social Citizen: Peer Networks and Political Behavior, author Betsy Sinclair (2012) sets out to theoretically and empirically expand how we understand political behavior in the context of the influence of social networks. Sinclair begins her prologue with a truism - “Humans are inherently social” – and yet our understanding of political behavior often ignores this outright, preferring to focus on the individual-level characteristics that determine political opinion and choice. However, even with decades of research into how race, income, education, and a host of other demographic factors, political scientists still have yet to lay out a satisfying explanation of political choice, belief, and behavior. 

Sinclair sets out to rectify this absence with a close examination of how social networks influence four political contexts – voter turnout, donating to political causes, vote choice, and political party identification. Each of the contexts is given focus in a single chapter, with sophisticated research designs to test each. To a great degree, Sinclair is successful in showing that social networks and connectedness influence political behavior in important ways. Social network effects must be taken into account by scholars attempting to construct a holistic theoretical view of political behavior.

In chapter two, Sinclair turns to a question which has been the subject of interest to political scientists for decades – what causes people to turn out to vote in elections? Given the nature of Sinclair’s interests, she tests how social networks can influence voter turnout. She does so with two field experiments, the first in Chicago and the second in South Los Angeles. Sinclair’s Chicago experiment is closely modeled on Gerber, Green, and Larimer’s (2008) famous field experiment in which they used post-card mailers to show the positive effects of social surveillance on voter turnout. Sinclair takes that basic idea to experimentally test three treatment populations – individuals receiving the treatment, individuals socially proximate to the treatment, and individuals socially isolated from the treatment. Sinclair’s design allows for the random treatment of both individuals and neighborhoods, generating estimates of direct effects on the individuals receiving the mailers, as well as those who are indirectly affected.

Whereas the original study by Gerber, Green, and Larimer (2008) was not able to control for spillover effects, Sinclair’s Chicago study was intended to measure those effects, which theoretically reflect the social network that potential voters reside within. Like the Gerber, Green, and Larimer (2008) study, Sinclair uses varying degrees of social surveillance on individuals, and replicates the earlier study’s finding that turnout increases a few percentage points. Where Sinclair builds on the original study, however, is she also varies the level of social surveillance intensity by neighborhood, allowing her to test the hypothesis that the neighborhood social network will have effects on turnout on other members of that neighborhood, even if they didn’t receive a mailer. While the findings overall for this part of the study did not reach statistical significance, the effect was in the hypothesized direction. 

At the household level, Sinclair found that in households where there are already frequent voters, other members of that household are more likely to vote as well, to a statistically significant degree. Even the within-household finding lacks a clear causal argument, as it might be argued that there are endogenous factors which would lead people to reside together which would also influence their likelihood to vote – they may have similar political interests which activated in a particular year which increased the likelihood that the typical non-voter would activate as a voter that election cycle. While Sinclair is more optimistic that the spillover effects are suggestive of a social network effect, more long-term work is needed to replicate and extend the findings of this particular claim.

The second turnout-related field experiment Sinclair reports on takes place in South Los Angeles, and the findings here are more robust compared to those in Chicago. Here, Sinclair studies political canvassing in low-income neighborhoods and hypothesizes that canvassers from within the community will be more effective in increasing turnout than canvassers from outside the area. Sinclair randomizes which neighborhoods receive canvassing from either local or non-local canvassers, and finds that indeed the local canvassers were successful in increasing the turnout more than their non-local counterparts to a statistically significant degree. This finding supports the book’s larger argument, that the social pressure exerted by those known to us – in this case canvassers from our own neighborhood – is more effective than social pressure exerted by those more socially distant.

#### Political Donations

Sinclair returns to Chicago and uses chapter three to examine the effects of social networks on political donations by individuals. She first reviews the literature in this area, pointing out that existing single-method research simply hasn’t been able to tease out the effects of descriptive (what people do) and injunctive (what people ought to do) social norm appeals for donations, or the effects that social networks would have (which generally appear as “spillover effects” in other research, but Sinclair is specifically interested in measuring). Given that difficulty, Sinclair turns to a mixed-methods approach, first using aggregate survey data and then following up with in-depth interviews with both donors to explore the effect of social networks on donations. The study first sent out a mail-based survey to 1000 respondents asking about their donation activity. Sinclair uses data from that survey to classify respondents by levels of donations and their “centrality” to the donation social network, which is a co-donation measure of how many organizations a respondent donated to that others also donated to. Those respondents who were major donors – those with high centrality – were then interviewed about their donation activity.

The Chicago study confirms that there is a small, positive, but statistically significant social network effect on an individual’s decisions to donate. Interviews with the major donor confirmed the findings in the aggregate data, that their decision to donate to specific organizations was often strongly affected by their social network, as friends solicited donations to organizations they had already donated to for example.

Sinclair leaves room for future researchers to confirm her findings outside the densely populated districts she investigated in this research. The findings here concentrated on the major donors in a highly urbanized environment, but it is not clear how those findings would apply in either a more suburban, or rural political environment, nor how non-major donors (or even those who report never donating) are affected by their social network. This is an increasingly important area of study, as the national presidential campaigns have reported increasingly rapid growth in small donations. This trend has begun, at least anecdotally, to spread “down” into smaller legislative races, as the national parties have been successful in raising the visibility of House of Representative candidates and raising money across traditional political subdivisions. Further replication and extension of Sinclair’s findings could help provide the social network explanations, should they exist, for why small or first-time donors choose to give to political campaigns.
Candidate Choice

In chapters four Sinclair addresses candidate choice in voting using national election survey data. She concentrates on the surprising phenomenon of individuals who identify ideologically as either a conservative or a liberal, but vote for the candidate of the party not associated with that ideology. In the 2000 ANES data, for instance (Sinclair, 2012, p. 78), “11 percent of self-identified liberals voted for the Republican presidential candidate, and 29 percent of self-identified conservatives voted for the Democratic presidential candidate.” Sinclair suspects that social networks play a role in driving these seemingly irrational candidate choices. She does so in two studies, the first using simple logit regressions to show that where these cross-voters live, that is their social context, matters. A great deal of the conservatives voting for Democrats, for instance, live in Republican-dominated areas, and vice-versa. Interestingly, Sinclair uses the example of a conservative respondent moving from St. George, Utah, one of the most conservative areas in the U.S., to Santa Monica, California, one of the most liberal. This simple, yet robust analysis highlights the social context effects on voting that Sinclair is interested in – where a voter lives can influence voting even in the face of ideological conviction.

Sinclair’s second candidate choice study uses national election survey data collected by a private opinion firm. The study attempts to locate a respondent’s “discussion group” and in similar way to how Sinclair used measures of “centrality” in the earlier donation chapter, she is able to demonstrate that the closer a member of the social network is to the respondent, the greater chance of that person swaying the respondent’s candidate choice. Both this finding and the geographic location study finding are not completely surprising, they have the feel of rigorous methodology highlighting what most of us would take for granted. Our closest friends are more likely to influence our political behavior than are more socially distant acquaintances, and our social network is likely to be comprised by those geographically close to us. These truisms are easy to state; however, they have proven difficult to show empirically, and Sinclair accomplishes much with these two studies.

#### Party Identification

Chapter five is titled “Peer-Pressured Party Identification: The Elephant in the Room,” and deals with how party identification is influenced by one’s social network.  In very similar findings to chapter four, Sinclair finds that those closest to us socially are more likely to influence party identification than those more distant. However, party identification has higher salience than candidate choice, and is thought to persist more strongly over time, as candidates change often but party identification rarely does. Sinclair’s most interesting finding related to party identification is that a person’s social network, to the extent that those closest to a respondent were of differing party affiliation, was less likely to cause a change in party identification than it was to cause the respondent to remain publicly silent about party identification. In other words, a Republican who is surrounded socially by Democrats, particularly if those closest are Democrats, the Republican is more likely to become reluctant or unwilling to share their own party identification, but not change it. This finding was the same for Democrats whose social network was primarily comprised of Republicans.

#### Sinclair Conclusion

Sinclair effectively takes on very big questions about the effects of social networks on political behavior. In four broad behavioral contexts – turnout, political donations, candidate choice, and party identification – she uses varied methodological tools to demonstrate the effects of social networks. Her research design choices in each are appropriate, and she is forthright regarding the relatively small magnitude of effect that is discernible, at least given the methods available.

Citizens are social beings, and there remains much to explore for researchers interested in this area of political behavior. The strength of this book is not in exploring the depth that social ties have on political behavior, but on the breadth of effects. By focusing horizontally across the sub-discipline of political behavior, Sinclair’s work should force other researchers to at least consider the social network effects that might be present in their own models and hypotheses. 

### Delli Carpini & Keeter: What Americans Know

Quite separate from the question of how we learn political information is the question of what Americans know about politics. In this area, normative democracy theorists will not find much peace of mind. Some of the original research in this area from Converse (1962) showed that Americans frequently have little political information with which to base their partisan identification, and lack coherence when asked open-ended political questions (Converse, 1964). 

In What Americans Know about Politics and Why it Matters, Michael X. Delli Carpini and Scott Keeter (1996) follow Converse, carefully and thoroughly undermining the idea that the average American knows much at all about politics. The crux of the book is that the distribution of political knowledge is very uneven. Americans from higher socio-economic backgrounds, older Americans, white and male Americans, in part or in sum, all have a fairly good basis of political information. However, if Americans who are young, from the inner city, black, or female,  tend to fare worse in tests of political knowledge. These differences matter not only for knowledge, but for rates of participation, and even the effectiveness of that participation. 

One area where the authors have been critiqued is that they do not distinguish what types of knowledge might be more useful for different demographic groups. For instance, while a white, middle-aged male might have his political needs served by certain information (the type the authors test for), a young black woman may have very different political information needs. Delli Carpini and Keeter do an excellent job documenting their research process, and the methods used to come to their conclusions. While their argument stretches in some areas, overall they make a good case for why we should be concerned about what Americans know about politics. 

### Conclusion

As demonstrated in the GPT-2 section of this essay, the tools to deceive are only getting more sophisticated, while there is little evidence that Americans are any more capable of remaining skeptical, especially when the fake news is intentionally aimed at taking advantage of the cognitive biases that define what it means to be human (Kahneman, 2011).

Do Americans have enough factual information to be able to participate meaningfully in their American democracy? The findings of Delli Carpini and Keeter (1996) along with others would argue that Americans do not have the political knowledge to participate fully. Because political information is distributed unevenly across demographic groups, groups with less access to political information participate less, and when they do participate, they are less likely to attain the political results they seek. This is reminiscent of the findings of Larry Bartels (2010), who shows that while lower-income Americans tend to vote at similar rates as higher-income Americans, their vote ‘counts’ for less because it is given less preference by politicians. The finding that socioeconomic status influences the impact of political participation is found across modes of participation, as lower-income Americans were less likely to become political activists, and even when they did they used different (although sometimes more effective) communication methods than did activists with more financial resources (Verba, Schlozman, Brady, & Nie, 1993). 

However, some scholars would disagree with the stark findings in the works I have highlighted here. Lupia (1994) argues that voters don’t need to be fully informed ‘encyclopedias’ in order to vote, and that less-informed voters use heuristic shortcuts (Tversky & Kahneman, 1974) to vote in ways very similar to their better-informed counterparts. Lupia uses empirical evidence from a California initiative on insurance reform, a topic that most typical voters will have very little information about. Low-information voters participate meaningfully by relying on cues from third-parties such as advocacy groups and political parties to form their political opinion. That information is then translated into voting that is not dissimilar to those who spend much more time delving deeply into political issues. However, this kind of “partisan motivated reasoning” has been found by other scholars to reduce the quality of political opinions (Bolsen, Druckman, & Cook, 2014), and to be more shaped by the power of prior belief than the accuracy of new information (Lodge & Taber, 2013; Taber & Lodge, 2006).

Americans must make political decisions in a complex, fluid environment, and much of the scholarship that finds lackluster participation and lower political efficacy is predicated on a somewhat elitist rational choice belief that “correct” participation or “incorrect” voting exists in the first place. Lau and Redlawsk (2006) undermine this assumption, as they use experimental methods to test four models of how voters process information and make voting decisions. They have subjects make voting decisions on hypothetical candidates in a time-pressured environment, and then in the second phase allow those same respondents to collect information for as long as they need before making a voting decision. Subjects who change their vote between the first and second phases are considered to have made an “incorrect vote” in the first phase, while those whose vote is consistent between phases made “correct votes.” They find that in up to three-quarters of the time, their subjects were able to make “correct votes” despite not knowing enough. 

The methods used by Lau and Redlawsk are subject to critiques that their experiments are artificial environments, and so don’t necessarily tell us exactly how voters make decisions in real elections. However, the value of these experimental findings is that they closely mimic the frantic, bounded rationality (Simon, 1972) and heuristic shortcuts (Kahneman, 2003; Lodge & Hamill, 1986; Taber & Lodge, 2006) that real Americans in the real world must contend with when making political decisions. In many ways, Lau and Redlawsk’s findings are supportive of Lupia (1994) and others who reject the belief that democracy can only function properly when Americans are participating with rational, well-informed political knowledge. 

Americans are remarkable political creatures. They form political knowledge along socioeconomic cleavages, vote at low rates and participate in declining numbers in other participation modes, possess little accurate political information, and their political participation tends to have little impact. Yet, their political behavior is redeemed when the context of that political behavior is more fully considered. It is a privilege to have the time, resources, and desire to fully pursue political information, and that privilege is extended to relatively few in American society. 

Still, despite the privations of political knowledge, expertise, and participation, most Americans manage to ‘get it right’ most of the time – they operate in political spheres remarkably well in the face of factors which in a more rational approach should convince them to not participate at all. That is not to attempt to dispose of the normative democratic ideal, but perhaps to soften the approach of its most ardent believers. We should still heed the warnings of scholars such as Bartels (2009), who rightly worry that growing economic inequality threatens democracy. While not yet a three-hundred-year proven success, the American democratic experiment has so far succeeded despite the flaws of its citizens’ political knowledge. Short of some unreachable ideal lies a political persistence in the American citizen which manages to know enough, participate enough, and succeed politically enough. 

## Congress and the Presidency: The Electoral Connection

> In *Federalist 51*, James Madison observes that the electoral connection is the public’s “primary control on government.”  Thinking broadly about political science scholarship on American political institutions, and focusing on at least two national governmental institutions (e.g., Congress, the Presidency, the bureaucracy, the courts), answer the following:  Does the reelection incentive induce our national governmental institutions, and the actors within them, to be responsive to the public and public interests?  Does the electoral connection create systematic biases in governmental action?  Please provide a critical look at the scholarship underlying your answers.

Or a similar question:

> In *Federalist 51*, James Madison observes that the electoral connection is the “primary control on government.”  Does the reelection incentive induce members of Congress to respond to constituents’ policy preferences?  Does the electoral connection create systematic biases in congressional policymaking that undermine Congress’s capacities as a legislative institution?

### Introduction

The founding of the United States was consumed with debate and concern regarding the construction of a constitutional system that would protect citizens of the new nation from the tyranny of another monarch. James Madison (Hamilton, Madison, & Jay, 2008, p. 257) wrote in Federalist 51 that the primary question in forming a representative democracy was to “first enable the government to control the governed; and in the next place oblige it to control itself.” Resolving this contradiction, he argued, was best done by ensuring governmental power was ultimately stored in the people, through elections. “A dependence on the people is, no doubt, the primary control on the government,” Madison writes (p. 257), “but experience has taught mankind the necessity of auxiliary precautions.

While few contemporary observers worry the country is on the verge of a resurgent monarchy, in over 241 years since its founding concerns about how the citizenry can best control its government have not abated.  In this essay, I will argue that the electoral connection still provides the best incentive for our elected officials to respond to constituents' policy preferences, with particular focus on two political institutions - the US Congress and the presidency. Systemic biases have existed and will continue to persist (Hall, 1987; Lee, 2015), which slows down governmental representativeness and responsiveness, but empirical evidence of these biases overcoming the urge for reelection among elected officials has not been convincingly found. Further, the biases are able to be adequately addressed in the long run through the electoral connection and collective action of opposing interests. While some other forces impede the electoral connection, they do not overwhelm it, and it is the system itself that allows for self-correction for the biases and forces which keep it from operating perfectly.

### Reelection and Mayhew

Do Members of Congress care about reelection? Do they respond to constituent policy preferences? David Mayhew (1974) and Richard Fenno (R. F. Fenno, 1978) are both preeminent scholars of Congress, and simultaneously they both decided to use new rational choice thinking to see what they could learn from Congress. Mayhew’s conception of Members of Congress as single-minded seekers of re-election is spelled out in his book Congress: The Electoral Connection (1974), which has remained a resilient theory of Congress. When written, rational choice theory was just starting to bubble up from the pure economics approaches into political ones.

One way of reading Mayhew is that he is not actually claiming Members of Congress are motivated only by reelection, but to put forth that assumption and see what behaviors would be expected if it were true. He finds that given that assumption, we would expect Members of Congress to engage primarily in three activities: Advertising, credit claiming, and position-taking. These three activities correspond with what we see from Congress members, and so, Mayhew argues, we can accept the assumption that they are single-minded seekers of reelection. 

Mayhew’s theory is convincing and has remained a predominant explanation for understanding why Members act the way they do. In fact, Members of Congress spend a tremendous amount of time claiming credit for policies they had little to do with, advertising their personal biographies, and taking positions on issues. It has proved difficult if not impossible to argue that legislators are not intimately and intentionally connected to the voters they represent.

However, Mayhew’s theory leaves some empirics unexamined and unanswered. The three most critical critiques leveled at Mayhew are: 1) If congressional members are only motivated by re-election how does Congress address common resource problems? 2) How to explain members serving on congressional committees? 3) Do all constituencies matter equally to a Congress member?

### Responses to Mayhew

Not every congressional committee assignment has particularized benefits for reelection - internal power, good policy, ability to actually get things done. There are institutional arrangements that just aren’t explained by “reelection” only - why did it take until like 1993 for the Post Office Committee to get disbanded, as it was useless for 80 years in terms of reelection?

Fenno (1973) (1973) offers a slight correction to address these gaps in Mayhew’s theory in his article “Congressmen in Committee.” Fenno argues committee assignments are not well explained by immediate direct reelection benefits, so there must be other goals. Prestige, power, leadership goals, and future pursuit of higher political office may also matter. Goals are malleable, not static, and Congress members engage in committee work despite relatively little connection between some committees and reelection.

Richard Hall (1987) also tests Mayhew's theory that Congress members are single-minded seekers of election and provides evidence the members are motivated by other goals. Hall tests his hypothesis that Members participation in committees is influenced by motivational factors (goals) such as district service, making good policy, making a personal ‘mark’, and supporting the President’s agenda. Hall’s evidence suggests there are participation biases earlier in the legislative process that have a larger influence than a standard preference-based model like Mayhew’s would presume. 

Fenno responds more fully to Mayhew in 1978 with Homestyle: House Members in Their Districts. Fenno answers a question left unexamined by Mayhew: which constituencies matter? Fenno agrees that Members of Congress are heavily incentivized by reelection, but in a far less deterministic way than that put forth by Mayhew. Members of Congress have different constituents, and thus different goals. Fenno names the different ‘circles’ of Members’ constituencies the geographic, primary, and personal constituencies. How the Members define and understand these different constituencies shapes their activities in seeking their primary goal of reelection.

In The Logic of Congressional Action, Douglas Arnold (1990) takes on the puzzle created by Mayhew: If congress members are only motivated by re-election, then how is it that Congress takes action on common goods and resource problems? Common good problems are at the heart of many modern problems (Ostrom, 2015), and addressing common good problems is at the heart of both politics and political science (Mansbridge, 2014). Arnold takes up this challenge not by directly critiquing Mayhew, but by beginning with the same assumptions and attempting to answer the puzzle created by it. 

Arnold’s answer is that Members of Congress are motivated to solve some common pool resource problems, namely those that allow them to claim diffuse benefits without having connections to concentrated costs, a theory of policy that has become central to understanding many policy conflicts (Ostrom, 2011). Congress members choose different strategies of procedure to allow for decision which impose concentrated costs, like the closing of a military base, to be made by committees outside their control. Citizens have varying levels of preferences, from informed to potential, and Members of Congress attempt to estimate these preferences in order to determine which procedure strategies to select for common good problems. 

Many observers and citizens tend to blame ideological differences between the parties for a Congress that can at least appear from the outside to be in constant gridlock, and that there is increasing ‘polarization’ between the parties that inhibit the electoral connection. In other words, legislators are more captured by partisan interests than the interests of their constituencies. Measures of ideology are a common correlate in studies of Congress, including the NOMINATE, D-NOMINATE, and DW-NOMINATE scores developed by Poole and Rosenthal (2001) to measure the role of ideology on roll-call votes. However, some congressional scholars have cast significant doubt that ideology plays a role, including Frances Lee (2010) in her book Beyond Ideology: Politics, principles, and partisanship in the US Senate. Lee operationalizes party conflict as two separate measures: true ideological differences (substantive), and those differences which are related to Senators wanting their own “team” to win (procedural). Lee shows that in fact it is the non-ideological procedural votes that tend to split the Senate, as these votes are more likely to be used to protect a party’s agenda and help build complex agreements between the parties. Lee builds a convincing case that ideology and polarization are not the root of conflict and therefore does not undermine responsiveness or representativeness of the Senate. While she does not address the House of Representatives directly, it seems likely the same would be found there. There are “harsh constitutional realities” (Curry & Lee, 2017, p. 5) which limit party government, and the best evidence is that bipartisan lawmaking persists.

### Systemic Bias in the Electoral Connection

A significant challenge to Mayhew’s Electoral Connection are arguments put forth in Theodore Lowi’s (1979) The End of Liberalism: The Second Republic of the United States. Congress has abdicated its responsibilities to interest groups and a growing administrative state, Lowi argues, advancing the country from the “First Republic” to the “Second Republic.” In the Second Republic, the people no longer control their government, and the electoral connection has been replaced the bureaucracy, interest groups, and congressional committees. Differing conceptions of this anti-electoral relationship have been the “iron triangle,” and “agency capture,” and is a modern example of the factional dangers warned against by Madison in Federalist 10 (1787). Congress no longer controls the administrative state, and so the people no longer have a realistic electoral connection to those agencies, which have incredibly powerful effects on the daily lives of citizens. The fundamental bias Lowi argues for is that Congress has essentially auctioned off oversight and the typical American does not have the resources to make themselves heard in the process.

Modern scholars continue to agree with the broad outlines of Lowi’s work, and find ways to test for evidence of a fundamentally biased system, as seen in an influential piece by Gilens and Page (2014), who find evidence for economic elite domination and biased pluralism theory. However, despite the influential article, the data and methodology underlying the piece render it unconvincing. They define ‘economic elites’ as those households earning $140,000/year, which is difficult to imagine as the ‘elites’ dominating politics, particularly as a special interest group. To their credit, the authors acknowledge the diminished explanatory power of their argument particularly in relation to economic elites.

One of the most sustained and convincing arguments against Lowi’s conclusions is provided by McCubbins and March (1984), who differentiate between “police patrol” and “fire alarm” oversight. Those who worry Congress has given over its oversight responsibilities, like Lowi, are looking for evidence of police patrol oversight – proactive, roving oversight that detects problems and curtails them before they grow out of control. However, as McCubbins and March argue, Congress has a rational interest in selecting fire alarm oversight, which is a less centralized and direct approach. Congress sets the rules and procedures to govern agencies and then allows for citizens and interest groups to set off ‘alarms’ when the agency is not operating correctly within those rules. Congress members are rationally motivated to select for fire alarm oversight, which allows them to claim the credit (Mayhew, 1974) when problems are detected. Members of Congress do not get credit for simple maintenance checks on agencies (a form of police patrol) when everything is fine and nothing needs substantial change. 

#### Pluralism, Parties, and Presidents

Factionalism is commonly cited as negatively affecting the electoral connection. This idea is as old as the American system itself.  Madison believed that in a nation with conflicted and competing interests, no one faction would attain political dominance. In Federalist 10 (Madison, 1787, p. 232) faction is defined as "a number of citizens, whether amounting to a minority or majority of the whole, who are united and actuated by some common impulse of passion, or of interest, adverse to the rights of other citizens, or to the permanent and aggregate interests of the community." In Madison’s view, the ‘mischiefs of faction’ would be a risk to the representativeness and responsiveness of the newly formed republic, and so the solution was to either remove the causes of faction or structure the government such that the damaging effects of factionalism could be controlled. Unfortunately, Madison believed, it was impossible to remove the causes of faction, as “liberty is to faction as air is to fire” (p. 232). That option no longer available, the Constitution structured a system of checks and balances to ensure the executive, legislative, and judicial branches would remain in balance. Each branch would remain independent of the other, and contain levers of power which would allow it to counteract the other.  

Pluralist theory argues that the political balance is a common good for the American electorate, as it allows for interest groups, which represent the group interests of citizens, to compete for influence within government. This belief in pluralism holds its place as the dominant theory in American politics for a very long time.  Truman (1971) contends that this competition of interests in pluralism is the balancing wheel of the American governmental structure. Where too great an imbalance occurs, small interest groups will be created to represent the interests of those on the threatened side. These interest groups grow in power, prestige, and usefulness, and eventually become part of the governmental policy machinery. Interest groups are therefore integral to the functioning of the American government, even surpassing the direct role of the electoral connection.

Dahl (1961) is perhaps the most important early political scientist on the pluralist side. The pluralist view is that systemic bias does occur, but in a limited form. Further, group conflict driven bias (as in elite-dominated issues) is mediated by the government. Competing groups means that no one voice can dominate, and the conflict also ensures that policy change is usually incremental. Dahl found that power was fairly widely dispersed rather than concentrated in a single actor in his study of New Haven, Connecticut. Urban governing was a coalition-building exercise, and anyone willing to work hard enough at it could achieve influence in politics. Dahl’s work motivated a lot of academic interest in studying local politics.

Even after the rise of theories that convincingly put the lie to normative pluralism (Schattschneider, 1960), artifacts of pluralism on the American political conscious could still be found. Neustadt (1990, p. 11) uses pluralism as a vehicle for his argument that presidents must bargain both externally and internally in order to wield the relatively weak power of the office: “Presidential power is the power to persuade.” Presidents bargain internally with competing factions of his own executive branch, and those heads of federal executive agencies, cabinet secretaries, and bureaucrats all have competing interests. Because power is decentralized in the American constitutional system, a president is required to marshall his resources around persuading the other branches to achieve his goals. These resources include his position, reputation, and prestige. 

Arguments against the pluralism of Truman and Neustadt are plentiful and convincing. Ultimately, Truman’s reliance on the ability of small interest groups to overcome systematic bias fails to answer Olson (Olson, 1965). While small groups may form in response to a perceived need for policy change, as the group grows in size the collective interest is no longer able to overcome the free-rider problem. Kernell (2007) argues that Neustadt’s theory was only correct for his time, and that later presidents shifted to a more confrontational tactic he calls ‘going public.’ Where Neustadt believed presidents rely on a Washington DC-based reputation, Kernell shows that modern presidents rely to a much greater extent on their reputation with the American public. Kernell’s theory seems more in line with modern presidential action, such as President Trump’s daily use of Twitter to get his message to the American public. 

### Parties and Ideology

Perhaps Mayhew’s greatest weakness is his disregard for political parties. Though he eventually softened his original argument that parties simply do not matter (Mayhew, 2005), the power of parties in the American political system is hard to deny, and has been the focus on many critiques on Mayhew’s electoral connection work. 

E.E. Schattschneider led the committee that produced the report (American Political Science Association, 1950) “Toward a More Responsible Two-Party System: A Report of the Committee on Political Parties: Summary of Conclusions and Proposals.” While this report ends up being more of a normative statement about what parties should be rather than a theory of what they are, it still serves as an accurate overview of Schattschneider’s enduring belief that parties matter. But Schattschneider’s better theoretical contribution is his (1960) The Semi-Sovereign People, which includes his criminally over-looked  quote: “The flaw in the pluralist heaven is that the heavenly chorus sings with a strong upper-class accent.” Parties are the antidote to overly powerful interest groups that fail to represent the poorer classes, according to Schattschneider, because parties allow for a widening of the conflict (Valdimer Orlando Key, 1963). “All conflict has about it some elements that go into the making of a riot,” writes Schattschneider, “Nothing attracts a crowd as quickly as a fight. Nothing is so contagious.” His main insight is that parties allow for ‘fights’ to expand their audience, effectively allowing for citizens from lower classes to elevate their fights to the national stage through parties.  

Mancur Olson (1965) goes further and more convincingly than Schattschneider to how small groups of isolated people with common problems to overcome can effectively challenge systemic bias in the political system. Olson’s primary insight is that individual rationality is not the same as group rationality. Where Schattschneider is still using a reductionist and behavioralist approach, making comparison s between national parties and street riots, as if the group and individual levels are the same, Olson proceeds carefully through examples of unions, cartels, and firms (all of which can be thought of as interest groups) and how they form rationally according to their interests. 
The collective action problems that spring from this solution create solutions to systemic bias.  In small groups, it is easier to identify free-riders, while large groups have more difficulty doing so. This creates a natural bulwark against the persistence of overly powerful interest groups, as smaller groups grow and larger groups decline. Coercion and incentives are variously used by all types of these groups to attempt to solidify their ability to hold together and consolidate power, such as mandatory union membership in “closed shops” and offering non-collective benefits to members. 

Olson’s treatment is sometimes coarse and limited by the time period he is writing about, but his central insight remains valid. Small isolated pockets of individuals are motivated by common cause, resources, and costs to strategically come together to promote group interests, which allows for systemic biases to be overcome.

###$ Key Differences

V.O. Key makes critical contributions to understanding how parties can skew the electoral connection is two books: Southern Politics in State and Nation (1949) and American State Politics: An Introduction (1963). Though the latter work is less famous, in it Key makes the important point that state and national politics are connected. Sectionalism in national politics has an affect on state politics. Key finds that  “national tides spill into state politics” as cleavages driven by national party concerns are projected into state elections. This phenomenon can be important, as expanding the scope of conflict made the civil rights movement possible only as Democrats came under national party pressure, but it can also relieve local officials of performance pressure (and thus diminish the electoral connection) who are able to ride on the national tide of their party. In this way, parties become the solvent of federalism.

Key’s Southern Politics makes several contributions for the purposes of this essay. First, it is important to understanding how political elites bias the electoral connection by substituting other issues (race in his example) for economic issues, thus creating cross-cutting pressures that inhibit cohesive group formation among the disadvantaged. This insight has held up over time with evidence that voters who feel that cross-pressure are less likely to participate politically (Mutz, 2002). 	

Though Key’s contributions are much deeper, for the present purpose the second insight springing from Southern Politics is that not all groups are equal. Pluralist theory holds that the intense political conflict and competition ends up balancing interests in the polity. However, the cases of partisan competition are relatively limited in the South. In most states, one-party domination by Democrats was the rule. More than any other, Arkansas was the example of a pure one-party state, though the same general pattern could be found in most of the South. Extreme political conformity meant that no consistent factions arose, as political hopefuls were basing success on friends and neighbors in the areas they lived. This reified the power of black belt whites, who in turn were motivated to rule through non-redistributive policy. After all, if winning elections means keeping your (white) neighbors happy, and politicians are at least strongly motivated to win re-election (Mayhew, 1974), then making sure those neighbors are well taken care of by the party machine is a rational strategy.

Key’s study of the South generated broader implications for political study of states and parties. His genius was in deriving more general political hypotheses from close study of Southern state cases. The primary insight is that the closer a state comes to pure one-party rule, the more multi-factionalism will result within those parties. In states with more (though not equal) partisan competition, both parties will become more cohesive. This begs the question: What is the difference between party and faction?

Key sees the differences as a continuum, from the most extreme one-party rule in Arkansas and South Carolina to states where the minority Republicans came close to establishing two-party rule, most identifiably in North Carolina. Key identifies at least five differential effects – things parties do that factions do not. First and primarily, when there is a unified and persistent minority party, the majority party rules more responsibly, redistributes benefits to compete for votes among blacks and minority party members, and is more disciplined. Fractured factional leaders do not do this. Second, parties can organize voters, and sustain those coalitions of voters around shared interests, whereas factions are too locally focused on merging interests with other outside factions. Thus, factional politicians end up having to rebuild any coalition around a new identity each election cycle, without continuity of effort or dedication. The third effect is that states with more than single-party rule, the minority party can sustain their opposition, whereas factional opposition cannot. Fourth, the sustained nature of parties means that leaders are vetted and experienced over time, whereas in factions, charisma becomes more critical as they attempt to build coalitions each cycle quickly. Fifth, corruption, nepotism, and political favoritism are more endemic to factional rule in one-party states because factional politicians must use “whatever means are available” (Key, 1949, p. 305) as they attempt to bribe friends and neighbors to stay loyal. In a cohesive party, leaders are still likely to rewards friends with a place in the unified party government, but again this is done in a more distributed manner. Because there is a unified opposition party with at least some governmental levers to pull, the risk of discovery of criminal behavior in the majority party serves to keep corruption relatively lower.  The differences between parties and factions show how complex results can spring from the relatively simple difference that parties have internal cohesion and discipline, while factions do not.

With some limited exceptions, the South was a political body comprised of factions, to the detriment of the electoral connection. Democrat rule was the rule. Whereas Key generally expects national politics to seep into local politics, if not dominate them, the South remained politically isolated from the nation. As he shows in later work (Valdimer Orlando Key, 1963), undoing this isolation was eventually what allowed for the civil rights movement to begin undermining Democrat single-party rule. Prior to that, however, the South’s atomized factionalism meant the governments were generally unable to “provide the political leadership necessary to cope well with the governmental problems of the South” (Key, 1949, p. 310). The South was primarily organized politically around the interests of black belt whites, who were primarily interested in preserving their power at the expense of the blacks who far outnumbered them. This was a political division that did not hold at the federal level and reinforced the political isolation of the South, which remained insular and overly focused on localism and racist traditions.

### Conclusion

Despite a laudable attempt by Lowi (1979), no single scholar or even collection of critiques is able to undermine Mayhew’s (1974) basic insight that our elected officials care a great deal about reelection, which reinforces the electoral connection of the American people to its government. Mayhew conceptions of legislator ambition, autonomy, responsiveness, and accountability serve to keep the electoral connection strong.  While Mayhew originally held a fairly narrow scope of focus on post-World War II congressional behavior, recent work shows that the electoral connections stretches back further in time (Carson & Jenkins, 2011). Mayhewian conceptions of ambition were present in the Civil War era (Kernell, 1977), as were examples of Members of Congress ‘advertising’ their accomplishments to constituents (Cooper & Young, 1989). 

It appears the electoral connection is in no current danger of being undermined, as politicians and parties continue to lobby furiously for votes, and the average tenure of legislators continues to grow over time(Glassman & Wilhelm, 2017). Fenno (1978) sharpens Mayhew’s argument by providing a detailed rendition of how exactly members of Congress shape their goals through defining their constituencies in ever-expanding circles of relationship to the member. 

While complaining of the political domination of economic elites and party ideology has proven a time-tested populist appeal, there is only mixed empirical evidence to show it is true, and those studies which do make the claim (Gilens & Page, 2014) end up failing to conceptualize and define the problem sufficiently to be convincing. Of course, ‘imbalance’ in the power of specific interest groups exist, and at times there have been interest groups which have an oversized influence on policy. However, through Olson’s The Logic of Collective Action, we can understand how systemic bias is addressed through the collective action decisions of small groups counteracting the larger groups. Where the pluralist dream of a perfectly balanced wheel of government has clearly been shown inaccurate, envisioning the system as a spinning gyroscope is perhaps more useful. It tilts from time to time and moves along its own wobbly path, but in the end, stays upright as long as there is enough energy in the system to continue.


## Foundations of Congressional Action

> In some ways, this is just a short piece using Krehbiel (1998) *Pivotal Politics: A Theory of U.S. Lawmaking*. But it could also serve as the foundation for critiquing (or employing) rational choice theory.

### Introduction

In Pivotal Politics: A Theory of U.S. Lawmaking, Keith Krehbiel (1998) constructs a general theory of lawmaking. Krehbiel uses a rational choice approach to construct a game model, which takes into account both presidential and legislative players. While Krehbiel is satisfied with the his “surprisingly simple” (p. 1) and “unabashedly elementary” theory, in chasing elegance he commits errors in constructing the assumptions of his model, rendering the model’s explanatory power highly suspect. This paper will proceed through three sections. First, I will describe the basics of Krehbiel’s theory so as to orient the reader to the next two sections, which offer critiques of Krehbiel’s theory centered around errors in construction of his general model of lawmaking (p. 35).  Krehbiel asks too much of the reader in asking them to assume away crucial realities of the legislative world as he builds game sequence and behavioral assumptions into the model. In the end, Krehbiel’s offering is useful as a thought exercise, but does little to convince me that his model explains much about U.S. lawmaking.

### Krehbiel: Pivotal Politics

What causes congressional action versus congressional deadlock? For Krehbiel, the answer is parties don’t matter to how members of the US House of Representatives vote, only Member preference can provide answers. To test the theory, Krehbiel constructs a sophisticated game model, in which he defines the pivotal players. Included in his pivot model are the median member of the House, the President, the Senate filibuster pivot, and the veto override pivot. The model places these pivot players along a single axis, and attempts to construct ideal models of legislative action dependent on their relative position to one another, as well as to a point he defines as the ideological nature of the bill in question.

Krehbiel’s model remains influential, though ends up rather unconvincing due to a host of problems, only some of which will be covered here, and which are selected in the context of what Lee’s explanations (2009, 2016) explain better. Overall, Krehbiel’s constructed game space does not accurately reflect the real-world context he intends to examine, and so the outcomes of that model should not be assumed to be applicable to the real-world context either. While all models simplified realities, Krehbiel makes model assumptions that he hopes are simplistic, but end up diametric. 

### Game Sequence Assumptions

Tucked into a footnote, we discover Krehbiel defines the game sequence as assuming (p. 24), “The game is finite, and noncooperative with full information.” Noncooperative and full information assumptions will be addressed in the paper later, and I begin my critique with the assumption of finiteness. By finite, Krehbiel means his models assume a game, and thus the players’ strategy and decisions, are not repeated – an incredible oversight in a chamber where proposed bills are routinely brought up every session, if not more than one time per session. Even in the context of game theory, an unrepeated game is a laboratory thought experiment, and Krehbiel’s failure to include repeated games into his analysis severely undermines his conclusions, as repeated games introduce complexity and often have differing strategies and outcomes for the players when compared to single shot games. Krehbiel, in a footnote, later justifies his selection of a static, non-repeated game by noting that (p. 40) “legislators have short time horizons.” This justification is offered without further defense, and seems in direct conflict with the reality that the average tenure both Representatives and Senators has been rising since 1789, and as of 2017 the tenure for a sitting Representative was 9.4 years, and for a Senator was 10.1 years (Glassman & Wilhelm, 2017). Even if Krehbiel is using the two-year election cycle for a Member of the House, the assumption that two years is a short time horizon is problematic, given a de minimis assumption of lawmaker preference for winning re-election (Mayhew, 1974). Under Krehbiel’s model assumptions, even this desire goes unaddressed. 

### Behavioral Assumptions

Krehbiel sets up the player’s game behavior with the reality defying statement that “Players know the game, know each others’ preferences, and understand who is the pivotal voter in any given setting” (p. 25), and are therefore able to construct utility optimizing strategies, where utility for a player is defined as the “ideal point” in the policy spectrum that a player prefers. Taken in sequence, the pivot model’s failures become clear. First, for a player to “know the game” is simply a restatement, in game theory terms, that the players have “full information” preceding strategy selection. This is demonstrably untrue in the legislative context, as information flow and its intentional disruption has been demonstrated to be an integral component of legislative leadership, power, and function, and “pervasive inequalities exist among members of Congress regarding the information they possess during the legislative process” (Curry, 2015). Even forgiving Krehbiel for not anticipating work a decade later than his own, however, the simplistic assumption that full information is (or was) available to lawmakers at any historical point seems an avoidable error. 

Continuing to Krehbiel’s second behavioral assertion, that legislators know each others’ preferences, is as vulnerable as his first. Legislators, we are to assume, have a mental map whereupon every other Congress member, all 534 of them, stand on Krehbiel’s unidimensional policy preference construct (p. 35). Assuming [N -1] possible policy issues i, any combination of which could comprise any single B (bill) that all L (population of legislators), we can easily see that the policy preference space any single Li (legislator) must possess would quickly pass by unmanageable as it continued on towards impossible. Even without a made-up graduate student pseudo-formula, however, we know from the statements of legislators themselves that they are not fully aware of the contents of many bills, and thus cannot possibly be aware of where any individual colleague stands on the bills, let alone a bill comprising multiple issues about which that individual may have a preference. 

Even if we are to grant his assertions about full player game and competitor knowledge, we must also address Krehbiel’s final behavioral assumption, that players “understand who is the pivotal voter in any given setting.” Central to this assertion is that of the “pivotal voter,” a hypothetical Member whose policy preference lies at the center of the policy dimensional space. Of course, in reality no legislative median voter exists, but without the construction of one, Krehbiel has no way to illustrate how his game actually proceeds – and so he constructs one, noting only that he finds this hypothetical adoption of a “plausible positive baseline” (p. 26, emphasis in original) more helpful than that of an assumption that a presidential preference would happen to align with legislative outcomes. He notes that scholarly support for the “median voter theory is nontrivial.” Hardly a rousing endorsement, Krehbiel’s use of “nontrivial” appears to be the deliberate waving away of the reality that the median voter theorem has well established problems which directly undermine his assumptions and model. Directly to this concern, “the absence of median voter equilibrium may also arise in in models where candidates can manipulate information and voter turnout” and that in those cases “chaos and indecision are predicted” (Congleton, 2004). In the legislative context, as discussed already, manipulation and withholding of information is a key strategy (Curry, 2015). In this context, the ability to manipulate voter turnout may not be a fatal wound to the pivot model, but the ability for party leadership to control votes, and arrange for Members to be deliberately absent, seems to cast at least further doubt on an already problematic model.

### Conclusion

On balance, Krehbiel sacrifices too much accuracy in his seeking of model simplicity. In defining the lawmaking space as one where the game is finite, noncooperative, and with complete information, the model fails to reflect the Congressional body in whole. And in assuming full knowledge of the game, full knowledge of both competitor and ally preferences, and full knowledge of who the pivot voter is in all settings, Krehbiel ends up describing a collection of players too far removed from any US Congress member to be relied upon for understanding how “the most intricate lawmaking system in the world” (p. 21) really works. 


## Mancur Olson, Collective Action, and Police Unions

### *The Logic of Collective Action* and the 'Tigers who Live on Oranges'

> “Those who tell you of trades unions bent on raising wages by moral suasion alone are like those who would tell you of tigers who live on oranges.”
> Henry George, 1891


### Introduction

*In The Logic of Collective Action* (1965), Mancur Olson set out a rational-choice theory of collective action, and turned the field of political science away from the leading theory of the time, pluralism. The problem of collective action – how to minimize “free riders” who want to collect the benefit of group action without working towards the group’s goals themselves – continues to be an active field of research today. 

Olson spends considerable time in his book on the subject of labor unions, and specifically in justifying the use of “closed shops,” the practice of requiring employees pay for membership in a union, regardless of their own personal stance or choice. Olson’s arguments have held great sway, but are now under considerable attack with the current US Supreme Court considering Janus v. American Federation of State, County, and Municipal Employees. In Janus the Court is considering the plaintiff’s claim that his First Amendment right to association (or dis-association) is violated through a governmental requirement that he pay union dues. If the Court finds for the plaintiff, some commentators predict the blow to organized labor organizations will be heavy, perhaps fatal.  Certainly, Olson would agree, contending as he does that labor organizations survive only because closed shops solve for the free-rider problem.

However, in this paper I will attempt to show Olson’s conception of unions does not adequately account for public employee unions, specifically public safety employee unions. I will use the case of the Fraternal Order of Police (FOP) as a negative case example to show specific areas where Olson’s assumptions of how labor unions are borne, survive, and attract new members are poorly theorized. Olson never acknowledges the existence of public sector unions, and therefore severely under- or wrongly theorizes how public-sector unions might differ from private sector ones. Indeed, despite Olson’s disbelief, the Utah FOP and many other police labor organizations are ‘tigers who live on oranges.’

### Is it fair to hold police unions “against” Olson?

It is tempting to let Olson off the hook for not adequately theorizing a type of union which, after all, he never actually mentions. Some might be tempted to argue only after President Kennedy signed executive order 10988, extending protections of the Walker Act to federal employees, did many states even recognize the right of public sector employees to collectively bargain. This is an unsatisfactory explanation, however, because while many police labor organizations don’t have collective bargaining powers, many do, with one researcher placing the number at 70% (Kadleck, 2003). Further, the FOP was first established in 1918, over forty years before the publication of Olson’s work, suggesting he had time to orient himself to a union landscape which included public sector unions. It seems more likely that Olson is a victim of blinding himself. Tied as he is to the idea that “unions are for collective bargaining” (p. 76) it seems likely he just did not remain open to the idea that some labor organizations not only do not collectively bargain, but do not need to.  And, as this paper will later show, police labor organizations provide enough evidence against Olson’s theory of collective action that he was well served not to confront them directly, at least in the name of keeping his theory whole.

To be fair to Olson, even decades after publication of The Logic of Collective Action, literature on police associations and unions is abnormally thin, and has tended to portray the organizations in undifferentiated, unchanging, and negative terms, usually as an impediment to progressive improvement of policing. Walker (2008,) finds only 19 published academic manuscripts of any type – peer-reviewed or not – on police unions in the 33 years previous to his study, and in his review of the literature finds it “extremely limited and unbalanced” (p. 97).

Though beyond the scope of this paper, the neglect of the American police union is surprising, as is the generally negative academic take on what is, after all, an institution that has successfully fought for benefits and professional standardization of a middle-class occupation.  The brief literature review I engaged in for this paper led me to agree with Walker’s (2008, p. 103) analysis that “most of the discussions are based on dated and unexamined assumptions.” 

### Is the FOP even a union?

Olson does not attempt to provide clear definition of a union, but instead ascribes certain attributes to them throughout his text. Perhaps the closest definition he attempts is (p. 76), “A labor union works primarily to get higher wages, better working conditions, legislation favorable to workers, and the like.” I argue the FOP does fit within Olson’s broad conception, as the organization’s founding impetus has been to improve wages and working conditions, as well as seeking favorable legislation at all levels of government. The FOP was founded in an era where police officers were working twelve-hour shifts, 365 days-a-year, and the founders’ message to their mayor indicated they sought to “get many things through our legislature that our Council will not, or cannot give us” (FOP, 2018a). 

Olson (p. 76) claims (emphasis in original), “Unions are for ‘collective bargaining.” However, the FOP as a national organization has no collective bargaining – there is no national police department with which to bargain. Further, a substantial portion of the subordinate local lodges also lack any kind of ability to negotiate on labor contracts on behalf of their members. While exact numbers are difficult to come by, descriptive findings by Kadleck (2003) show that among police employee labor organizations, only 70.6% of the organizations have collective bargaining powers. For example, within Utah, not a single local lodge, nor the state lodge, has collective bargaining powers. Additional evidence the FOP is a union is found within the organizations historical information, which states the very name “Fraternal Order of Police” was chosen to avoid the politicization of the word “union” (FOP, 2018a): “They decided on this name due to the anti-union sentiment of the time. However, there was no mistaking their intention.”

The sources Olson cites do not restrict themselves to a strict use of the word union. For example, Olson uses Henry George (p. 71) to show that the “essence” of a union is in it’s ability to use force, but George himself uses the phrase “labor association.” 
Finally, in some places Olson uses some broad definition which seems to easily accept the FOP as a “union,” such as when he asserts (p. 73), “…almost every union handles members' grievances against the employer” and (p.74), “many national unions draw some strength from federation, that is from the fact that their members belong to small union locals, and thus at one stage have the advantages of the small group. The small groups, in turn, can be held in the national union through noncollective benefits provided to the locals by the national union.” The FOP meets with both these formulations, as it is a federated organization with state organizations overseeing local lodge organizations, and provides representation for its members during administrative, civil, and criminal complaint proceedings.

### Collective Benefits

“By far the most important single factor enabling large, national unions to survive was that membership in those unions, and support of the strikes they called, was to a great degree compulsory” writes Olson (p. 68). “Compulsory membership and picket lines,” he writes (p. 71), are, “of the essence of unionism.” This may have been true some places, some of the time, but is hardly applicable in the public sector, which is generally prohibited from striking, let alone picket lines. Even more specifically, as noted earlier, in law enforcement contexts the strike is exceedingly rare; yet, still, membership has grown, and public-sector labor organizations have broadly seen great growth even in the face of a private sector labor movement which has declined in membership and power. 

In terms of critiquing Olson, the counter-examples of public sector unions are very troubling for Olson’s claims. He ties his strong arguments for compulsory union membership directly to the success of the labor movement: “If the conclusion that compulsory membership is usually essential for an enduring, stable labor movement is correct, then it follows that some of the usual arguments against the union shop are fallacious.” 

Wage and benefit increases are generally part of any labor organization’s central goals, no matter which sector they hail from. However, that similarity masks an important difference which Olson does not take into account. Olson claims, “Employers often will not be able to survive if they pay higher wages than competing firms.” (p. 67). Of course, in the public sector it is not as if the government agency often just folds up shop. For police labor organizations without collective bargaining, wage growth is more likely a counter-cyclical question for jurisdictions competing for law enforcement officers. In the current growth cycle, police departments in Utah are facing critical deficits in their manpower (Caldwell, 2018). In response, cities are overtly raising wages and benefits. Salt Lake City just announced they are adding fifty more officers and are continuing to increase wages well in advance of competing jurisdictions. Unified Police Department is also actively recruiting “lateral” transfers from other police jurisdictions, as well as increasing benefits or even adding ones that very few other agencies (if any) offer, such as 12% matching 401k contributions. This in particular is rare, if only because police officers still have pension benefits, in addition to so-called 457k accounts, which operate like a 401k for tax and savings purposes, but can be withdrawn when the officer retires, instead of age 65 ½.

### Noncollective Benefits

Olson leaves a foothold for future scholars to expand his theory in the context of public safety unions such as the FOP (p. 67): “Small unions may have a further advantage over larger unions stemming from the fact that they can be meaningful social and recreational units, and thus also offer noncollective social benefits that attract members.” Olson may be underselling the “social and recreational” units here, at least in the context of public safety unions. The national FOP has within its mission statement that they seek (FOPb, 2018) “to encourage fraternal, educational, charitable and social activities among law enforcement officers.” 

While Olson tends to be dismissive of the importance of noncollective benefits in union growth and success, in police unions the noncollective benefits are extremely important, especially given the lack of features compulsory benefits, strikes, and picket lines. An empirical example can be found in the Utah FOP. One of the primary benefits the organization provides to its members is that of legal protection, especially in the face of increasingly brazen charging decisions by prosecutors in the aftermath of police use-of-force incidents. So, if the collective action problem to be solved is “how do we best protect ourselves from legal threats resulting from on-the-job incidents” the obvious answer is to secure the largest legal plan for FOP members. And, in fact, such a solution did eventually present itself, in the form of the FOP’s national legal plan. Yet, due to concerns among the local Utah FOP leaders regarding the quality and cost of that national plan, in 2012 they formed the Utah FOP Legal Plan. Legal representation, especially in the aftermath of critical incidents (usually on-duty shootings) has proven to be the single greatest recruitment tool for the Utah FOP (source: me), and following the founding of a locally controlled legal plan and the high profile criminal cases against several officers after shootings, the Utah FOP increased its membership from approximately 800 officers in 2010, to just over 3300 officers today. Legal representation is clearly a “noncollective” benefit – it is not bargained for, is individually collected for upon need, and fits Olson’s own definition (p. 73) of a noncollective benefit “such as insurance.”

Olson (p. 75) contends that unions are strong because they are large, and “small groups, and a union’s noncollective benefits cannot usually be sufficient to bring in very many members.” Here Olson clearly is relying on a conception of “bargaining” taking place on a national scale, such as a contract negotiation for all factory employees at General Motors for instance. But this is clearly an example where police unions such as the FOP have succeeded because of their noncollective benefits and members’ small local lodge affiliation, exactly opposite what Olson expects. There is no national American police department with which to bargain, and contract negotiations, where they are even available, take place within either a statewide or municipal context. Further, the “noncollective” benefits are indeed what have proven in the modern era to be the greatest recruitment tool for local FOP lodges, particularly in those areas without collective bargaining rights. 

Olson claims the “the political strength of a large union is obviously greater than that of a small one.” (p. 68). While there is obviously strength in numbers generally, the picture of large, national unions wielding political power is, again, more at home within private sector labor. Within state, county, and municipal public safety labor organizations, local groups tend to have more local political power, and be more effective with their city council than the state organization can be. But similarly, at the county or state level, the state is better equipped. Not every local lodge leader is effective on the larger stage, but this is not a limitation (necessarily) of the local lodge leader, as many of our better state and national leaders have not necessarily been effective at the granular local level. This may be because some of the tools that are effective at the state level, particularly the relationships with and use of news media, may not translate as well for a local lodge. It may be harder for a Nebo City police officer issue to gain traction with a media market dominated by Salt Lake City interests. This further justifies the “umbrella” organization model, for if a local lodge isn’t able to use local tools, they can ask for assistance at the state level, which has the media experience and resources to change a local issue into a state-wide media issue.

### Strike Three

Olson contends that willingness to use force and the strike are the level with which unions accomplish their goals: “…the strike that is the union’s major weapon…” (p. 67). There is very little evidence to suggest this is the case in the public safety union context generally, or the FOP specifically. Upon its founding in 1918 as a national organization, the FOP included an anti-strike provision in its constitution. That provision has been rarely tested, although some historical examples have occurred, such as a 1967 work-stoppage in Youngstown, Ohio by officers during a salary dispute with the city. It is because they are so rare that those that do occur stand out so strikingly. Still, the “blue strike” is incredibly rare within policing generally, and FOP lodges specifically, and cannot be considered seriously as the organization’s “major weapon” as proposed by Olson.

Olson endorses fully an idea put forth by Henry George (1891, p. 86) that unions must use force to attain their goals, and in fact this is “the essence of unionism” (p. 71): “Labor associations can do nothing to raise wages but by force…Those who tell you of trades unions bent on raising wages by moral suasion alone are like those who would tell you of tigers who live on oranges.”
“Moral suasion,” though, appears to be exactly what the FOP national organization has used both historically and in the contemporary context. Left undefined by both George and Mancur, we can read into the phrase to assume moral suasion includes rhetoric, media pressure, and the like. This sounds very similar to the FOP founder’s letter to Mayor Joseph G. Armstrong, justifying the organization’s goal to “bring our aggrievances before the Mayor or Council and have many things adjusted that we are unable to present in any other way...we could get many things through our legislature that our Council will not, or cannot give us." Further, the modern FOP organization’s formal language states (FOPc, 2018), “We are committed to improving the working conditions of law enforcement officers and the safety of those we serve through education, legislation, information, community involvement, and employee representation.”

### Conclusion

Olson was writing in a world of “big corporations, big unions, big media, and big political parties” (Rauch, 2013) and his book takes on those large interests, effectively doing away with the prevailing belief in a pluralism which dominated thinking at the time. In retrospect, his sections on unions effectively captured what had occurred, and why, in private labor organizations. But he was writing at a time when union membership was at it’s height, and within a decade of writing The Logic of Collective Action the American labor movement would have begun its long slow decline. Today, public employees are five times more likely to be a member of a labor organization than are private sector employees, and Olson’s rational choice formulation of how labor organizations attempt to deal with free-riders do not translate easily to public employee labor organizations. More research is clearly needed to better theorize police unions specifically, and public-sector unions more generally. Given their relative importance in the modern labor movement, it is surprising more has not already been done to theorize exactly how these tigers survive on oranges.


## The Advocacy Coalition Framework

### Introduction

The advocacy coalition framework (ACF) was developed by Paul Sabatier and Hank Jenkins-Smith (1993), primarily using environmental case studies from the United States. ACF attempts to develop a holistic theory of policymaking. Because it is designed to be applicable across a variety of policy contexts, and because it assumes that each policy environment is inherently intricate, the ACF framework is itself quite complex (see Pierce, Peterson, Jones, Garrard, & Vu, 2017, fig. 1). 

In the ACF framework, policymaking takes place in a complicated environment which contains multiple actors across multiple levels of government (Weible, Sabatier, & McQueen, 2009). Policy makers have very high levels of uncertainty, and so make decisions imbued with inherent ambiguity (Jones, 2002; Simon, 1972). Policy decisions take years to produce policy outcomes, and those outcomes are difficult to ascertain with any certainty. In the ACF framework, policy change takes place through either “policy-oriented learning from accumulated technical and scientific information” or “external shocks or perturbations to otherwise stable scientific, social, economic, and political factors” (Luxon, 2019, p. 106). ACF recognizes that there are also different types of policies at play, with some policies being fairly straightforward, others being very technical and complex and done outside public notice, while some policies are incredibly political, controversial, and evoke national partisan fights. These policy disagreements are primarily value driven, rather than by technical or analytic differences. ACF recognizes that because competing values and beliefs are at the heart of the policy agendas, understanding policy process demands scholars develop “a good understanding of the political context of the problem” being studied (Weible, 2006, p. 96).

### Components of the Advocacy Coalition Framework

Beliefs are a crucial concept in ACF, as individuals and coalitions compete in politics to turn their beliefs into implemented policy. In ACF, there are three types of fundamental beliefs (Pierce et al., 2017). “Core” beliefs are those that are so rooted in the individual that they are unlikely to change or be changed by external events. Core beliefs tend to be so broad that they are unlikely to provide precise rules for policy. 
Conversely, “policy core” beliefs are more likely to be changeable, and more likely to influence how an individual believes policy should be constructed. The third type, “secondary aspect” beliefs are far less important to the definition of the individual, and much more likely to shape their views on policy implementation, while being more easily shaped than the other types of belief by learning new information about a policy (Luxon, 2019). Individuals with shared belief systems are likely to be found together in coalitions, which form “policy subsystems” in ACF analysis.

ACF treats policy subsystems as the unit of analysis. These subsystems are comprised of politicians, policy experts, advocates, and professionals. These subsystems are similar to the “policy stream” in the multiple streams framework (Kingdon, 1995), and in ACF the policy subsystem has within it coalitions of associated members all focused on a specific policy issue (Jenkins-Smith, Silva, Gupta, & Ripberger, 2014). These coalitions are “epistemic communities” (Haas, 1989) –  systems of shared belief and activity – and networks within the subsystem can cooperate or compete to bring their preferred policy solutions to the forefront during policy debates. Strong coalitions often dominate policy issues for long periods, and because ACF is concerned with policy cycles, the periods examined often stretch out a decade or more.  Lawmakers, who are constrained in attention and time, often assign responsibility in a policy area to senior public administrators. In turn, these officials rely on the advice and consultation from policy subsystems and coalitions, which are framed as the experts in the area. 

Within a policy subsystem, coalitions compete in the policy space. In a pure hypothetical, Coalition “A” and Coalition “B” each has a preferred policy solution. Between the two coalitions is are “policy brokers” who are also part of the policy subsystem, and who work between coalitions and lawmakers. Each coalition has its own policy beliefs and resources to compete with the other with, and policy brokers help structure that competition. Eventually, a decision is taken by the governmental authority with jurisdiction over the policy space in question. 

The governmental decision has outcomes for new institutional rules (or removal of old rules), new resource allocations, and appointments to new institutional bodies. There are also policy outputs from the policy decision, which in turn lead to policy impacts. The new rules, resources, appointments, outputs, and impacts all exist in a feedback loop which in turn alter the existing coalition arrangements. In extreme cases, a policy coalition may cease to exist as the winning coalition solidifies itself as the dominant voice in the policy subsystem for years to come. More commonly, there is merely a shifting of resources and policy strategy among the advocacy groups which comprise the coalitions, and the “game” plays on. 
There are factors in the ACF framework outside of the policy subsystem, all of which have effects on the subsystems. There are relatively stable factors, reminiscent of the forces in punctuated equilibrium theory, which serve to produce a policy environment which favors the status quo against change. These stable parameters include the core and policy core beliefs of the policy actors, social values, the distribution of resources, the social structure in which the subsystems exist, and the core structures of the government (i.e., a constitutional democracy versus a communist state). The common theme of the stable factors is that they tend to be exogenous to the subsystem and other influencing factors. These stable factors influence the rest of the framework but tend not to be influenced themselves. 

Unlike the relatively stable factors, ACF also recognizes that there are endogenous factors which both significantly alter, and are altered by, the policy environment; these are events which are reminiscent of the “shocks” in punctuated equilibrium theory. These changes can include systemic changes in the governing coalition of a subsystem, socio-economic changes such as significant financial crises, sudden shifts in public opinion (such as those seen in the last decade on gay rights and marijuana legalization), and finally, decisions in other policy subsystems which have significant impact on the subsystem being analyzed. Rarely, like in punctuated equilibrium theory(Jones & Baumgartner, 2012), these external events can be linked to a shift in the policy environment, most likely by providing a shift in the internal environment of the policy subsystem. An example of this can is seen in the policy environments following both World War I (a move towards US isolationism), WWII (a move towards international organizations to prevent widespread war), and the Great Depression (a move towards a social safety net).

Also external to the policy subsystem are the opportunities for long-term coalitions to take advantage of. These opportunities are themselves influenced by the stable factors described earlier, but also directly affect how policy subsystems operate. These factors are related to the political systems in which policies are considered, such as the difference between divided party control of the executive and legislative branches of the US government. These factors will dictate whether, and how much, consensus is needed before a policy can be adopted. In broadly democratic political systems, the amount of consensus is relatively high compared to systems with politics which allow for a single governmental actor to take drastic policy action. 

The final structure in ACF theory to be considered are the short-term constraints in which policy subsystem actors operate. These constraints are affected by both the opportunities factors and the external events, but the constraints also operate directly upon the policy subsystem as well. For example, in a policy environment where there have recently been significant policy shifts, the coalitions within a policy subsystem are all constrained from further action as lawmakers turn their attention to other policy subsystems. 

### Conclusion

ACF theory is considered to be the creation initially of Paul Sabatier, but upon his passing, the theory has been continuously refined and adapted to the policy realities of the different contexts within which it continues to be applied. ACF has proven remarkably resilient, as it maintains the flexibility of the policy process approaches that preceded it, while recognizing the feedback loops (Pierson, 2000; Soss & Schram, 2007) which recognize and incorporate the influence of stability, “shocks,” constraints and opportunities in the policy environment, and political systems. A drawback of ACF theory is that it remains a difficult proposition to translate effectively for non-academic audiences, whereas multiple streams analysis and punctuated equilibrium theory both benefit from the ability for non-academic audiences to easily construct metaphors which illuminate policy problems. Relatedly, the majority of ACF applications are ‘one-off’ studies by scholars who use the framework a single time on a single case of interest, often in the environmental policy area (Pierce et al., 2017). ACF scholarship would benefit from a sustained empirical study by scholars over multiple cases, and a broadening of the policy areas investigated. 

## Policy Process in Comparative Context

### Why Comparative?

While most of the policy process theories originated within the United States context, there is broad agreement that good policy study (emphasis in original) “both is and should be comparative” in application (Wilder, 2017, p. S47). There are a number of reasons why comparative applications of theory are broadly supported in social science research generally, and political science and public policy research specifically. Research methods texts justify comparative research in many ways (Halperin & Heath, 2016; Singleton Jr & Straits, 2018). Often the most important reason to engage research comparatively is as an exercise in theory testing. Comparative methods allow the researcher to test the extent a theory built upon cases in one context to be tested for generalizability in cases from a differing context. 

In policy studies, this point has interesting implications. First, as noted in an article (Cairney & Jones, 2016) tracing the claimed impact of the multiple streams approach (Kingdon, 1995), the comparative context allows researchers to identify both ‘universal’ and ‘territorial’ contributions in a theory.  When a theory is successful, as the authors argue multiple streams analysis (MSA) has been, it has ‘universal’ aspects that apply without respect to culture or country. For example, MSA’s focus on bounded rationality applies to humans, regardless of their cultural or national origins. This universality allows for MSA to be transported easily, and likely contributes to how widely it has been applied. At the same time, a good theory has ‘territorial’ aspects which allow the theory to apply in specific contexts, such as authors who use MSA concepts to describe policymaking differences between types of national governments, or regional similarities in responses to policy problems.
A second implication is increasingly seen in policy studies, and more broadly in public administration scholarship. Because the vast majority of public policy theory has been developed in the United States, and the remainder in predominantly Western nations, it is often an untested assumption that the theory will apply in very different cultures. An example of this is in applying the theory of emotional labor, which was first explicated by Hochschild (1983), an American scholar. The theory was then tested, refined, and built in the same US context, with some Europeans applications as the theory was broadened to the public sector (Guy, Newman, & Mastracci, 2014). Still, until very recently the theory was not well tested in collectivist cultural contexts (Hofstede, Hofstede, & Minkov, 2010). Testing the invariance of measurement across multiple Asian countries finds that while broadly applicable, the emotional labor model needs to be more carefully investigated in those contexts (Mastracci & Adams, 2018).

### The Comparative Turn & Recent Examples

The ‘comparative turn’ in policy studies is well documented by Wilder (2017), who sees the domination by American political scientists as fomenting a “reaction” from non-US scholars in the form of network theories of the policy process (p. S50). Three of the four main policy theories – advocacy coalition framework, punctuated equilibrium theory, and multiple streams analysis – were “developed to explain political behavior in the American institutional environment” (p. S51). The other leading theory, Institutional Analysis and Development (IAD), developed by Elinor Ostrom (1990), has a metatheoretical orientation which helped transcend the constraints of the particular American political environment. Still, given the origins of the main policy process theories and frameworks, the majority of work in those theories tended to result in findings which were salient only in that same, or very similar, political context. 

The recognition of an American-centric bias in policy scholarship has led to a ‘comparative turn’ in policy studies (Schlager & Weible, 2013), and the literature in “comparative public policy is voluminous and growing” (Wilder, 2017, p. S56). This recognition has led to a “sea change in the discipline,” (Wilder, p. S59), as new theoretical work has generated approaches which find synergy between the positivist, post-positivist, and constructivist ontologies. Approaches such as the narrative policy framework (NPF) and social construction of target populations (SCTP) have moved beyond critiques of previous discourse analyses as ‘unscientific’ by producing well-specified models with falsifiable hypotheses (DeLeon, 2005; Epstein, Farina, & Heidt, 2014; Jones & McBeth, 2010; Pierce et al., 2014). This has led to a resurgence of policy scholars willing to “take ideas and discourse seriously” (Schmidt, 2008, p. 304). 

This essay is too short to consider anything beyond a few examples of recent comparative work. An example of an applied comparative study is Fifer and Orr’s (2013) study of the Yellowstone fires of 1988. The authors investigate how fire policy in both the United States and Canada differed, and attribute the differences to how the problem of wildland fires are defined in the two countries. Commonly understood as the first step in linear policy models, defining the problem is key to shaping which policy responses will even be considered in policy debates. In the US context, political forces interfered with science-based policymaking by defining wildland fire as a “destructive force without ecological purpose” (Fifer & Orr, 2013, p. 651). However, in Canada, the fires were defined as a natural ecological phenomenon which, if interfered with through more logging and prescribed fires (as in the US), would result in even more negative outcomes. The findings from Fifer and Orr demonstrate the power of discourse, and more broadly, how comparative institutional differences lead to divergent policy interventions.

A further example of public policy theory being applied comparatively is recent work by Jennifer Hadden (2018), who wants to know how advocacy coalitions make tactical choices. Hadden uses evidence from climate advocacy organizations in both the United States and Europe to build upon existing coalition theory. Using qualitative data drawn from interviews with environmental advocacy organizations, Hadden finds new variables for advocacy coalition scholars to include in future quantitative work. In this way, she demonstrates the value of both comparative policy work, and along the way makes a convincing case for the use of a plurality of methods in policy work generally. For instance, Hadden finds that the tactical choices of peers influence how actors make their own tactical choices. This suggests advocacy coalition studies should find ways to operationalize peer choices and their influence, a suggestion which hearkens back to policy network studies which were the first significant European response to the American policy study hegemony (Wilder, 2017).

## Comparing and Combining Policy Theories

The study of public policy proceeds under a relatively constrained number of accepted theoretical approaches. As of yet there is no single public policy theory, framework, or approach which is adequate to explaining all policy process (Cairney & Heikkila, 2014), nor is a grand unifying theory of public policy likely given the complex human factors which are ever-present. This front half of this essay will highlight a set of criteria on which to attempt to compare policy theories, which otherwise are often difficult to compare given their varying focuses, definitions, and methods. The second half of the essay will discuss three approaches to using different theories – for synthesis, for comparison, and for competition. 

### Comparing Theories

Cairney and Heikkila (2014, p. 363) aim to bring “rigor to the process” of comparing public policy theories. The authors are not seeking to identify the ‘best’ theory. Rather, they are interested in “identifying their key concepts, when and how each is particularly useful, and the extent to which the insights of different theories can be combined” (Cairney & Heikkila, 2014, p. 363). 

Perhaps most useful of these three goals is the effort to identify when and how the different theoretical approaches are most useful for researchers. While there are relatively few approaches, even the eight identified in the third edition of Sabatier and Weible’s Theories of the Policy Process (2014) can be overwhelming. Each of the theories are complex and many scholars only use one approach for one or two studies, so learning to differentiate the approaches and identify which is most appropriate for a given subject is valuable. 

Even choosing which criterion to judge policy process theories can be difficult, given the very different concepts which each is most interested in explaining. For instance, where the punctuated equilibrium (PE) approach (Baumgartner et al., 2009; Jones & Mortensen, 2018)is most interested in identifying when and why a theory changes (or remains the same), the social construction of target populations (SCTP) approach (Schneider & Ingram, 1993)is focused on how different populations affected by a given policy are viewed by policy makers end up over- or under-advantaged. This lack of consistent focus and terminology is a major complication to any comparison attempt. 

Cairney and Heikkila (2014, fig. 10.1) choose to use five criterion in their comparison of the elements of each theory: (1) scope and levels of analysis; (2) shared vocabulary and defined concepts; (3) assumptions; (4) model of the individual; and (5) relationships among key concepts. Using these elements allows a researcher to identify the most promising theoretical approach for a given study subject. For example, a researcher who’s research question is related to attention and framing might find that PE theory most appropriate, with its model of the individual more focused on individuals attention than other approaches. Alternatively, a research question which is more explicitly interested in values, emotion, and heuristics could use the five criterion to investigate and find that SCTP theory is more appropriate given its model of the individual. 
However, while selection of an appropriate model is a necessary first step for policy scholars, there is also the “pretty standard advice” (Sabatier, 1999, p. 330) that scholars combine, compare, and contrast their primary theoretical focus with multiple theories. This advice is convered in more detail in the next section. 

### Combining Theory

There are three main reasons to engage with multiple policy process theories in the study of a subject of interest (Cairney, 2013). First is the goal of synthesis, in which the insights from separate theories are combined into a more explanatory theory. A second goal is to complement a primary analysis with insights from another approach, in order to ‘shore up’ areas in which the primary approach is not as able to offer understanding. The third primary motivation for using multiple theories is to introduce contradictory explanations in policy studies. This last may also be understood as an attempt to falsify the claims, particularly the causal claims, offered by the primary theoretical approach.

The goal of synthesis is difficult to attain in policy process studies, for the reasons articulated in the first section of the essay. Most of the policy process approaches are ridden with incomparable concepts, scopes, models of human cognition, or even broad goals of what is important to explain (Cairney & Heikkila, 2014). Peter John(2003, 2013) identifies five aspects of policymaking that interact in complex ways to make processes within it difficult to study: the role of institutions in structuring behavior; policy subsystems; external factors; the choices of actors in policymaking; and beliefs of actors which guide their choices. Synthesis is possible in the policy process context, John argues, because the mainline theories attempt to address these five areas, but none is adequate on its own to do so. However, one critique of the synthetic approach is that although the varying theories often use similar phrases,  “people may attach different meanings to key terms” (Cairney, 2013, p. 7). 

The second common goal in combining policy theory is the complementary approach. This approach is less concerned with providing alternatives for hypothesis testing, and more interested in bringing different lenses to an empirical case. In doing so, the hope is that the researcher can broaden how a case is understood by showing how competing theories explain what has occurred. The complementary approach leverages the differences that are a weakness in the synthetic approach, because the differing worldviews can “produce different answers to the same question and prompt us to seek evidence that we would not otherwise uncover” (Cairney, 2013, p. 8). 

However, in practice the complementary goal can be quite difficult for two reasons. First, due to the varying concepts in the different approaches, constructing an adequate research design is difficult. Related to that difficulty, the second obstacle is that many studies which purport to use a complementary approach do not actually engage the theories at a deep level. Instead, they use a “more manageable, and superficial, proxy for theoretical comparison” (Cairney, 2013, p. 9). When this occurs, the lessons drawn from the proxy are less satisfactory. Given the realities of constrained research resources most academics confront, it is simply impractical to expect a full-fledged application of multiple policy process theories, which are often so complex that even engaging in one approach fully is difficult, and many papers only use a subsection of one.
The third approach to using multiple policy process theories is a contradictory approach. This approach is closest to what Paul Sabatier’s (1999, p. 330) aim in giving  “pretty standard advice” to policy researchers, and reflects his relatively strict interpretation (or post-positivist, depending on the view) of what makes good theory. Namely, by using alternative, competing theories we are able to falsify the claims of theories, and secondarily can make the researcher more aware of the assumptions built into a policy theory. 

### Conclusion

Scholars will generally broadly agree that any of the three aims of using multiple theories in studies is a beneficial goal. Sabatier(1999, pp. 321–322) laid out his vision of what makes good policy theory, principles which are widely endorsed by other scholars (King, Keohane, & Verba, 1994). These principles are that: (1) the theory should be coherent so other scholars are able to test any findings; (2) the causal claims, drivers, and processes should be clear; (3) propositions should be falsifiable; (4) the scope of the theory should be relatively broad; (5) the theory should generate “non-obvious findings” (p. 12) and each assumption should give rise to (at least) several predictions.  

However, Paul Cairney (2013, pp. 10–12) identifies at least five complications to basing scholarship on those principles. First, despite widespread acceptance of the goal of using multiple theories, few researchers actually do so. Second, most of the reason for the support for using multiple theories is because it helps scholars weight claims made from repeated replication studies – but most social research relies on trust, reputation, and peer review rather than intense replication. Third, even when multiple policy theories are engaged, outside of the primary theoretical focus most researchers rely on proxies rather than full engagement with competing or complementary theories. Fourth, the complexity of the policy process is so immense that true falsification may not even be possible. Finally, the fifth obstacle to such ‘pure’ research is that few social science projects adhere closely to all five goals as described by Sabatier, because social science research is more of a social product of the academic environment than the sterile proclamations issued by academia on what constitutes good research.


## Five Policy Theories

There are a lot of potential generic policy framework questions, but as a very broad example consider the following three questions that might be answered with the following short essay:

> -  A number of frameworks have been used to analyze the process and outcomes of American public policy.  Among these frameworks are elitism, pluralism, neopluralism, incrementalism, feminism, subgovernments, policy process models, issue networks, advocacy coalitions, and others.  Identify two frameworks that you regard as especially useful for the study of public policy.  For each, explain how the framework approaches the study of public policy and evaluate its strengths and weaknesses as  tool for understanding the process and substance of public policy.
> -  Scholars make use of frameworks, theories, and models in their attempts to explain the politics of policy-making processes and the collective authoritative public choices that come from them.  Distinguish among these three approaches to empirical inquiry, and discuss how they are interrelated, if at all; then describe the extent to which researchers have improved our understanding by applying them to specific substantive areas of public policy.
> -  Imagine that you have a representative sample of Americans captive for a single lecture on public policy.  What are they least likely to know about public policy making and implementation?  What would be most important to communicate to them?  Choose two different policy areas in your answer and support your decisions about what to emphasize with references to the public policy literature.

The following bits got used a lot in both my American and Public Administration exams. In fact, they may have been the most useful pre-writing I ended up doing. One lesson: concentrate on covering different theories in some depth. They are easy to drop into a variety of contexts. That is, after all, one of the goals of a good theory.

### Five Policy Theories Compared

Ideally, policy process theories help make sense of what appears from the outside to be an inexplicably complex process. Theory helps us identify when policy change occurs, or when the status quo prevails. It identifies factors which propel a policy proposal forward or cause it to stall. A mature theory can help citizens, practitioners, and scholars understand how to use those factors to participate in the policy process themselves, and advocate for their desired policy outcomes (Stone, 2002). Policy studies are essential to political science because they are intimately related to politics – or as Lowi (1972) described, the way in which we find (p. 299) “policy conditions underlying our political patterns” and how (p. 209) “policies determine politics.” 

This essay will cover five main theoretic approaches to public policy and is structured to move somewhat temporally as well. It begins with the oldest and still most commonly understood policy process theory, the linear policy process. Following the rejection of the assumptions of the linear process (Cohen, March, & Olsen, 1972) came punctuated equilibrium (PE) theory, which still retained some linear elements but moved away from the incrementalist predictions of the linear theory.  The third theory, multiple streams analysis (MSA) helped explain the significant policy shifts in punctuated equilibrium theory but added theoretical flexibility which gave it greater explanatory power for understanding why some policies are adopted, while others lay fallow. While MSA has proven popular for its parsimony and flexibility, some scholars have long complained it lacks the ability to explain process in the many different contexts in which policy is found. The fourth theory, advocacy coalition framework (ACF), takes many concepts from the MSA framework, but trades parsimony for greater explanatory power. Finally, the essay concludes with a synopsis of social construction of target populations (SCTP) theory. SCTP theory doesn’t attempt to explain everything in the policy process but instead deconstructs the language and assumptions of policy actors which then are used to justify actions which have inequitable consequences for different populations affected by the policy.

### The Linear Policy Process

The development of non-linear policy process theory is a relatively recent development, and the linear process model remains the most common way of thinking about how policy is developed outside policy studies. The linear policy process is the natural outgrowth of applying the most simplistic rational choice assumptions to policy. In policy studies, the theories that come after the linear process, and its assumptions, are reacting to it. 

Linear policy process theory makes assumptions about the people involved in the process which are rooted in rational choice assumptions. It sees people as self-interested beings who have ordered preferences (preferring one option over another), and that those preferences are stable. It assumes that the individual has no mental, emotional, or cognitive deficiencies which would hamper their ability to make choices in their own best interest. It assumes that the individual is capable of knowing all information related to the choice before them, and thus is able to make a reasoned decision about the which option to select, with the predicted choice being the one which maximizes the individual’s benefit while minimizing the cost. 

The most common linear model holds that there are a series of steps to the policy process. First, a problem is recognized; (2) actors work to define the problem carefully, and in particular the ‘root cause’ of the problem; (3) a number of policy options which would address the problem are conceived; (4) the options are evaluated for costs, benefits, and likelihood of success, and the ‘best’ option is adopted; (5) the chosen policy is implemented; (6) following implementation, the policy is occasionally evaluated for success or failure, and modified accordingly. 

The linear process was never useful as a predictive model of policymaking, and the Garbage Can Model of policy (Cohen et al., 1972) takes the opposite assumptions, and successfully challenged rational choice as an appropriate set of assumptions for policy theorists. The Garbage Can model explains the policy process as inherently chaotic and unpredictable, a mix of problems, ideas, technology, and solutions, all flowing around in an amorphous soup, from which a policy eventually congeals when the right components interact with one another. While Garbage Can modeling has not proven useful as a theoretical framework in the long-run, it is important for the theories it inspired, particularly multiple streams analysis (Kingdon, 1995; Zahariadis, 2014) and the advocacy coalition framework (Sabatier & Weible, 2007).

### Punctuated Equilibrium

Punctuated equilibrium (PE) (Baumgartner & Jones, 2009; True, Jones, & Baumgartner, 1999) is a policy theory which borrows from biological science to describe long periods of policy status quo, suddenly interrupted by significant shifts in the policy landscape. Baumgartner and Jones recognized that the slow incremental policy changes predicted by the base linear policy model were not reflected in the empirical policy evidence. Rather than slow, steady policy progress, they saw long periods of policy stability which were then suddenly disrupted by sharp changes in short periods of instability. This, to the authors, seemed to reflect the sudden evolutionary adaptations seen in the biological sciences, as species maintain long periods of stability, with sudden natural adaptations (Gersick, 1991; Gould & Eldredge, 1977). 
Though more useful than its simplistic predecessor, PE is still at its root a theory of linear change, though with generally more relaxed assumptions about the rational nature of the individuals involved. For instance, PE assumes that people are boundedly rational (Simon, 1976) rather than perfectly so. Similarly, PE recognizes that policymakers have limited attention capacity, and cannot know everything about a policy issue. Another critical concept in PE is that of framing (Zaller, 1991; 1992), which groups use to define how a policy problem is understood, in order to better position their preferred policy solution. 

Similar to both multiple streams theory and advocacy coalition framework, the PE framework attempts to understand how policy groups operate to bring about policy change. PE uses concepts such as agenda setting, policy monopolies, and venue shopping to explain how these groups overcome the natural tendency towards stability and continuity in the policy environment. With agenda setting, groups make strategic choices about how much attention to bring to their preferred policy solutions. If they worry that attention will risk derailing the policy they will work to minimize attention, while at other times, particularly with lawmakers reluctant to pay attention, the policy groups actively manage attention around a policy problem and solution in order to generate political momentum. 

Policy monopolies, which are reminiscent of the ‘iron triangles’ of earlier policy studies (Jordan, 1981) develop in specific policy areas, and like in advocacy coalition framework (Sabatier & Weible, 2014), these monopolies can persist for long periods of time, as they work to continue passing policy which reinforces their access to resources and thus policy influence. The answer to policy groups which are locked out of a policy monopoly is what Baumgartner and Jones (Baumgartner & Jones, 2009) refer to as “venue shopping.” If a group is locked out of the legislative policy arena, for instance, they may choose to change venues and begin looking for ways to pass their preferred policies at the executive or judicial levels of government.

Punctuated equilibrium policy theory was developed in the United States, and it provided a useful explanation of empirical policy changes there. Early use of PE theory (Baumgartner & Jones, 1993; 2010) was used to explain US nuclear policy, which existed mostly out of public sight in the post-war period. Following decades of that stability, where the policy was primarily left to technical experts and legislative subcommittees, anti-nuclear power advocates were successful in challenging the positive image of the nuclear industry, and venue shopped their policy ideas to courts and the public. The existing policy monopoly was broken, and heavy regulation of the nuclear industry effectively halted the expansion that had been seen in the post-WWII period. 

The US government is structured in a divided power arrangement, which tended to reinforce status quo arrangements, and was seen as responsible for the periods of policy stability. However, the PE theory has been usefully applied in non-US contexts as well. A comparative study of policy regimes in the US, Denmark, and Belgium (Baumgartner et al., 2009, p. 615) using data from “dozens of processes across three nations and covering hundreds of thousands of observations” found the same non-normal distribution of policy inputs and effects. This study provides strong evidence that it is not necessarily the US constitutional system which is providing friction in policy development and thus favoring status quo. While all three countries in the study are democracies, there are enough structural differences to suggest that a “General Punctuation Hypothesis” can be applied in comparative contexts. 

Punctuated equilibrium theory is robust and takes its place, alongside multiple streams analysis and advocacy coalition framework, as one of the most cited and useful modern theories of the policy process. It provides a framework that allows even non-scholars to immediately connect with the relatively simple idea – things tend to stay the same, until they do not. At the same time, it has enough complexity and flexibility to be adopted in varied political contexts. 

### Multiple Streams Analysis

The appeal of the linear policy process theory has been its simplistic, straight path construction of policy development. However, scholars have long recognized that the linear process is far more normative than descriptive. John Kingdon attempted to lay out a more realistic, descriptive model (Kingdon, 1995), in what is known as Multiple Streams Analysis (MSA). While on the surface MSA has similarities to punctuated equilibrium theory (Baumgartner & Jones, 2009), it does differ in its departure from the linear process assumptions which punctuated equilibrium held. MSA takes the path between the utter chaos of the garbage can and the too-linear punctuated equilibrium frameworks, and in doing so presents a more compelling theoretical structure than either. 

MSA recognizes that policy is complex, like the social problems it attempts to address. Ambiguity is at the heart of why policy is so difficult to study (Zahariadis, 2014), because policy actors can never really know the root cause of a social problem, and even problem definition – the start of the linear process model – is ultimately a contestable, political step. Rather than assume policy actors are purely rational beings, MSA holds that humans are boundedly rational (Simon, 1976), and so operate with limited information capacity, selective attention, and imperfect cognition. Further, there are significant time constraints which limit the ability of policy makers to ever know enough, let alone know all the facts that perfect decision making would require. In the end, MSA seeks to answer the question: In a universe of nearly limitless policy problems and solutions, how do the relatively few new policies rise above the rest?

MSA is one of the most cited academic theories of the policy process and “key influence on the study of public policy” (Cairney & Jones, 2016, p. 1) with over 12,000 citations as of 2015. The appeal of Kingdon’s framework lies in its flexibility. MSA posits three “streams” in the policy process – the problem, political, and policy streams. How these three streams come together, or fail to, is a useful structure for thinking about why certain policies are implemented, while other, similarly good policies, fail. Understanding the three streams is key to understanding MSA. While the streams are discussed in a certain order here (problem/political/policy), in practice the analysis is much more about how the streams interact than about their temporal order. 

In the problem stream, attention is paid to how attention is gathered around a policy problem. “Attention” in this frame can be mean many things, none of which are necessarily objective indicators. Attention might be statistical information which point to a problem, or in some cases a crisis gathers immediate and widespread attention to a problem. Problems exist whether or not attention is being paid to them, and MSA recognizes that policy makers are only ever paying attention to a very small number of the universe of problems which they could be minding. 
The political stream refers to the many people, advocacy groups, and political bodies such as legislatures, interested and involved in policy making. The partisan composition of a US Congress, for example, will have an impact on whether or not a policy solution which is perceived as increasing tax burdens has any chance of being implemented. Similarly, the national mood in the wake of a financial crisis must be considered when considering whether complex regulatory policy might be implemented. The political stream is about the actors who must pay attention to a problem, and possible solutions, before a policy can be implemented. During some time periods, the political stream dictates that some problems in the problem stream won’t gather attentions, while ideas from the policy stream won’t be considered. In the US context, periods of divided government, when one party controls Congress while the other party controls the presidency, are predicted to have relatively little policy movement. Conversely, periods where one-party controls both the executive and legislative functions are predicted to have a better chance of implementing larger policy changes. 

The policy stream is described as a “policy primeval soup” by Kingdon, where potential policy ideas from a variety of policy actors conceive of potential policy ideas in policy communities. At any point in time, the policy stream contains a large number of possible policy solutions, but not all of them are feasible, supported, or available. While a policy idea may originate with a single actor, the ideas change as they are exposed to and considered by other actors in the policy stream. The ideas that eventually become policy are the result of a large number of participants modifying the original idea, and is often a much wider solution than the original, narrower solution. Some of these actors are so-called “policy entrepreneurs” who recognize an opportunity to insert their own policy solutions into one which is gaining support. 

“Policy entrepreneurs” are important to MSA. These are the people and organizations who recognize that the politics and policy streams are often not in sync. A policy entrepreneur waits for, and recognizes, when the two streams offer an opportunity for their preferred policy idea to be implemented. These policy ideas are developed before the actual streams coincide, and the preferred policy is offered as a solution to a problem which has garnered attention. At the same time, these policy entrepreneurs will work to bring the streams together, for example by attempting to bring greater attention to a problem while the politics stream is perceived as favorable to their already developed policy solution. 
Metaphors to think about the stream process in MSA are numerous (Cairney & Zahariadis, 2016), and that flexibility has been key to its success. Some think of the streams as literal rivers, which once mixed or merged, are difficult to unentangle. Kingdon himself suggests a space launch metaphor in which all factors must be perfect for “launch,” implying that policy makers will abort a policy before implementation if all the factors in the streams are not ideal. 

MSA assumes that when the problem, policy, and political streams come together, there is opportunity for policy change, but most of the time the streams are not in synchrony. Policy entrepreneurs are thus critical to policy change, as they work to align timing in the streams to create the window of opportunity for their preferred policy change. They work to gather critical mass attention to a problem, so that a solution is demanded. They work with other policy actors in the policy stream to develop their preferred solution to the identified problem. They work to shift the political landscape so that policy and law makers are persuaded to adopt their solution. 

In the end, the flexibility of the MSA metaphor, and the relatively low barrier-to-entry for scholars to understand policy through the MSA framework has made it one of the most popular and useful ways to examine policy (Cairney & Jones, 2016). The ease of use of MSA, and the associated limited empirical usefulness, will stand in stark contrast to the advocacy coalition framework (Sabatier & Weible, 2007).

### Advocacy Coalition Framework

The advocacy coalition framework (ACF) was developed by Paul Sabatier  and Hank Jenkins-Smith (Sabatier, 1988; Sabatier & Jenkins-Smith, 1993), and from the beginning has been defined by defining the role of belief systems in the policy process, especially their role in shaping policy-oriented learning. The most complex of the mainline policy theories covered in this essay, ACF attempts to develop a holistic theory of policymaking. Because it is designed to be applicable across a variety of policy contexts, and because it assumes that each policy environment is inherently complex, the ACF framework is itself quite complex. 

In the ACF framework, policymaking takes place in a complicated environment which contains multiple actors across multiple levels of government (Weible, Sabatier, & McQueen, 2009). Policymakers have very high levels of uncertainty, and their decisions are always made with inherent ambiguity. Policy decisions take years to produce policy outcomes, and those outcomes are difficult to ascertain with any certainty. ACF recognizes that there are also different types of policies at play, with some policies being fairly straightforward, others being very technical and complex and done outside public notice, while some policies are incredibly political, controversial, and evoke national partisan fights. 
Beliefs are a key concept in ACF, as individuals and coalitions compete in politics to turn their beliefs into implemented policy. In ACF, there are three types of basic beliefs. “Core” beliefs are those that are so rooted in the individual that they are unlikely to change, or be changed by external events. Core beliefs tend to be so broad that they are unlikely to provide meticulous rules for policy. Conversely, “policy core” beliefs are more likely to be changeable, and more likely to influence how an individual believes policy should be constructed. The third type, “secondary aspect” beliefs are far less important to the definition of the individual, and much more likely to shape their views on policy implementation, while being more easily shaped than the other types of belief by learning new information about a policy. Individuals with shared belief systems are likely to be found together in coalitions, which form “policy subsystems” in ACF analysis.

ACF treats policy subsystems as the unit of analysis. These subsystems are comprised of politicians, policy experts, advocates, and professionals. These subsystems are similar to the “policy stream” in the multiple streams framework (Kingdon, 1995), and in ACF the policy subsystem has within it coalitions of associated members all focused on a specific policy issue. These coalitions are “epistemic communities” (Haas, 1989) –  systems of shared belief and activity – and networks within the subsystem can cooperate or compete to bring their preferred policy solutions to the forefront during policy debates. Strong coalitions often dominate policy issues for long periods of time, and because ACF is concerned with policy cycles, the periods examined often stretch out a decade or more.  Law makers, who are constrained in attention and time, often assign responsibility in a policy area to senior public administrators, who in turn rely on the advice and consultation from policy subsystems, who are framed as the experts in the area. 

Within a policy subsystem, coalitions compete in the policy space. In a simple hypothetical, Coalition “A” and Coalition “B” each have a preferred policy solution. Between the two coalitions is are “policy brokers” who are also part of the policy subsystem, and who work between coalitions and lawmakers. Each coalition has its own policy beliefs and resources to compete with the other with, and policy brokers help structure that competition. Eventually, a decision is taken by the governmental authority with jurisdiction in the policy space at question. 
The governmental decision has outcomes for new institutional rules (or removal of old rules), new resource allocations, and appointments to new institutional bodies. There are also policy outputs from the policy decision, which in turn lead to policy impacts. The new rules, resources, appointments, outputs, and impacts all exist in a feedback loop which in turn alter the existing coalition arrangements. In extreme cases, a policy coalition may cease to exist as the winning coalition solidifies itself as the dominant voice in the policy subsystem for years to come. More commonly, there is simply a shifting of resources and policy strategy among the advocacy groups which comprise the coalitions, and the “game” plays on. 

There are factors in the ACF framework outside of the policy subsystem, all of which have effects on the subsystems. There are relatively stable factors, reminiscent of the forces in punctuated equilibrium theory, which serve to produce a policy environment which favors the status quo against change. These stable parameters include the core and policy core beliefs of the policy actors, social values, the distribution of resources, the social structure in which the subsystems exist, and the core structures of the government (i.e. a constitutional democracy versus a communist state). The common theme of the stable factors is they tend to be exogenous to the subsystem and other influencing factors. These stable factors influence the rest of the framework, but tend not to be influenced themselves. 

Unlike the relatively stable factors, ACF also recognizes that there are endogenous factors which both significantly alter, and are altered by, the policy environment; these are events which are reminiscent of the “shocks” in punctuated equilibrium theory. These changes can include systemic changes in the governing coalition of a subsystem, socio-economic changes such as large financial crises, sudden shifts in public opinion (such as those seen in the last decade on gay rights and marijuana legalization), and finally, decisions in other policy subsystems which have large impact on the subsystem being analyzed. Rarely, like in punctuated equilibrium theory, these external events can be linked to a very large shift in the policy environment, most likely by providing a shift in the internal environment of the policy subsystem. An example of this can be seen in the policy environments following both World War I (a move towards US isolationism), WWII (a move towards international organizations to prevent widespread war), and the Great Depression (a move towards a social safety net).

Also external to the policy subsystem are the opportunities for long-term coalitions to take advantage of. These opportunities are themselves influenced by the stable factors described earlier, but also directly affect how policy subsystems operate. These factors are related to the political systems in which policies are considered, such as the difference between divided party control of the executive and legislative branches of the US government. These factors will dictate whether, and how much, consensus is needed before a policy can be adopted. In broadly democratic political systems, the amount of consensus is relatively high compared to systems with politics which allow for a single governmental actor to take drastic policy action. 

The final structure in ACF theory to be considered are the short-term constraints in which policy subsystem actors operate. These constraints are affected by both the opportunities factors and the external events, but the constraints also operate directly upon the policy subsystem as well. For example, in a policy environment where there has recently been significant policy shifts, the coalitions within a policy subsystem are all constrained from further action as law makers turn their attention to other policy subsystems. 

ACF theory is considered to be the creation initially of Paul Sabatier, but upon his passing the theory has been continuously refined and adapted to the policy realities of the different contexts within which it continues to be applied. ACF has proven remarkably resilient, as it maintains the theoretical flexibility of the theory systems that preceded it, while recognizing the feedback loops (Soss & Schram, 2007) which recognize and incorporate the influence of stability, “shocks,” constraints and opportunities in the policy environment, and political systems. The drawback of ACF theory is that it remains a difficult proposition to translate effectively for non-academic audiences, whereas multiple streams analysis and punctuated equilibrium theory both benefit from the ability for non-academic audiences to easily construct metaphors which illuminate policy problems. 

### Social Construction of Target Populations

To this point, this essay has concentrated on the large theories and frameworks, which all aim to explain the policy process as a whole. This proves to be a difficult goal for policy theory to meet, given the extreme complexity in policy types, political environments, and even policy problems. However, not all theory must be so comprehensive, and in some ways theories with more restricted aims, like the social construction of target populations (SCTP), are more able to give clear theoretical explanations coupled with the ability to offer predictive and empirical power, because they examine smaller pieces of the larger policy environment. 

SCTP was first developed by Schneider and Ingram (1993), and defeats another assumption of the linear process, that policy makers, and policy itself, are neutral or unbiased actors. Their argument is not itself post-modern, but builds on post-modernist critiques of language which deconstruct the power relationships inherent in language (Foucault, 1991, 2005; Yanow, 2003). “Foucauldian discourse analysis” has been effectively used by researchers to understand how different approaches to language “contain critical assumptions about how changes in policy relate to broader social change” (Sharp & Richardson, 2001, p. 193). The central role of language in policy debates (Schmidt, 2000) reflects how important language is to the human experience more generally: “The limits of my language mean the limits of my world,” (Wittgenstein, 2013, pt. 5.6).

Schneider and Ingram show that elected policy makers adopt value judgements about the social groups which are impacted by policy programs, and that those value judgements have an impact on the policies they create and implement. In this framing of political statements, some politicians will for instance, use language which implies that individuals living in poverty are lazy and have created their own situation, that may justify policies which withhold government benefits from that group. But just as language can justify under benefitting certain social classes, it can also over-benefit others. The same politician may use language which confers noble, worthy qualities on business owners, which would then serve to justify policies which shift resources to that social class. Moreover, this type of construction is not limited only to politicians, and front-line, street-level bureaucrats (Lipsky, 1983). Police officers and teachers  have been found to construct their own identities of the citizens they serve, sometimes to the detriment of those citizens (Maynard-Moody, Musheno, & Musheno, 2003).

Construction of target populations is not as simple as “positive” and “negative” populations though in the SCTP theory, and Schneider and Ingram illustrate this (1993, p. 336) through their use of a two-axis notional figure, where measures of “positive and negative” constructions are paired with “high and low” perceived power constructions. Power in this use is the ability of a social group to accept or reject the image painted onto them. This gives a four-category scheme of “advantaged” (high power/positive), “contenders” (high power/negative), “dependents” (low power/positive), and “deviants” (low power/negative) social groups. 

These simplistic categorizations of complex populations make it easier for politicians and governments to implement policies which over-benefit the advantaged, while making it extremely difficult for “deviant” populations to even challenge their disadvantaged status in policy debates. This creates a policy feedback loop, as an advantaged group like homeowners not only are over-benefitted in terms of resources granted by policy, but then are able to reify their position in the social hierarchy, leaving to more opportunity to implement even more policies which will benefit them. This creates asymmetries of participation and power, and those asymmetries are reinforced by the system creating and accepting social constructions of the deserving and undeserving. This flaw in the system has long been recognized in political studies (Schattschneider, 1960; Schlozman, 1984), but SCTP offers a useful empirical starting point for understanding how particular social groups have been affected by policy choices influenced by social construction. 

One of the most powerful critiques offered by SCTP is showing how these constructed beliefs about social classes not only has immediate effects on resource allocation, but has effects on those social classes long after the policy maker has left office. This phenomenon is known as a feed-forward effect, or in other literatures as path dependence (Pierson, 2000). Path dependence recognizes that the timing of policy choices matters, and policy makers select policies which have self-reinforcing feedback processes (Soss & Schram, 2007). These processes represent the resiliency of institutions which are far longer lived than the policy makers tenure (Sanders, 2006). Path dependency imposes a cost to going “back” to a previous point, and the longer a policy scheme has lived, the higher the cost. In this way, earlier choices have greater impact than later ones, as the policies themselves shape the institutions which house the policies (Mettler, 2002). 
SCTP offers a compelling critique of policy studies itself, as it uncovers how the language involved in policy can compel certain beliefs and narratives which can hinder, harm, or help certain classes of individuals (Sharp & Richardson, 2001), even as policy scholars unthinkingly use the same language. Further, SCTP critiques the underlying, formative ideals of early public administration and political science, that of the neutral and unbiased bureaucrat, or public administration scholar. 

From the founding of the American political system, the ideal of competing factions balancing the power of any one faction (Dahl, 1982; Madison, 1787) has provided powerful argument that the US constitution and division of power among the federal branches would protect minority interests from the powerful machinations of the majority. However, the justifications of the pluralist federalist system was largely imputed by Madison and Hamilton into the Federalist Papers in a post-hoc manner intended to justify ratification of the US Constitution (Peterson, 2012). VO Key (1963) was ahead of his time in noting the effects of sectionalist national politics on state politics and parties, noting that national political tides “spill” into local politics. Party cleavages at the national level are projected into state elections, with the national party need to retain control used to justify the racist practices of the Southern Democrats of Key’s time. This can reduce the incentive for local and state politicians to perform well, as they justify their control – and pardon their own structurally biased practices – in the name of national party priorities. So, rather than the administrative and policy state providing an intricate balancing wheel (Madison, 1787; Rohr, 1986) against the predations of a majority, at least in some cases SCTP theory allows us to see how policy and law makers are able to use the concealed power of language to prolong and protect the interests of the already powerful.

One limit of SCTP is that it is less concerned with comprehensive theoretical explanations of policy process, and so in cases where there are not clear-cut social classes at play, SCTP may be less useful. A second limit is that, at least in the original construction of the theory, there is little said about how social classes might contest how they’ve been constructed by policy makers and the public. SCTP has little to say about how, or more importantly why, one social group may attempt to help challenge the social construction of a less powerful one. Why for instance, would a feminist group – hypothetically located as a “challenger” in SCTP – want to help restore the voting rights of an ex-prisoner class? SCTP still has buried assumptions of rational choice, presenting the actions of the powerful as merely, or purely, self-interested.  A final limit of early SCTP theory was a lack of direction – what should individuals and advocates do with this knowledge. That critique has been substantially, though not wholly, blunted as more researchers have become interested in critical policy theories, and extended the original insights into examining the ill effects of economic policy concentrated on women and blacks (Andersen, 2001), food justice (Billings & Cabbil, 2011), and Native American school children (Quijada Cerecer, 2013).

### Conclusion

Good theory is portable – it can be carried across contexts, and when “contexts differ…theory is required to generalize from one to another” (Coppock, 2018, p. 11). Kingdon’s multiple streams framework allows for the identification of  “universal concepts” (Cairney & Jones, 2016) which can be applied in multiple contexts. However, critics of MSA have pointed out that while MSA has contributed greatly to the theoretical literature, it has had relatively less impact in empirical studies. Kingdon developed MSA in the US context with case studies of policymaking at the federal level, and that has limited application of the framework in the international context. 

Recent work linking the streams framework of MSA with the stages analysis of the advocacy coalition framework (Howlett, McConnell, & Perl, 2017) shows a path forwards for researchers who want a more robust system for analyzing policy, particularly in the comparative policy literature. Howlett and his colleagues formulate a five-stream framework, adding a program stream and a process stream, all of which proceeds along the traditional linear policy stage path. Though too soon to judge whether such a combined model will prove any more useful in both theoretical and empirical – and it must, given the additional complexity the model has compared to more parsimonious models – there is at least an attempt to synthesize the main policy theories, most of which are at least several decades old. 

At this time in policy studies, the advocacy coalition framework is still the most flexible and explanatory theory available, and though far from perfect, allows for a broad examination of policy in many contexts. The adaptability of the ACF framework means it can fold in even critical theory insights, which by themselves do not produce a fully satisfactory explanation of how policy is conceived, adopted, and implemented. 

## Institutional Analysis and Development Framework

Common pool resources are those “for which the exclusion of users is difficult (referred to as excludability), and the use of such a resource by one user decreases resource benefits for other users (referred to as subtractability)” (Heikkila & Carter, 2017). Some typical common pool resources are fisheries, water systems, wildlife, and forests. Prior to Elinor Ostrom’s Governing the Commons (Ostrom, 2015), most policy analysis was done using assumptions borrowed from rational choice theories. Ostrom’s (1999, 2011) Institutional Analysis and Development framework is one approach to the policy process that retains rational economic assumptions about policy choice, while focusing on the institutional environments that allow for management of common pool resources.

Unchecked and unexamined, rational choice assumptions can lead to a conclusion that disastrous collective action problems are unavoidable when confronting common pool resources. The rational man, formally modeled as homo economicus, assumes that a person has full information about a problem, well-ordered preferences, and will always act to maximize the net value returned to them among alternatives (Aaron, 1994; Olson, 1965; Ostrom, 2011). This situation results from an inability to effectively manage common pool resources, as actors all ‘rationally’ act in their own best interest. Where a resource is non-excludable and subtractable, this often leaves the resource degraded for all users. When self-interested actors take what they can without regard to the other actors, degradation such as rain forest depletion and troubling reductions in fisheries can result. 

The pure rational choice approach to common resource pools can give rise to the ‘tragedy of the commons,’ (Hardin, 1968). Hardin’s argument was directed at ‘breeding rights’ as a common pool resource, with a Malthusian critique that those who overuse breeding rights will drive the human race towards extinction. People who recognize the danger will choose not to have children, while those without a conscience will continue to have larger and larger families, unfairly drawing on the breeding resource. This rational choice conundrum can only be solved, Hardin argues, by government regulation of breeding rights. 

Ostrom takes on the tragedy of the commons, and the rational choice games which too uncritically give rise to it. The assumptions of rational choice games are too bleak and too uncritical. People often do act cooperatively, and work to govern collective resources. Ostrom developed the Institutional Analysis and Development (IAD framework to give a common scholarly language and approach for researchers interested in studying policy approaches to common resource problems. 

### Elements

What allows some communities to effectively manage their common pool resources? Ostrom’s IAD offers a set of eight design principles that help analysts parse why some common pool resources (CPR) are effectively managed, while others are not. While Ostrom emphasizes that there are not hard and fast rules which will define success in every CPR context, the design principles provide an adequate starting point for policy analysts. The design principles are (1) CPRs are confined, with users able to define the resource being managed as well as legitimate users of the resource; (2) CPRs have rules which define how much a user can have as well as what a user is expected to contribute; (3) Rules are influenced by the users who are affected by them; (4) CPR are governed by ‘monitors’ who watch user conduct and the state of the CPR; users are often monitors, or at least in charge of monitoring the monitors, keeping costs low and consequences swift; (5) Consequences for rule breaking can be low or high depending on pattern of conduct and outcomes of the rule-breaking – higher costs equate to higher consequences for the rule breaker; (6) Mutual monitoring means conflict is routine, but resolution normally low cost and rapid; (7) CPR users can self-organize unrestricted by outside impediment; (8) CPRs are often connected across different geographic and political contexts, but in a way that does not undermine individual projects.

Communities which are able to structure self-governing of CPRs with the eight design principles are able to overcome the tragedy of the commons, Ostrom argues, because users become mutually committed to avoiding rule breaking, conserving the resource, and managing the long-term resource. However, three major complications can undermine even the best designed plans – trust, rules, and complexity. 

Trust between users is crucial because it keeps monitoring and rule enforcement low. While simple rules are key to design, there is often a proliferation of rules, rules about rules, and norms. Different types of rules need to be analyzed for costs and benefits, otherwise rules themselves can become a source of strategic conflict between users seeking maximum benefits. Complexity is a related concern, and because CPRs are often complex systems there can be a tendency to develop complex designs. When the design of a CPR management scheme becomes too complex, however, some users can become overwhelmed by the costs of compliance, while other users leverage the complexity to engage in rule sanctioned behavior that would otherwise be considered to be against the spirit of the CPR design scheme. 

### Applying the IAD

Rules and institutions are central to IAD analysis. Institutions are “rules, norms, and strategies, or collectively shared prescriptions that guide behavior in any given situation” (Watkins & Westphal, 2016, p. S99). Yet, because those rules are composed of words, no ruleset is ever complete, perfectly clear, or free from misunderstanding: “Words are always simpler than the phenomenon to which they refer” (Ostrom, 2011, p. 19; Wittgenstein, [1921] 2013).The rules surrounding common pool resource management are complex, and the IAD framework attempts to provide a common scholarly language to assist in studying those rules. 

In service to that common scholarly language Crawford and Ostrom (2005) developed the ADICO syntax to classify institutional  rules and norms. ADICO is an acronym: (A) attribute, (D) deontic, (I) aim, (C) condition, and (O) or else. These five types of classifiers can be combined to define three types of institutional statements found in action arenas. Strategies have attributes, aims, and conditions (AIC); norms have attributes, deontics, aims, and conditions (ADIC); and rules have the entire ADICO syntax. The ADICO approach can be used by scholars to properly classify the many institutional rules found in studies of institutional policies. Attributes define which actors a statement applies to; a deontic describes what behavior by actors is permitted, expected, or prohibited; the goals a deontic applies to are the aims; the condition refers to the when and where of an aim; and the or else defines the consequence for failure to comply with the statement.
The ADICO syntax is important because it allows scholars to conduct their inquiries and collect data in such a manner that scholars who may have never studied the same context can understand. Doing so adds to the reliability of institutional policy studies, and “conveys valid and substantive meaning” (Basurto, Kingsley, McQueen, Smith, & Weible, 2010, p. 523). IAD studies use the ADICO syntax to impose order on incredibly complex policy contexts which otherwise appear beyond definition.

Basurto and his coauthors  (2010) use the ADICO system (Ostrom & Crawford, 2005) in an IAD approach to comparing tow types of policies – United States transportation policy and the U.S. state of Georgia’s abortion legislation. While the authors intend their use of the ADICO syntax as a proof-of-concept, their descriptive outcomes provide some interesting insights. For example, despite both policies being incredibly dense and complicated, the actual rules of each comprise very little of the whole. The abortion bill contains four rules, and the transportation policy has none. Rules, though, depend on sanctions, which neither policy emphasizes. Rather, the emphasis in both is on norms, which have no sanctions (no “or else” in the ADICO syntax). While including only four rules, the abortion policy had 110 norms; and with no rules, the transportation policy managed 128 norms. 

### Conclusion

The finding (Basurto et al., 2010) that the number of norms in policy far eclipses actual rules underlines a concept borrowed by theorists in the narrative policy framework world (Jones, 2018; McBeth, Jones, & Shanahan, 2014) – the stories we tell are often more important than the (claimed) objective facts within a policy system. Norms are often just as important as rules, despite the central role of rules in the IAD framework (Watkins & Westphal, 2016, p. S99), as “norms are understood as strong motivational and guiding forces of human behavior,” and this understanding transcends many academic disciplines. Ostrom (Ostrom, 2000, p. 154) recognized the central role of norms in managing common pool resources, commenting that policy initiatives too concerned with rational rule making may have been “misdirected – and perhaps even crowded out the formation of social norms that might have enhanced cooperative behavior” that would have helped in policy success. Elinor Ostrom’s contributions to understanding the policy process is difficult to overstate, a contribution which helped make her the sole woman to be awarded a Nobel Prize in Economics (Nobel Prize, 2009). IAD remains one of the ‘giants’ of  public policy theoretical approaches (Cairney, 2013), and because of its careful construction and application over many decades (Sabatier & Weible, 2014) remains an important tool in the policy analysts’ kit.

## Ambiguity and Multiple Streams

The appeal of the earliest policy process literature, linear policy process theory, was its simplistic, straight path construction of policy development. However, scholars have long recognized that the linear process is far more normative than descriptive. John Kingdon attempted to lay out a more realistic, descriptive model (Kingdon, 1995), in what is known as the Multiple Streams Framework (MSF). While MSF has surface similarities to punctuated equilibrium theory (Baumgartner & Jones, 2010), it differs in its departure from the linear process assumptions which punctuated equilibrium holds. MSF was inspired by, but takes the path between, the utter chaos of the garbage can (Cohen, March, & Olsen, 1972) and the too-linear punctuated equilibrium frameworks (Baumgartner & Jones, 2010), and in doing so presents a more compelling theoretical structure than either. 

### Complexity, Ambiguity, and Subjectivity

MSF recognizes that policy is complex, like the social problems it attempts to address. Ambiguity is at the heart of why the policy process is so difficult to study (Herweg, Zahariadis, & Zohlnhöfer, 2018; Jones et al., 2016; Zahariadis, 2014), because policy actors can never really know the root cause of a social problem, and even problem definition – the start of the linear process model – is ultimately a contestable, political step. Rather than assume policy actors are purely rational beings, MSF holds that humans are boundedly rational (Simon, 1976), and so operate with limited information capacity, selective and serial attention, and imperfect cognition, negating “the existence of a rational solution to a given problem” (Herweg et al., 2018, p. 18). Further, there are significant time constraints which limit the ability of policymakers ever to know enough, let alone know all the facts that perfectly sound decision making would require. In the end, MSF seeks to answer the question: In a universe of nearly limitless policy problems and solutions, how do the relatively few new policies rise above the rest?

### Problem, Political, and Policy Streams

MSF is one of the most cited academic theories of the policy process and “key influence on the study of public policy” (Cairney & Jones, 2016, p. 1) with over 12,000 citations as of 2015 (Jones et al., 2016, p. 13). The appeal of Kingdon’s framework lies in its flexibility. MSF posits three “streams” in the policy process – the problem, political, and policy streams. How these three streams come together, or fail to, is a useful structure for thinking about why certain policies are adopted, while other, similarly good policies, fail to gain support. Understanding the three streams is key to understanding MSF. While the streams are discussed in a particular order here (problem/political/policy), in practice the analysis is much more about how the streams interact, without a temporal order. The streams exist independently of one another, and require exogenous actors (policy entrepreneurs) to recognize that a window exists to bring the three streams together. 

In the problem stream, attention either gathers or remains diffuse around a policy problem. Problems can be defined as (Herweg et al., 2018, p. 21) “sets of circumstances that deviate from policymakers’ or citizens’ ideal states.” Problems are therefore not objectively perceived, but subjective social constructs which are shaped through interactions between people, experiences, and circumstances. Problems shape people, while simultaneously people shape problems. Those problems that gather attention gain saliency in the policy environment and are more likely to attract competing solutions. ‘Attention’ in this frame can mean many things, none of which are necessarily objective indicators. Attention might be statistical information that point to a problem, or in some cases a crisis gathers immediate and widespread attention to a set of conditions. In this shapeable context, then, agency becomes an important concept and has been the focus of recent work which attempts to more deeply understand how agency is shaped (Winkel & Leipold, 2016).

In the case where a problem has no identifiable policy solution, few policymakers will be motivated to identify a problem as such, because there is little efficiency or effectiveness in merely identifying the unsolvable. Borrowing from Simon (1976), policy actors are boundedly rational sequential processors of problem information, preventing them from taking on all problems. Problems exist whether or not attention is given to them, and MSF recognizes that policymakers are only ever paying attention to a scintilla of the possible problems that they could be minding. 
The political stream refers to the many people, advocacy groups, and political bodies such as legislatures, interested and involved in policy making. In the US context, periods of divided government, when one party controls Congress while the other party controls the presidency, are predicted to have relatively little policy movement. Conversely, periods where one-party controls both the executive and legislative functions are predicted to have a better chance of implementing more substantial policy changes. The partisan composition of the U.S. Congress, for example, will impact whether or not a policy solution perceived as a tax burden has any chance of being adopted. Similarly, the national mood in the wake of a financial crisis must be considered when considering whether complex regulatory policy might be embraced. The political stream is about the actors who must pay attention to a problem, and possible solutions, before a policy can be selected and adopted. During some periods, the political stream dictates that some currents in the problem stream will gather attention, while ideas from the policy stream will not be considered. To the well-attuned policy entrepreneur, mismatches of this sort signal an opportunity to promote their favored policies. 

The policy stream is described as a “policy primeval soup” by Kingdon, where potential policy ideas from a variety of policy actors conceive of potential policy ideas in policy communities. At any point in time, the policy stream contains a large number of possible policy solutions, but not all of them are feasible, supported, or available. While a policy idea may originate with a single actor, the ideas change as they are exposed to and considered by other actors in the policy stream. Ideas that eventually become policy are the result of a large number of participants modifying the original idea and is often a much wider solution than the original, narrower solution. Some of these actors are so-called “policy entrepreneurs” who recognize an opportunity to insert their preferred policy solutions into one which is gaining support. 

“Policy entrepreneurs” are essential to the MSF. These are the people and organizations who recognize that the politics and policy streams are often not in sync. A policy entrepreneur waits for, and recognizes, when the two streams offer an opportunity for their preferred policy idea to be implemented. These policy ideas are developed before the actual streams coincide, and the preferred policy is offered as a solution to a problem that has garnered attention. At the same time, these policy entrepreneurs will work to bring the streams together, for example by attempting to bring greater attention to a problem while the politics stream is perceived as favorable to their already developed policy solution. 
There are numerous metaphors we can use to think about the stream process in MSF (Cairney & Zahariadis, 2016), and flexibility has been key to this framework’s success (see also: Jones et al., 2016, fig. 1). Some think of the streams as literal rivers, that once mixed or merged, are difficult to unentangle. Kingdon himself suggests a space launch metaphor in which all factors must be perfect for “launch,” implying that policymakers will abort a policy before implementation if all the factors in the streams are not ideal. 

MSF assumes that when the problem, policy, and political streams come together, there is an opportunity for policy change, but most of the time the streams do not synchronize. Policy entrepreneurs are thus critical to policy change, as they work to align timing in the streams to create a window of opportunity for their preferred policy change. They work to gather critical mass attention to a problem so that a solution is demanded. They work with other policy actors in the policy stream to develop their preferred solution to the identified problem. They work to shift the political landscape, such that policy and lawmakers are persuaded to adopt their solution. 

### Conclusion

In the end, the flexibility of the MSF metaphor, and the relatively low barrier-to-entry for scholars to understand policy through the MSF framework has made it one of the most popular and useful ways to examine policy (Cairney & Jones, 2016). Though the framework is most often applied in a positivist analytical tradition, recent work combining MSF and discourse analysis “demonstrates that a post-positivist perspective holds great potential for enriching the MSF theoretically and strengthening it analytically” (Winkel & Leipold, 2016, p. 108). The ease of use of the MSF and its associated limited empirical usefulness stands in contrast to the advocacy coalition framework (Sabatier & Weible, 2007) or the institutional analysis and development framework (Ostrom, 2011), both of which harbor complexity and at least some predictive qualities.

## The Narrative Policy Framework

Most of the policy process theories covered by in the Theories of the Policy Process (Sabatier & Weible, 2014) could be considered positivist approaches to questions engaged in policy scholarship. One clear exception is the Social Construction of Target Population (SCTP) theory (Schneider & Ingram, 1993), which takes a critical approach to language used in policy contexts to understand how some social groups are advantaged and others are disadvantaged.  An enduring critique of SCTP has been that as a post-positivist approach, it does not offer falsifiable hypotheses. 

The early 2000’s saw purposive effort to develop the Narrative Policy Framework (NPF), which clearly borrows from SCTP the insight that policy debates are socially constructed. Early NPF work beginning in 2000 was originally critiqued by established policy scholars such as Paul Sabatier, who pushed the originators to be “clear enough to be wrong” (for historical tracing see McBeth, Jones, & Shanahan, 2014; Shanahan, Jones, & McBeth, 2018, p. 332). To that end, “inspired by postmodernism and the seemingly contradictory charter of science” (Jones, 2018, p. 724), NPF aimed to produce work that could stand up to this Popperian critique (Popper, 2005). By 2010, the was formalized as a “structural account of narrative” which sought to “test the influence of policy narratives on policy processes, designs, and outcomes at three different levels of analysis” (McBeth et al., 2014, p. 227). By 2013 the Policy Studies Journal held an NPF focused symposium featuring tests of the framework, and in 2014 the NPF held its place in the third edition of Theories of the Policy Process (McBeth et al., 2014; Sabatier & Weible, 2014). 
The historical context above is important, and it helps illustrate how the NPF was born from post-positivist instincts but adopted to the positivist, falsifiable demands of the policy process academy. Proponents of NPF argue that the admixture of positivist/post-positivist scholarship renders critical discourse studies and other poststructuralist concepts normally “outside the realm of empirical study” (Jones & McBeth, 2010, p. 329) into a scientific approach “clear enough to be wrong” (Shanahan et al., 2018, p. 332).  

### Elements of the Narrative Policy Framework 

The NPF was proposed as a “quantitative, structuralist, and positivist approach to the study of policy narratives” (Jones & McBeth, 2010, p. 330). NPF scholarship borrows from a wide variety of academic fields including discourse analysis, rhetoric, and critical literature studies (McBeth et al., 2014). NPF builds on what it borrowed, overlaying a structuralist framework to narratives in order to give a common theoretical language for policy scholars.  

The structuralist approach of the NPF defines a narrative as having four core elements – setting, characters, plot, and a moral. A setting is the policy context that a policy problem is situated in, such as the “legal and constitutional parameters, geography, scientific evidence, economic conditions, agreed-upon norms, and other features” which constitute the policy arena (McBeth et al., 2014, p. 228). NPF requires at least one character (but this is a minimal requirement which is almost always surpassed. The characters in NPF can be victims, heroes, or villains. The plot in policy narratives has a beginning, middle, and end, and serves to connect the characters to each other and to the policy problem. Finally, policy narratives have a plot, which serves to promote a particular policy solution favored by the coalitions promoting the narrative. 

Evolving from the structuralist definition of narrative described above, the NPF approach makes five assumptions (McBeth et al., 2014; Shanahan et al., 2018). First, important elements of the policy process are socially constructed, a clear lesson drawn from SCTP. Second, those social constructions vary boundedly, not randomly, and have temporal stability. Third, policy narratives have generalizable structural elements, which allow for the social constructions to be measured. Fourth, policy narratives occur at three levels – micro, meso, and macro, corresponding to the individual, group, and cultural and institutional levels. The fifth and final assumption of the NPF approach is that narratives play a central role in how humans communicate and cognate, known as the “homo narrans model of the individual” (Shanahan et al., 2018, p. 333). 

From these five assumptions, NPF scholars constructed a long series of propositions and hypotheses for each of the three levels of analysis. McBeth, Jones and Shanahan (McBeth et al., 2014, pp. 234–246) describe the set of hypotheses: five at the micro/individual level (table 7.2, p. 234), nine at the meso/group level (table7.2, p. 244), while the macro/institutional hypotheses remain, as yet, unspecified. Given the nascent nature of NPF scholarship, the authors concede that the majority of even the specified hypotheses remain completely untested, and all hypotheses lack broad, consistent testing. Despite its structuralist nature, and the effort to adapt to Sabatier’s positivist requirements through the integration of “behavioral, evolutionary, cognitive, and neuroscientific theories in understanding the public policy process,” (McBeth et al., 2014, p. 256), the NPF still requires a great deal of empirical testing. 

### Applying the Narrative Policy Framework. 

The list of assumptions, core aspects, and multiple hypotheses of the NPF can be overwhelming, though Michael Jones (Jones, 2018, p. 726) warns not to be turned off by the seeming complexity, because “at base, the NPF is as simple as the differences” in two stories. The idea that NPF’s ‘complexity’ could turn away policy process scholars is perhaps overblown, particularly in the face of competing frameworks such as the Advocacy Coalition Framework (Pierce, Peterson, Jones, Garrard, & Vu, 2017; Sabatier & Jenkins-Smith, 1993; Sabatier & Weible, 2007) or Institutional Analysis and Development theory (Ostrom, 2011), both of which offer at least equally complex, if not more, takes on public policy processes. The relative ease of applying the NPF approach to policy questions may help explain why, despite only a few years passing since NPF was formalized, there has already been fairly robust empirical application. 

Merry (2018) is perhaps the exemplar of baking in the SCTP origins with the quantitative aspirations of NPF with her study of gun control policy in the United States. Merry investigates the gun policy narratives of gun control advocacy coalitions by examining over 58,000 Facebook posts of fifteen different policy groups. Her analysis shows how the racial aspects of gun control are ignored by both coalitions for and against gun control, an interesting finding which NPF is well poised to explain with its ‘hero’ and ‘villain’ concepts. Merry’s findings suggest that the objective policy ‘realities’ are distorted by the policy narrative demands.  

Gottlieb, Bertone Oehninger, and Arnold (2018) use a case study approach to the hydraulic fracking policy debate in New York over a four-year period. The authors findings challenge the existing NPF hypotheses related to the “devil shift” and “angel shift,” (Weible, Sabatier, & McQueen, 2009) an important study outcome for NPF’s falsifiability. This challenge to a key concept within NPF, a concept which was conceived of by some of the most senior, respected scholars in public policy (Weible et al., 2009), is important. It shows that the NPF scholar community is not wed permanently to its origins, and is willing to move and adapt as needed and as demanded by empirical tests of its predictions. 

Continuing the trend of combining insights from NPF with better established theories of the public policy process, McBeth and Lybecker (2018) investigate the policy narratives of so called ‘sanctuary cities’  in the United States. The authors combine NPF insights with elements from Multiple Streams Analysis (Jones et al., 2016; Kingdon, 1995; Zahariadis, 2014) using a mixed-methods case exploration. The findings demonstrate the value of combining elements from the much better studied Multiple Streams Analysis (MSA) into NPF, especially in policy cases where the researcher is interested in agenda setting. The authors also contribute to the ongoing formulation of NPF methodology, critiquing the difficulty of temporal scope. Narratives have internal structure (beginning, middle, and end) but when and how are the narratives themselves born, age, or die? Through their study, McBeth and Lybecker (2018, p. 868) demonstrate it is “difficult to determine how long a policy narrative remains powerful and relevant,” a methodological limitation of MSA as well in regards to a lack of clarity on how to define a policy window, let alone its beginning, duration, or end point. 

### Conclusion 

If a framework is to meet Sabatier’s requirement of “clear enough to be wrong,” then, at least sometimes, its predictions should be wrong. NPF scholars have not flinched from that demand. In particular, Gottlieb, Bertone Oehninger, and Arnold’s (2018) study of sanctuary cities undermines the devil/angel shift hypotheses which are key to the original NPF formulation. The fact that the NPF community of scholars is willing to absorb such major reworkings shows its strength and commitment.  

By taking on the importance of storytelling to the human experience and applying it in a structured, testable manner to the policy process, NPF is a valuable approach for researchers. Like it’s close cousin ACF, that value is strongest, so far, for case studies within an environmental policy frame, with empirical tests of both approaches mostly taking on environmental policy debates (Jones, 2018; Pierce et al., 2017). 

That said, NPF has broadened its policy scope with the examples of gun control policy (Merry, 2018) and sanctuary cities (McBeth & Lybecker, 2018) in the latest highlighting of the NPF in Policy Studies Journal, which included works on other policy areas as well (Jones, 2018). With an identifiable Lakotosian scholarly community (Lakatos, 1976), the NPF is well poised to address the critiques and gaps identified already. Key among these will be enhancing the lackluster meta level of policy narratives, which NPF takes as a key component but so far lacks accepted predictions, and thus falsifiability (Peterson, 2018).  


## Advancing the Narrative Policy Framework

> Another NPF essay here, this time expanding the "where to go next" with the theory. You may have noticed I have quite a few NPF essays, and as it turns out I was able to use much of it in the comprehensive exam. One lesson - if you have a theory you are interested in, try to develop a few different length pieces around it. That gives you a lot of flexibility to make it the focus of an answer, or just a little extra bit of theory when you're really focused elsewhere.

### Introduction

The Narrative Policy Framework (NPF) is the latest approach in the policy process literature to gain significant adoption by researchers. The NPF was developed as a “quantitative, structuralist, and positivist approach to the study of policy narratives” (Jones & McBeth, 2010, p. 330). This policy process approach asserts that  “narratives (or stories) are primary mechanisms by which individuals process complex information and communicate about events and issues” (Merry, 2018, p. 749). In other words, the NPF recognizes that the stories people tell, and are told, are critically important to understanding how policy is adopted. NPF scholarship borrows from a wide variety of academic fields including discourse analysis, rhetoric, and critical literature studies (McBeth, Jones, & Shanahan, 2014). NPF builds on what it borrowed, overlaying a structuralist framework to narratives in order to give a common theoretical language for policy scholars. 

The paper proceeds in three sections. First is an introduction to the NPF through a literature review,  including the explicit assumptions of the approach. Complementary discussions around media framing and scope of conflict in the policy process give rise to a set of preliminary hypotheses. Second, the sampling strategy is discussed, with attention to the time frame and source material justifications. Finally, three possible analytic strategies are considered. This section is long, as it traces deep into the theoretical proffer of NPF in order to develop specific analytic tests of implicit assumptions that have not yet been tested. This section offers substantial potential contributions to overall NPF scholarship, as both theory testing and theory building outcomes are possible, or even likely. 

There are a handful of essential assumptions and research decisions to be made before engaging with the NPF on a research topic, as the approach “is not a one-size fits all for projects centered on narratives” (Shanahan, Jones, & McBeth, 2018, p. 333). There are several strong assumptions built into NPF which must cohere with the aims of the research question before proceeding. 

### Narrative Strategies and Hypotheses

Stories count, but how do we count stories? The Narrative Policy Framework (NPF) allows the researcher to go beyond evaluating a news article as positive/neutral/negative, a common operationalization in framing studies that can be sensitive to rater bias. An NPF study is interested in how that information is transmitted through narrative, rather than if an opinion is transmitted. Through operationalization of characters, setting, and plot, an NPF study can surface commonalities in narratives even when the authors have opposing views. Stories are either told well, or poorly and investigating the differences in how stories are connected to successful policy initiatives is the goal.

The five assumptions of the NPF are worth making explicit (McBeth et al., 2014). First, the approach assumes that the policy reality we perceive is socially constructed, rather than objectively true. This assumption reflects the influence of Schneider and Ingram’s (1993) Social Construction of Target Population (SCTP), another policy process approach. Second, NPF scholars recognize that those socially constructed policy realities have meaning which necessarily varies, but in boundedly rational ways. The meaning has stability over time, can be measured, and is not random – an assumption that has yet to be tested, and which I address at some length in the analytic plan section of this paper. The third assumption is that narratives have a general structure which can be identified, such as including characters and plot. The fourth assumption is that narrative has effects at multiple levels (micro, meso, and macro) and that narrative interacts across these levels. The final assumption could be considered a meta-assumption, and is said to be the “homo narrans” model of the individual. Homo narrans assumes that narratives play an important role in how humans understand the world. In other words, that “people prefer to think and speak in story form” (Shanahan et al., 2018, p. 333). 

As noted above, the NPF includes three levels of analysis – the micro, meso, and macro. The micro level is concerned with the effect of narrative strategy on how individuals perceive policy choices (McBeth et al., 2014). The meso level is focused on broader narrative strategy by policy actors, and understanding how those actors build those narratives. This is the level of analysis of the present study. Finally, a macro level of analysis is proposed by the NPF, though at this time no published work operates at this level. The macro level has also been synonymized with “meta-narrative, grand narrative, and master narrative” (Shanahan et al., 2018, p. 341). 

The structuralist approach of the NPF attempts to surface the most essential elements of a narrative and make them quantifiable. The structure defines a narrative as having four core form elements – setting, characters, plot, and a moral. A setting is the policy context that a policy problem is situated in, such as the “legal and constitutional parameters, geography, scientific evidence, economic conditions, agreed-upon norms, and other features” which constitute the policy arena (McBeth et al., 2014, p. 228). NPF requires at least one character, but this is a minimal requirement which is almost always surpassed. The characters in NPF can be victims, heroes, or villains, and importantly for this study, need not necessarily be human characters as “non-human characters retain their character status in most NPF applications” (Shanahan et al., 2018, p. 335). The plot in policy narratives has a beginning, middle, and end, and serves to connect the characters to both each other and the policy problem. Finally, policy narratives need a moral, which serves to promote a particular policy solution favored by the coalitions promoting the narrative. 
NPF presumes that these four core elements are generalizable across contexts, and can be operationalized. The coding schema used in content analysis is relatively stable, and a full rendering can be referenced in Shanahan and her co-author's appendix (2018, appendix A, pp. 343–344). The coding strategy outlined here closely adopts that schema. The standard narrative elements identified within the NPF scholarly community are listed along with corresponding potential body-worn camera examples in Table 1, located in the appendix to this paper. 

### Framing Effects in Media

The study of framing effects in media has a long research tradition (Goffman, 1974; Iyengar, 1991; Kahneman & Tversky, 1984), and the findings are essential for understanding why media narratives are an appropriate source of data in NPF studies. The choices made in narrative structure by policy actors are communicated through the media, and “reflect embedded policy beliefs and strategies that seek to influence readers’ policy preferences” (Shanahan, McBeth, & Hathaway, 2011, p. 378).

The linkage between framing and narrative policy analysis is clear, as the importance of story is highlighted in both. However, as Jones (2018) notes in his recent review of NPF studies, more needs to be done to understand how the two intersect. The most common understanding of framing comes from Gamson and Modigliani (1987, 1994), who define frames as the “central organizing idea or storyline that provides meaning to an unfolding strip of events, weaving a connection among them.” Frames are communicated by many sources, but mass media outlets are key actors in this process (Scheufele, 1999; Zaller, 1992). 

How members of the public form opinions has been a consistent source of scholarly work from the earliest days of political research. The consensus that emerged is that the ideal of high-quality opinion is “stable, consistent, informed, and connected to abstract principles and values” (Chong & Druckman, 2007, p. 103). This ideal is rarely found among the general public. Because people are not capable of perfectly locating, absorbing, understanding, or recalling information (Simon, 1972; Zaller, 1992), they rely on information shortcuts in forming opinion, and use frames to develop “a particular conceptualization of an issue” (Chong & Druckman, 2007, p. 104). 

An illustrative example of framing effects is found in the policy of providing federal funding for local police departments in the US to purchase body-worn cameras (BWCs). In 2015, as funding implementation decisions were considered, the question of BWCs did break down into a partisan policy question. Elite policy actors from across the political spectrum supported adoption of the cameras. President Obama, a Democrat president, supported BWCs as a solution, and matched his vocal support with millions in federal funding to police agencies to purchase the technology (Office of Public Affairs, 2015). Donald Trump, then a Republican primary contender, supported BWCs and believed they “can solve a lot of problems – period” (Jacobs, 2015, para. 3). Hillary Clinton, Donald Trump’s eventual opponent in the 2016 presidential race, agreed, making federal funding of BWCs a key point of her criminal justice policy platform (Clinton, 2015) which she believed were capable of “improving transparency and accountability” (Laughland & Gambino, 2015, para. 11).

This bipartisan agreement on significant a policy proposal is rare enough, and rarer still in a period of increased partisanship during a presidential campaign. However, this does not mean media framing is not a factor in the BWC policy story. In their analysis of Newsweek stories from 1975 to 2008, Wagner and Gruszcynksi (2016) show that framing tends to affect attitudes towards policy but not partisanship. As such, bipartisan support of BWC policy (in this case federal funding) serves as evidence of a policy topic on which attitudes can be affected, even (or especially) when no partisan differences are detected. Non-competitive environments are where most is known about framing effects, while research into how framing works in competitive elite environments lagged behind (Chong & Druckman, 2007). While this is a deficit in the broader framing literature, it does not impact this study of narrative effects on BWCs policy, which was situated in a non-competitive partisan context.  

Policy process theories have long been concerned with the role of public opinion on policy formation. The advocacy coalition framework places policy belief as a core component of analysis (Jenkins-Smith, Silva, Gupta, & Ripberger, 2014). Similarly, scholars working in the punctuated equilibrium approach generally follow Kingdon’s (1995) conceptualization of the media’s effect on public opinion through agenda setting. In this view, the effect of the media is most potent at the agenda-setting level, rather than on individual-level policy preference. In other words, media effects are rarely expected to change individual opinion on policy preference. Instead, the media acts to constrain policy choices through agenda-setting effects. In an alternative theory of the policy process, the advocacy coalition framework (ACF) originally conceived of public opinion as an unimportant force within policy subsystems (Sabatier & Jenkins-Smith, 1993), while the opinion of political elites was the center of subsystem power. This early view has evolved; depending on the policy context, public opinion can play a central role as an external shock to coalitions within the subsystem, or even as an internal force (Shanahan et al., 2011).

Media reports are among the most common source of narrative data in NPF studies (McBeth et al., 2014; McBeth & Lybecker, 2018). Media accounts, including newspaper articles, often contain “embedded policy beliefs and narrative framing strategies,” meaning they do not just convey factual information, but instead act as “more of a contributor than a conduit in the policy change process” (Shanahan, McBeth, Hathaway, & Arnell, 2008, p. 115). Newspaper stories and editorials are a common source of data in NPF studies (McBeth & Lybecker, 2018; Shanahan et al., 2008), and more generally across social science studies interested in media effects (Edy, Althaus, & Phalen, 2005; Lecheler & de Vreese, 2012; Matthes, 2009). 

### Scope of Conflict

NPF proceeds from the assumption that public opinion, at the least, plays a part in policy formation, and that opinion is affected through media framing. The framework differs in focus however, as it seeks to quantify not whether media narrative framing is important, but rather how it is done (McBeth et al., 2014). At the meso-level, a strategic reason that policy actors leverage narrative is to either expand or constrict the scope of the policy conflict. 

Long understood as a political strategy (Schattschneider, 1960), the concept of scope of conflict is explained in the NPF as “a narrative strategy that distributes the costs and benefits of a proposed policy to the array of characters in the policy narrative” (Shanahan et al., 2018, p. 337). The dominant group benefiting from the status quo policy environment will generally attempt to limit the scope of conflict, while the policy coalition or actors who perceive themselves as less powerful will attempt to expand the scope of conflict. The winning coalition is generally the one that is most successful in concentrating the costs (typically on the villain) while diffusing the benefits. 

In Professor Merry’s research on narratives in gun control policy advocacy organizations (2018), she notes the link between social construction policy theory (Schneider & Ingram, 1993) and the NPF. Merry predicts that policy advocates will select and construct characters in their narratives in order to influence how a policy issue is framed. In other words, policy actors will use policy narratives to “evoke sympathy” and “might highlight victims who are positively constructed, such as children” (Merry, 2018, p. 751). This is another tactic in an overall strategy of conflict expansion, as actors seek to grow their coalition by appealing to compassionate onlookers by highlighting positively constructed victims. 
A second conflict expansion strategy is when actors seek to translate a “public problem into a personal threat” (Goss, 2010, p. 107) by increasing the perceived proximity of a problem for the reader. This strategy is intended to bring in support from those who might otherwise feel a policy issue does not affect them. Proximity can be literal, in which the study would expect actors to highlight a problem in the community of the intended audience. It can also be figurative, in which “framing choices evoke a feeling of closeness to a problem” (Merry, 2018, p. 750). 

### Analytic Strategies

In a recent review of progress in NPF studies, Jones (2018) highlights the relative sameness in NPF research designs. The prevailing NPF analytic method was a mix of descriptive statistics and measures of association, “and, of course, plenty of regression analyses” (Jones, 2018, p. 734). This analytic monotony is an understandable feature of early NPF work as scholars attempted to build out and justify the framework. Recently, NPF scholars have begun to broaden the analytic scope, with the use of rare event analysis (Kirkpatrick & Stoutenborough, 2018) and causal mediation analysis (Zanocco, Song, & Jones, 2018). Network analysis and in the inclusion of big data have been suggested as potential next steps for researchers looking to expand the analytic toolkit of NPF (Shanahan et al., 2018). For this study, in addition to the orthodox regression analysis, I propose a further way to contribute to the testing of the NPF through the inclusion of structural equation modeling (SEM).

While SEM is capable of directly testing the same null hypothesis as logit regression, it can also test hypotheses constructed to test indirect paths of causation. Indirect causation is an especially salient question for NPF, which assumes that narrativity is a dynamic process, but which is generally only tested in a linear regression formulation. Those tests assume a closed causal space and one-to-one interactions on the causal path between independent and dependent variables. For example, perhaps both heroes and villains are important (a hypothesis supported by current NPF theory), but that narrativity increases only when a villain is accompanied by a hero. This logic is an example of a dynamic process not testable with linear regression, but one that is well-fit to the analytic domain of SEM.

### Linear Regression

Regression is the most common analysis done in NPF studies. The approach is well-fitted to this study for several reasons. For those studies moving beyond the mere description of narrative elements, is it the most common method to establish (arguably) causality. Given its widespread use in the social sciences, regression is an easily interpretable and familiar method. As a first analytic stop, using linear regression has much to recommend it here as well. Given its extensive use in NPF studies, scholars are unlikely to object to its underlying assumptions, usefulness, or explanatory power. In a non-specific example, a regression design would seek to test a model where a dichotomous outcome (conflict expansion=1, conflict containment=0) is explained by some combination of character (villain, victim, hero), setting (nominal), and moral (proPolicy=1, antiPolicy=0) formulation. 

However, there are a number of limitations to linear regression in an NPF study as well. Linear regression has several strong assumptions which must be met before its output can be relied upon. Among these is an assumption of independence for the explanatory variables. For example (and as discussed in much more detail in the SEM section below), the NPF presumes that the core elements of narrativity can be collected into a ‘narrativity index.’ If one attempts to model that index with a regression, with the core components as a set of independent variables, then one is assuming that plot, setting, characters, and setting (at least) are completely independent of one another. This is a heroic assumption, and not one we should be willing to move past easily given the dynamic nature of story elements implied by the NPF. 

Further limitations to consider include the difficulty of interpreting logit coefficients (as would be required in a policy win/loss outcome), and limited interpretability. What does it mean that relative density of characters increases ‘narrativity index’ exactly (see Shanahan et al., 2018, p. 337)? What would a non-significant result mean? And finally, with regression the core analytic choice of many NPF studies, we should consider that a lack of methodological triangulation to test the theory which undergirds the NPF either explicitly or implicitly has limited the overall reliability of its predictions.

### Structural Equation Modeling (SEM)

Regression methods are hampered by the inability to easily identify measurement error. This is a salient problem within the social sciences broadly, but specifically for research within the NPF framework, which relies on data collection techniques that are vulnerable to bias. Structural equation modeling (SEM) is a second-generation statistical technique that allows for both identification of measurement error (and therefore, correction), but also takes into account the indirect effects of endogenous variables within the model (Kline, 2015). These analytic benefits make SEM a “significant and indispensable tool for empirical researchers” (Tarka, 2018, p. 338), and the method is widely applied across scientific disciplines.

Taking an SEM analytic approach to an NPF study has several identifiable benefits. First, one assumption of linear regression is that the explanatory variables are independent from one another. Because NPF assumes the narrative elements are not only related, but dynamic, regression is at a basic level an inappropriate test for NPF models. SEM can handle non-independent causal variables, a clear advantage in this case. 
A second advantage of SEM is related to measurement error. Researcher supplied measurement error is unavoidable in a method such as NPF. This bias is baked into the very assumptions of the approach, especially the homo narrans base of the approach. Stories universally influence how humans (including the researcher subgroup of humans) perceive and understand their world. This influence can lead to inter-coder bias as data collection proceeds. Even an experience, careful researcher coding the narrative content of newspaper stories is likely to (at a minimum and in the best case) supply a bias in how structured narrative content is encoded. 

Intercoder reliability is not a balm either, and in fact, may exacerbate measurement bias. For example, a graduate student or research assistant can reasonably be assumed to harbor the same rating bias as the principal researcher who trained them. In such a case, high intercoder reliability can be interpreted as, at least in some cases, highly reinforced systemic mismeasurement. While SEM does not magically do away with mismeasurement, but the relaxed assumptions and ability to measure latent error could be useful in the specific case of narrative content.
Finally, SEM’s use of latent constructs is an ideal modeling technique to test foundational NPF theory. Latent constructs are a way of measuring the unobservable latent character of complex social phenomenon. Any individual question or observable item is subject to bias and mismeasurement, or likely does not capture the full meaning of a social phenomenon. However, through the use of several observable measures, each of which captures an aspect of the overall unobservable latent character of the phenomenon being studied, a researcher can be more assured of the validity of the overall measure. 

### Applying SEM to the NPF

What constitutes a policy narrative? In the specific case of NPF, “narrative” is the unobservable social phenomenon, and SEM offers a unique contribution to both the overall framework of the NPF approach and the policy process theory environment as well. The framework assumes this unobservable measure “narrative” is comprised of several underlying, unordered, but observable constructs – the four core form elements of setting, characters, plot, and moral. The elements come together to form a ‘narrativity index.’  This index is assessed to “understand the robustness of any given narrative or set of narratives” (Shanahan et al., 2018, p. 337). However, “the effect of higher or lower narrativity is yet to be known (p. 137).

I suggest an analytic pause before turning to assess the effect of narrativity, and a step back to assess what is even meant by ‘index’ in this framing. It appears that the implicit assumption is that the elements are not only essential but essential at the same level. The plot, therefore, is just as critical to narrativity as moral, characters, or setting. Perhaps, but this far from settled within the NPF literature, as indicated by the advice that “policy narratives may include all or some narrative components” (Shanahan et al., 2018, p. 336).

The implied index formulation is akin to a stew recipe, where the overall outcome is relatively invariant to the order, amount, or prevalence of ingredients. Scholars generally agree that at a minimum it must have at least one character, and must refer to a public policy, but past this minima, the recipe is a dash of plot, other characters, setting, and moral as needed. Continuing the metaphor (perhaps a bit too far), if NPF is to enhance the replicability and rigor of a ‘narrativity index,’ then it needs a baking recipe, with more exactitude in types, amounts, and timing of the ingredients.

In the grammar of SEM, this narrativity index recipe forms an implied covariance matrix. A diagram of the implied covariance matrix would appear as shown in Figure 1 below. In the diagramming syntax of SEM, an oval shape is a latent construct; rectangles are observed items; and small circles are error terms for observed items . Directional arrows from observed items to latent constructs denote a regression-like relationship. Following model resolution (discussed below), path coefficients would be calculated for the weight of each observed variable on the latent construct. These coefficients can be interpreted much like linear regression coefficients. They can denote a positive or negative relationship, and each path can be significant or non-significant at an alpha level specified by the researcher. In the ideal case, each path in this test would be significant and in a positive direction.

#### Figure 1: Base Implied Covariance Matrix for the ‘Narrativity Index’
 
Importantly, Figure 1 should not be construed as representing the only implied covariance matrix implied by the NPF approach. Instead, it is the most likely given the current assumptions and structural considerations of NPF. The NPF is “open to alternative definitions operationalizing the structures of a policy narrative” (Shanahan et al., 2018, p. 335), and so alternative model specification is possible. However, given that even the base NPF model has not been tested in this way, at this time the four core elements of narrativity are adequately encoded in the first diagram. It is easy to consider and diagram a multiplicity of other models of narrativity which take into account the possibility of indirect relationships as well. For example (Figure 2): What if variance in setting strengthens or weakens the influence of a character type on how robust a narrative is (indirect path ‘A’), while simultaneously exerting a direct influence (path ‘B’) on narrativity as well? SEM is able to solve these equations simultaneously in the implied open solution space.

#### Figure 2: Hypothetical Indirect Implied Covariance Matrix

This paper noted relatively early that NPF explicitly assumes bounded rationality in narratives. This assumption suggests that narrative has stability over time, can be measured, and is not random. Another way of describing that stability of measure is ‘measurement invariance’ (Nesselroade & Cattell, 2013). A full discussion of measurement invariance is beyond the scope of this paper. However, the key idea is that by modeling datasets from a variety of times, contexts, and sources, we test the underlying model for measurement artifacts. This provides a check that any change we are seeing in the model output is due to actual underlying changes in the data, rather than some other source (Boker & Laurenceau, 2006; Newsom, 2015). Academic psychology pioneered much of the work in longitudinal modeling to detect measurement variance at the inter- and intra-individual level of measurement (Nesselroade & Baltes, 1979). The application of SEM to the problem of measurement variance is considered state-of-the-art methodology (Deboeck, Nicholson, Kouros, Little, & Garber, 2015). Relatively few published articles consider measurement invariance testing directly, beginning around 2011. In the case of testing NPF models of narrativity, SEM techniques can ensure measurement invariance (or diagnose variance) by conducting a series of tests that impose equality constraints to establish parameter stability. If done well, this could result in a genuinely reliable narrativity index which far surpasses the current ‘stew stage’ of recipe development. 
Note that Figure 2 above does not require a change in variable or operationalization of the model shown in Figure 1, though a hypothesis reformulation is likely required. SEM is imbued with natural flexibility that allows the researcher to consider a wide variety of narrativity index models. Further, once a likely candidate model is identified, it can be tested across policy contexts. If the model holds, we can begin to isolate the relative weighs of each component and path, allowing for even more theory building. For example, it may be characters (or plot, or setting) to are ‘worth’ more narrativity. In that case, experimental designs with narratives designed with more or less robustness (macro level) could be tested for their (micro-level) effects on the formation of policy belief in individuals. However, we are now far afield in the what-if scenarios, which all hinge on first establishing configural validity for the basic model illustrated in Figure 1.

### Model Testing

Progressing from the specification of the implied covariance matrix, SEM then tests the ideal model against the observed data matrix (known as the sample covariance matrix) using maximum likelihood estimation (MLE). This estimation attempts to maximize the likelihood of the observed data given the parameters encoded in the implied model. In other words: Does what we observe in a collection of observed data on the underlying story elements fit well into the unobservable latent construct of ‘narrative’?

Through the use of omnibus goodness-of-fit statistics, such as RMSEA, CFI, TFI, and more, the researcher can compare how closely the observed data fit the implied model. Guidelines for assessing the fit of an SEM model as specified is relatively straightforward (Hooper, Coughlan, & Mullen, 2008). While model assessment is an active area of debate among scholars , the analysis proposed here can safely be considered among the most well-trodden and least controversial SEM methods (Kline, 2015; Marsh, Morin, Parker, & Kaur, 2014).

### Drawing Conclusions (from drawings)

Several conclusions might follow the above analysis, which is a type of confirmatory factor analysis (CFA). First, the observed covariance model could obtain very close fit with the implied covariance model. In this case, the structural elements of narrativity would be shown to be just as the NPF has assumed. This outcome is the most likely, given that the structural elements of NPF are the result of careful research which has been developed by talented scholars for many decades. The NPF was, at least in part, developed with the transparent goal of meeting policy process scholar Paul Sabatier’s requirement that a theory possess a clear model specification and be “clear enough to be wrong” (Jones & McBeth, 2010; Sabatier, 1999; Shanahan et al., 2018, p. 332). Having only recently being accepted into an important policy process handbook (Sabatier & Weible, 2014), the approach detailed here offers a real possibility for method triangulation in support of the still-growing NPF. 

Alternatively, if the two covariance structures are incompatible, essential assumptions of the NPF approach can fairly be said to have been falsified. In that case, assumption three of the approach would require significant interrogation. Assumption three is that narrative has generalizable structural elements, and building on that assumption NPF models the four core elements of plot, setting, characters, and moral as the quantifiable elements.  To undermine that construction would be to undermine foundational aspects of the NPF. However, before drawing such a damaging conclusion, the limitations of the analytic tests must be assessed.

### Limitations

There are three obvious limitations to consider, though these are only three limitations that would have to be considered and are not intended to be an exhaustive list. Applying and testing the NPF is “more of an iterative venture, with some ideas developed concurrently and not necessarily bound” by unyielding order. In that spirit, the identified limitations at this proposed stage are likely to be joined by others as the research process takes place.

In the case of poor model fit, there are several sub-analyses which would be required before drawing firm conclusions. First, the observed data would need to be assessed. Perhaps the specific sample here – a New York Times content analysis of body-worn camera stories over eight months – does not provide an adequate testbed for the NPF model. This weakness is protected against somewhat by the number of NPF studies which have drawn substantive conclusions using newspaper articles. However, because of the reliance in SEM on the observed covariance matrix, if the underlying data is a priori not a good test of the implied covariance matrix, nothing interesting has been learned. If the sample does not provide an adequate corpus for testing, several options are available. A separate narrative source on the same topic, and in the same timeframe, could be obtained.
A second limitation might be the implied model. Some authors have opined that perhaps only two elements, a character and a public policy, are needed for a narrative. These character in a narrative is a “foundational narrative element” that “differentiates a narrative from a non-narrative, such as a chronology or a report” (Shanahan et al., 2018, p. 336). Perhaps, then, the observed covariance matrix is mis-specified, and the latent narrativity construct should contain only two observed items based on two questions: (1) Does a character exist in the article? And (2), does the article pertain to public policy? In such a case, SEM would not be an appropriate analytic tool, because of the vulnerability of two-item constructs. If this conclusion was supported, it would still be a worthwhile contribution to the field, and would require re-specification of the ‘core’ narrative elements model of NPF. 

Related to the above, a third limitation might be a lack of comparison narratives. In their application of NPF to the policy of sanctuary cities, McBeth and Lybecker (2018) select Breitbart News, a more reliably partisan news source, to construct their dataset. The authors then built a companion dataset derived from congressional GOP floor debates. Comparing the two datasets, the study concludes that the narratives used by Breitbart News were able to set the agenda on an important public policy by influencing the policy beliefs of policy elites. 

### Conclusion

Stories count; this is the homo narrans assumption of narrative influence on human activity. The NPF has been successful in providing structure to the assumption such that the relationship between narrative and policy can be assessed. NPF synthesizes the quantitative demands of policy process scholarship with the qualitative instincts of social inquiry interested in meaning-making. To date, the burgeoning NPF evidence base has demonstrated its value to our overall understanding of the policy process. In order to build on the early success of the NPF, I have suggested an analytic expansion is needed to provide clarity and falsification of untested assumptions related to how the approach conceives of a ‘narrativity index.’ 

The NPF’s ‘front-end’ of demonstrating that stories count is more well-developed than the analytic back-end of counting stories. This critique is not intended to underplay the contributions of NPF scholars or scholarship. An academic community must find its grounding in description before it can stretch to inference. It is only due to the determination of early NPF researchers that any demand for more theory testing can be made, as has been done here. 

To its credit, and in contrast to many of the other policy process theories, NPF makes its explicit assumptions clear. Implied, or at least rarely stated assumptions still exist. Perhaps the most untested of the implied assumptions of NPF is that of dynamism. Story elements of plot, characters, setting, and moral are all assumed to interact in some way to produce an index of narrativity (Shanahan et al., 2018, sec. 6.1.4). The same fluidity is presumed for the narrative levels, as micro-, meso-, and macro-level narratives interact with and reshape one another. As demonstrated in the exploration of possible analytic strategies undertaken here, more effort should be given to testing that assumption.

The strongest likelihood is that applying new methodology, or as stylistically applied here, an improved method of ‘counting stories,’ will only serve to solidify the strong assumptions of NPF. Unfortunately, linear regressions are built on strong assumptions which are at odds with the equally strong ones of the NPF. Fortunately, SEM is a well-accepted methodology which can triangulate with the existent research to test and strengthen NPF theory.


## Policy Feedback Theory

In its most general definition, policy feedback theory (PFT) captures the idea that “new policies create new politics” (Moynihan & Soss, 2014, p. 320, quoting from Schattschneider, 1935, p. 288). In turn, the created politics then influence future policy (Moynihan & Soss, 2014; Pierson, 1993). At its best, PFT generates better causal hypotheses in political science (Tilly, 2001) by encouraging political scientists to (Pierson, 2000, p. 264) “think more clearly and explicitly about the role of time, and history, in social analysis.” 

### The Policy Feedback Approach

The ‘feedback’ in policy feedback can take many forms. As Fleming (2014) notes, one simple form of policy feedback used by scholars is public opinion. Once a policy is passed into the public sphere, citizens can “demonstrate their approval or opposition to the effects” (Fleming, 2014, p. 56) of the policy through their votes. Such votes select policymakers who then create more policy, keeping in mind the public opinion of the previous policy outputs. In this way, policy creates politics, politics creates policy, and on ad infinitum. An example of this feedback is how voters’ personal experience with public health insurance regimes affects their attitudes towards Medicare and the Affordable Care Act in the United States (Lerman & McCabe, 2017). Linking personal experience with previous policy outputs to future attitudes towards a similar policy is a crucial feature of PFT. This link also provides a new way to answer to a critical pillar in American political study, which is interested with understanding how public opinion is formed (Mettler & Soss, 2004; Zaller, 1992).

This cyclical repetition is a weakness of PFT, as it implies there is an extensive system from which to draw from. The systemic level of analysis in PFT can present difficulty in choosing which politics and policies to concentrate on. Mettler (2002), for instance, in her study of the effects of the G.I. bill on the political activity and engagement of benefitted veterans, is taking on a feedback loop lasting multiple decades. Such a time lag invites a host of confounding variables which need to be analyzed. In parallel work, Mettler and Welch (2001) analyze some of those potential confounders using a two-stage methodology find similar results (Mettler, 2002, n. 13). So, the long scope is not necessarily a fatal vulnerability but must be dealt with by scholars interested in using PFT. 

Another form of feedback is between different elements of the policy system, as seen in how advocacy coalition framework (ACF) theorists (Sabatier & Jenkins-Smith, 1993; Sabatier & Weible, 2007) incorporate policy feedback theory. In ACF, coalitions of actors who share policy beliefs compete against other coalitions who favor different policy solutions. Two types of shocks, internal and external, can change coalition positions within the policy subsystem. An internal shock might be coalition members changing their minds when new evidence from a previous policy outcome becomes available. An external shock could be a new policy environment created when a policy is passed, giving the competing coalition a political advantage. Both types of shocks are reliant on policy feedback theory, as policy creates new politics and the coalitions react to them, positioning themselves for the next policy battle.

### Policy Feedback Applications

As a concept, PFT stands in stark contrast to much of the political science which came before it, which searched for the big causes resulting in big changes and rooted that search in the political process. In contrast, PFT allows scholars the ability to locate large effects in the small causes, or at least a system of smaller causes, giving a more nuanced and detailed historiological analysis to political inquiry. Pierson (2000, p. 215) predicted that if policy feedback theory was found to be appropriate to social research questions, it would “shake many subfields of political inquiry.” 

Pierson was right, and scholars have since found a great deal to like in PFT, and the concept has found root as a mechanism within many other policy process theories (Fleming, 2014). Both social construction of target populations theory (Schneider & Ingram, 1993), and advocacy coalition framework (Sabatier & Jenkins-Smith, 1993), for instance, use the concept of policy feedback within their larger theoretical explanations (Cairney & Heikkila, 2014). PFT is also key to the underlying approach of punctuated equilibrium, spurred in part by PFT’s assumption that (Pierson, 2000, p. 251) “political development is often punctuated by critical moments or junctures that shape the basic contours of social life.” 

An example of applying PFT to the public administration context comes from a study from Wichowsky and Moynihan (2008). A policy is more than the sum of the technical language passed as law, and “includes the administrative practices of translation and implementation” (Moynihan & Soss, 2014, p. 320). Taken with the policy feedback assumption that ‘policy creates politics’ then (p.320) “implies that administration shapes politics.” Policies affect (Wichowsky & Moynihan, 2008, p. 908)“political participation, social capital, a sense of civic belonging, and self-worth as a citizen.”  In other words, the authors are able to extend the policy-politics cycle to a policy-politics-administration-citizen cycle, with all components affecting each other. While early iterations of policy feedback built to the conclusion that citizens create policy through feedback, acting “as thermostat” (Fleming, 2014, p. 56), Wichowsky and Moynihan show how policies shape citizenship through public administration of those policies. 

While early presidential scholars tended to focus on the personalities of presidents, Skowronek’s (2008) concepts of “presidential time” and “secular time” take into account the temporal context that affects the presidency, a critical assumption of path dependence and feedback approaches. Only when the political and secular cycles are serendipitously arranged, concurrent to a president of sufficient skill taking office does presidential power indeed become strong. Where presidential scholars in the behavioralist tradition argued about a president’s character and temperament, and then drew reductionist conclusions about relative strength or weakness (James D. Barber, 1968; James David Barber, 1974), Skowronek provides a more compelling account of a generally weak presidency by incorporating policy feedback and path dependence approaches. Skowronek’s focus on context and how the institution of the presidency interacts with time on multiple fronts is more explanatory and ultimately more convincing because of feedback concepts. 

### Conclusion

The behavioralist tradition dominated political science through the 1980s, but functionalist theories did not provide a compelling picture of how policy processes interact with time. Emerging from that period, the notion of "path dependence" used extensions from economics but departs from the purely rational choice conclusions which dominated political inquiry through the 1980s (Cohen, March, & Olsen, 1972; March & Olsen, 1984). Using path dependence correctly can support claims from the historical institutionalists that the timing of events matters (Pierson, 1993), and offer a better causal explanation than the competing debates about ‘isms’ which dominated previous political discourse (Tilly, 2001). Policy processes do not end at implementation but continue to have effects on future policy and politics. 


## Punctuated Equilibrium Theory

Punctuated equilibrium theory (PET) (Baumgartner & Jones, 2009; True, Jones, & Baumgartner, 1999) is a policy process theory that borrows from biological science to describe long periods of policy status quo, suddenly interrupted by significant shifts in the policy landscape. Baumgartner and Jones recognized that the slow incremental policy changes predicted by the base linear policy model were not reflected in empirical policy evidence. Rather than slow, steady policy progress, they saw long periods of policy stability, which were then suddenly disrupted by sharp changes in short periods of instability. To the authors, this cycle seemed to reflect the sudden evolutionary adaptations seen in the biological sciences, as species maintain long periods of stability, with swift natural adjustments (Gersick, 1991; Gould & Eldredge, 1977).

### PET: Framework and Considerations

Though more useful than its overly simplistic linear model of policy change, PET is still at its root a linear theory, albeit one with generally more relaxed assumptions about the rational nature of the individuals involved. For instance, PET assumes that people are boundedly rational (Simon, 1976) rather than perfectly rational. Similarly, PE recognizes that policymakers are boundedly rational – they have limited attention capacity, and cannot know everything about a policy issue, problem, or solution. Another critical concept in PE is that of framing (Zaller, 1991, 1992), which groups use to define how a policy problem is understood, in order to better position their preferred policy solution for consideration and adoption by lawmakers. 

Key to PET is understanding how attention is gathered, and by who. Early policy theories assumed electoral processes were the key to policy change, but in the face of policy stability across electoral outcomes, a fresh perspective was needed (Baumgartner & Jones, 2009). Politicians and policymakers are boundedly rational (Simon, 1976), and along with those bounds come restrictions on how much information – attention – those people can give to policy problems. Attention gathers and dissipates without (necessarily) regard to electoral outcomes because policy problems exist before, during, and after elections. Even in policy areas where opposing parties/candidates hold starkly different policy views – a relative rarity itself – the winning candidate does not have the attention or capability to address all policy problems at the same time. Policymaking thus becomes “a continual struggle between the forces of balance and equilibrium, dominated by negative feedback processes, and the forces of destabilization and contagion, governed by positive feedback processes” (Jones & Baumgartner, 2012, p. 3). Policy information feedback itself is an active area of research, even when it does not use PET overtly as a research framework (Mettler, 2002; Soss & Schram, 2007).

Similar to both multiple streams theory and advocacy coalition framework, the PET approach attempts to understand how policy groups operate to bring about policy change. PET uses key concepts such as agenda setting, policy monopolies, and venue shopping to explain how these groups overcome the natural tendency towards stability and continuity in the policy environment. With agenda setting, groups make strategic choices about to bring attention to their preferred policy solutions. If they worry that attention will risk derailing the policy, they will work to minimize attention. At other times, particularly with lawmakers reluctant to pay attention, the policy groups actively manage attention around a policy problem and solution in order to generate political momentum. 

Policy monopolies, which are reminiscent of the “iron triangles” of earlier policy studies (Jordan, 1981) develop in certain policy areas, and like in the advocacy coalition framework (Sabatier & Weible, 2014), these monopolies can persist for long periods of time, as they work to maintain policy that reinforces their access to resources and policy influence. The answer for policy groups who find themselves locked out of a policy monopoly is what Baumgartner and Jones (Baumgartner & Jones, 2009) refer to as “venue shopping.” If a group is locked out of the legislative policy arena, they may choose to change venues and look for ways to pass through their preferred policies at the executive or judicial levels of government.

Punctuated equilibrium theory has much in common with another main approach to the policy process, the multiple streams framework (MSF) (Cairney & Jones, 2016; Zahariadis, 2014). Both MSF and PET are heavily cited and relied upon in the policy process literature, and formed the basis for later, more complex approaches such as Sabatier and Jenkins-Smith’s (1993; Sabatier & Weible, 2007) advocacy coalition framework (ACF). Both PET and MSF benefit from a simplicity lacking in the later frameworks, and both suffer from that simplicity in the form of relatively little predictive power, though PET advocates rightfully point to a better record on this front than the MSF literature can boast. Both PET and MSF can be usefully applied to different policy contexts, and where one might be less useful, the other often provides a better analytic frame.

### Applying PET Across Contexts

Punctuated equilibrium policy theory was developed in the United States, and it provided a useful explanation of empirical policy changes there. Early use of PET theory (Baumgartner & Jones, 2009) was used to explain US nuclear policy, which existed primarily out of public sight in the post-war period. Following decades of that stability, where the policy was left mainly to technical experts and legislative subcommittees, anti-nuclear power advocates were successful in challenging the positive image of the nuclear industry, and venue shopped their policy ideas to courts and the public. The existing policy monopoly was broken, and burdensome regulation of the nuclear industry effectively halted the expansion that had been seen in the post-WWII period. Following the early use in the nuclear policy arena, further testing of PET to predict shifts in nuclear budgets has provided further evidence for the predictive ability of PET in budgetary contexts (Hegelich, Fraune, & Knollmann, 2015).
The US government is structured in a divided power arrangement, which tended to reinforce status quo arrangements, and was seen as primarily responsible for the periods of policy stability. However, PET theory can be usefully applied in non-US contexts as well. A comparative study of policy regimes in the US, Denmark, and Belgium (Baumgartner et al., 2009) using data from “dozens of processes across three nations and covering hundreds of thousands of observations” found the same non-normal distribution of policy inputs and effects. This study provides strong evidence that it is not necessarily the US constitutional system which is providing friction in policy development and thus favoring the status quo. While all three countries in the study are democracies, there are enough structural differences to suggest that a “General Punctuation Hypothesis” can be applied in comparative contexts. 

Comparative study has also contributed to understanding PET in non-democratic contexts such as the People’s Republic of China. Chan and Zhao (2016) find that the authoritarian Chinese government alienates opposition views, leading to a lack of information about problems outside the ‘official’ government view. Without that information, the system is more susceptible to large punctuations, as problems are slow to gather the necessary attention necessary to address them, which “undermines their ability and incentive to make frequent adjustments to the status quo” (Chan & Zhao, 2016, p. 148). When time authoritarian officials finally realize there is a problem, the nature of authoritarian regimes allows them to “undertake radical changes unopposed” (Chan & Zhao, 2016, p. 148). This cycle means that over time, authoritarian governments are more susceptible to rapid, overly large shifts of punctuation after periods of stasis which are longer than they would be if there were more information available. 

### Conclusion

Punctuated equilibrium theory is robust and retains its place as one of the most cited and useful modern theories of the policy process. It provides a framework that allows even non-scholars to connect with the relatively simple idea immediately – things tend to stay the same, until they do not. At the same time, it has enough complexity and flexibility to be adopted in varied political contexts. Flexibility and applicability have been key to its success as an approach to policy process studies. There are no ‘final answers’ available to policy scholars, ‘success’ in this context “centers on the extent to which the idea is fruitful, by which we mean the extent to which it stimulates further research that itself raised more new questions” (Jones & Baumgartner, 2012, p. 1).

What punctuated equilibrium theory may lack in predictive ability outside constrained systems such as budgeting (Flink, 2017), it makes up by offering a relatively low-barrier for both scholars and practitioners to understand critical aspects of the policy process. The stability brought by a disjointed American constitutional regime tends to support stasis – but in the wake of the rapid dissolution of structural and policy factors which supported that stasis, we occasionally see substantial shifts in the policy environment. 

## Social Construction of Target Populations

Developed by Anne Schneider and Helen Ingram (1993; 1997), Social Construction of Target Population (SCTP) theory confronts an assumption of the linear process: that policymakers, and policy itself, are neutral or unbiased actors. Their argument is not itself post-modern but builds on post-modernist critiques of language which deconstruct the power relationships inherent in language (Foucault, 1991, 2005). ‘Foucauldian discourse analysis’ is critical to SCTP (A. Schneider & Sidney, 2009) and has been effectively used by researchers to understand how different approaches to language “contain critical assumptions about how changes in policy relate to broader social change” (Sharp & Richardson, 2001, p. 193). 

### The SCTP Approach

Schneider and Ingram (1993) show that elected policymakers adopt value judgments about the social groups that are impacted by policy programs and that those value judgments have an impact on the policies they create and implement. In this framing of political statements, some politicians will, for instance, use language which implies that individuals living in poverty are lazy and have created their own situation, arguments which may justify policies that withhold government benefits from that group (Ingram & Smith, 2011). However, just as language can justify under serving certain social classes, it can also over-benefit others. The same politician may use language that confers noble, worthy qualities on business owners, which would then serve to justify policies that shift resources to that social class. 

Construction of target populations is not as simple as ‘positive’ and ‘negative’ populations though in the SCTP theory, and Schneider and Ingram illustrate this (Schneider & Ingram, 1993) through their use of a two-axis notional figure, where measures of “positive and negative” constructions are paired with “high and low” perceived power constructions. Power in this use is the ability of a social group to accept or reject the image painted onto them. This conceptualization gives a four-category scheme of “advantaged” (high power/positive), “contenders” (high power/negative), “dependents” (low power/positive), and “deviants” (low power/negative) social groups. 

These simplistic categorizations of complex populations make it easier for politicians and governments to implement policies that over-serve the advantaged, while making it extremely difficult for “deviant” populations to even challenge their disadvantaged status in policy debates. This phenomenon creates a policy feedback loop (Pierson, 2000), as an advantaged group like homeowners not only are over-benefitted regarding resources granted by policy but then can reify their position in the social hierarchy, leading to more opportunity to implement even more policies which will benefit them. This system of self-reinforcing policy benefits is known as the ‘feed forward’ proposition in SCTP theory (Pierce et al., 2014; A. Schneider & Sidney, 2009) creates asymmetries of participation and power, and those asymmetries are reinforced by the system creating and accepting social constructions of the deserving and undeserving. This flaw in the system has long been recognized in political studies (Schattschneider, 1960; Schlozman, 1984), but SCTP offers a useful empirical starting point for understanding how particular social groups have been affected by policy choices influenced by social construction. 

One of the most potent critiques offered by SCTP is showing how these constructed beliefs about social classes not only have immediate effects on resource allocation but have effects on those social classes long after the policymaker has left office. This phenomenon is known as a feed-forward effect (A. Schneider & Sidney, 2009), or in other political science literature as path dependence (Pierson, 2000). The ‘feed forward’ proposition recognizes that the timing of policy choices matters, and policymakers select policies which have self-reinforcing feedback processes (Ingram & Schneider, 2006; Pierce et al., 2014; A. Schneider & Sidney, 2009; Soss & Schram, 2007). These processes represent the resiliency of institutions that are far longer lived than the policy-makers tenure (Sanders, 2006). Path dependency imposes a cost to going “back” to a previous point, and the longer a policy scheme has lived, the higher the cost. In this way, earlier choices have more significant impact than later ones, as the policies themselves shape the institutions that house the policies (Mettler, 2002). 

SCTP offers a compelling critique of policy studies itself, as it uncovers how the language involved in policy can compel certain beliefs and narratives which can hinder, harm, or help certain classes of individuals (Sharp & Richardson, 2001), even as policy scholars unthinkingly use the same language. Further, SCTP critiques the underlying, formative ideals of early public administration and political science, that of the neutral and unbiased bureaucrat, or public administration scholar. A considerable advantage to SCTP is that it invites, or even demands, methodological breadth in studying the policy process. In contrast to much of the policy process and design scholarship, which is focused on large quantitative studies, SCTP “requires interpretive research methods” (A. Schneider & Sidney, 2009, p. 115) that allow the scholar to engage in ‘meaning making’ (Yanow & Schwartz-Shea, 2006) as they reflect on their research practices, their subjects of study, and “the ‘world making,’ the images of reality, the stereotypes people use to make sense” of their reality (A. Schneider & Sidney, 2009, p. 105).

### SCTP and U.S. Historical Context

From the founding of the American political system, the ideal of competing factions balancing the power of any one faction (Dahl, 1982; Madison, 1787) has provided a powerful argument that the U.S. Constitution and division of power among the federal branches would protect minority interests from the powerful machinations of the majority. However, the justifications of the pluralist federalist system were largely imputed by Madison and Hamilton into the Federalist Papers in a post-hoc manner intended to justify ratification of the U.S. Constitution (Peterson, 2012). V.O. Key (Key, 1963) was ahead of his time in noting the effects of sectionalist national politics on state politics and parties, noting that national political tides “spill” into local politics. Party cleavages at the national level project into state elections, with the national party’s need to retain control used to justify the racist practices of the Southern Democrats of Key’s time. This political macro reality can reduce the incentive for local and state politicians to perform well, as they justify their control – and pardon their own structurally biased practices – in the name of national party priorities. Consequently, rather than providing an intricate balancing wheel (Madison, 1787; Rohr, 1986) against the predations of a majority, the administrative and policy state SCTP theory allows us to see how policymakers and lawmakers, at least in some cases, can use the concealed power of language to prolong and protect the interests of the already powerful (Foucault, 1991).

### Limits and Directions of SCTP

One limit of SCTP is that it is less concerned with comprehensive theoretical explanations of the policy process, and so in cases where there are not clear-cut social classes at play, SCTP may be less robust. A limit of early SCTP theory was a lack of direction and application – what should individuals and advocates do with this knowledge? That critique has been substantially, though not wholly, blunted as more researchers have become interested in critical policy theories, and extended the original insights into examining the adverse effects of economic policy concentrated on women and African Americans (Andersen, 2001), food justice (Billings & Cabbil, 2011), and Native American school children (Quijada Cerecer, 2013). While one of the oldest critiques of SCTP is that it lacked empirical application (Sabatier, 1999), a recent review of SCTP theory (Pierce et al., 2014) has identified 123 application and theory building publications.

A second limit is that, at least in the original construction of the theory, little is said about how social classes might contest the ways they were constructed by policymakers and the public. SCTP has little to say about how, or more importantly why, one social group may attempt to help challenge the social construction of a less powerful one. Why for instance, would a feminist group – hypothetically located as a “challenger” in SCTP – want to help restore the voting rights of the formerly incarcerated? SCTP still has buried assumptions of rational choice, presenting the actions of the powerful as merely, or purely, self-interested.  In her study of the gun control policy debates, Merry (2018) posits that argues that one way for the less dominant policy groups (in this case, gun control groups) to gain cohesion and power is through building exactly such cross-cutting alliances. Merry suggests nascent efforts of the groups Cure Violence and Gays Against Guns to “highlight the role of race in gun violence” (Merry, 2018, p. 764) can draw in the effort and interests of groups not traditionally aligned with gun control, such as Black Lives Matter. Such alignments are difficult though, and carry a risk that if the involved groups do not align their policy narratives, the effort can stymie momentum and give their opponents “dominance in the policy area” (Merry, 2018, p. 764). A final related critique of SCTP is the lack of explanation for how various socially constructed groups move from one categorization to another (DeLeon, 2005). SCTP recognizes that such moves take place, but understanding the underlying mechanisms by which such change takes place is an ongoing challenge for SCTP researchers (Pierce et al., 2014).

## Political Participation 1

In response to: 

> The ability of citizens to participate in politics and to influence government action is often considered to be one of the defining characteristics of democracy. Based on the research you have read, what do we know about who participates in politics, how they participate, and to what effect?

### Who Participates?

A core belief of democratic theory is that in order to function properly, a democracy must have citizens able and willing to participate, and that their participation must have the ability to influence the public policy decisions taken by the government. Political participation is at “the heart of democracy,” (Verba, Schlozman, & Brady, 1995, p. 1), and only through participation can democratic citizens “seek to control who will hold public office and to influence what the government does.” However, citizens participate in unequal and varied ways, and their participation doesn’t always equate to policy outcomes. 

In three parts, this essay will offer a brief overview of some of the important literature which informs our academic understanding of political participation. In the first section, the question of “who participates?” will be reviewed, with a focus on the unequal ways in which differing parts of the American electorate participate politically. Following that a review of the differing methods of participation are covered. Finally, the connection between political participation and government action is considered. 

### Unequal Participation

While American’s generally endorse the normative democratic belief that everyone should have a voice in politics, the reality is that there are distinct patterns of participation, patterns which serve to distort that normative ideal (Bartels, 2016). The most important pattern throughout studies of participation is that socio-economic status matters a great deal, and that citizens “with education and money participate; the poor and uneducated do not. As a consequence, the interests of the affluent are well represented in government, and those of the less advantaged are not” (Flanigan, Zingale, Theiss-Moore, & Wagner, 2014, p. 60). Perhaps the most influential scholar of political participation is Sidney Verba, and his work alongside Norman Nie (1972) in Participation in America has shaped how participation is studied and understood. Verba’s work centered socio-economic status (SES) at the center of participation studies. The SES model of participation posits that a person’s income, education, and class status affect their likelihood of (Verba and Nie, 1972, p. 2) “acts that aim at influencing the government, either by affecting the choice of government personnel or by affecting the choices made by government personnel.” SES remains the most accepted, central, explanatory factor in political participation. 

In Pathways to Participation, Beck and Jennings (1982) examine the formations of political participation from a sociological standpoint, using panel data from the second iteration of the survey used by Jennings in earlier work (Jennings and Niemi, 1968). The authors seek to differentiate between types of participation, such as activities, and to show that voting is just one of those methods. They structure four path models, testing effects of parental SES, parental political participation, high school activity, and parental civic orientations, and find each model performs well. Still, their findings show that SES and education continue to have the largest effect. They are successful in finding that education level has a larger effect than income, but the two are highly correlated and work slightly differently in how they affect participation. For the most part, the authors use clear theoretical framing and empirical evidence, with the exception of their finding that high school activity participation has a positive effect on civic orientation. It's not clear that activities lead to civic orientation as they claim, and a reverse causal relationship is just as conceivable. Even with that critique, and despite their theoretical stance, Beck and Jennings findings serve to further reinforcing just how important SES is in the participation literature. 

With SES so clearly in the foreground of the participation literature, some scholars soon turned their attention to the affect of race, which is correlated with SES, on political participation. Race matters broadly for participation. Non-white people, especially black and Latino populations, have less political information. One of the most interesting and replicated findings to come out of Participation in America (Verba & Nie, 1972) is that once SES is controlled for statistically, black Americans are more likely to participate than white Americans. Verba and Nie theorized this was due to a “race consciousness” in black Americans, and found support during interviews where they found that black respondents who mentioned race were more likely to participate than those who didn’t. For African-Americans in particular, race matters in participation, but in voting specifically, this difference is less clear. Once you control for SES, race matters less, but in the full range of political participation, race matters a lot. Most scholarship shows that blacks participate less than whites, and Hispanics less than blacks, though in recent years, black and whites have relatively similar rates of political participation once SES is accounted for.

Bobo and Gilliam (1990) test the race consciousness theory with a hypothesis that black citizens living in areas with black mayors are more likely to participate. While their findings support the hypothesis, their assertion of causal direction is problematic, as it appears just as likely that it is the political participation of black citizens which leads to black political leadership in the community. Further, Bobo and Gilliam’s theory implies a robust time effect, but their reliance on cross-sectional 1987 survey data is unsuitable to test that theory.

Differences in how men and women participate politically is a continuously salient question for scholars. Stanley Verba has, of course, investigated, this time with co-authors Nancy Burns and Kay Schlozman in “The Public Consequences of Private Inequality: Family Life and Citizen Participation” (Burns, Schlozman, and Verba, 1997). While the researchers began with a theoretical disposition which assumed women participate less than their male partners, their findings did not fully reflect those assumptions. Using a national telephone survey, the research team narrowed down to a sample (n=609) which contained both married and unmarried individuals. Married couples were both interviewed by phone separately. While the researchers expected the household roles of women to suppress their political participation, the research found instead that it is men’s household roles which affect male participation. Men with more free time and control of financial resources tended to equate to greater political participation, while womens’ participation was boosted when both partners held beliefs of gender equality. 

The latest wrinkle in modern studies of political participation is the growing scholarship on the network and social effects which affect participation. Gerber, Green, and Larimer (2008) offer an excellent example of this research in their piece “Social pressure and voter turnout: Evidence from a large-scale field experiment” in which they used a large, sophisticated experimental design to test the effect of “social surveillance” on voter turnout. The authors sent single postcard sized mailers with one of four simple messages, using a stair-step approach of increasing social surveillance pressure – civic duty (voting is your duty as a citizen, least pressure), the “Hawthorne effect” (researchers are watching), the “self” message (showing the recipients voting record), and the neighbors message (most pressure, showing their neighbors’ voting record). While the magnitude of effect for the messages was relatively small, only a couple percentage points increase to turnout, the finding that every increased level of social pressure had a corresponding statistically significant amount of increased turnout was clearly an advance in participation scholarship. This research has been replicated since (Sinclair, 2012), although with most researchers finding lower magnitude of effect, and remains a touchstone of modern work in participation. Sinclair (2012) extends the work of Gerber, Green, and Larimer (2008) by testing how social pressure affects a variety of participation modes – voting, political canvassing, affiliate with political parties, and political donating. The findings of Gerber, Green, and Larimer (2008), and Sinclair (2012), do not fully depart from the SES model, which remains the dominant theoretical model for understanding political participation. But their work does expand theoretical structure of the field in ways that are appropriately reflective of the modern context in which participation is taking place. 

### Modes of Political Participation 

The most visible and common act of political participation is voting. Americans love the idea of voting, but in practice fall short of the ideal of one vote for every voice. The most basic measure of voting is turnout, measured as the percentage of eligible voters who actually cast a vote in an election. Studies of voter turnout form one of the largest subsets of American behavior literature, and confront the most basic discrepancy in American democracy (Powell, 1986): In a country that holds the citizen vote in such high regard, why do so relatively few American’s regularly vote?

In the international comparative context, American turnout rates are low, as other industrialized democracies have consistently had higher rates. Generally, American presidential elections hover around the 50% turnout rate, though in recent elections that number has begun to reach back towards 60%. Still, this modern rate appears to be low even compared historically, as turnout between 1860 and 1900 was between 70% and 80% (Gans and Mulling, 2011). Most scholars accept the beginning premise that voting turnout has significantly declined, and their research reflects that broad research question. And while that research orientation is broadly true for the literature, it’s worth noting that some scholars claim methodological problems with how turnout has been calculated are responsible for creating the false impression of declining turnout (McDonald and Popkin, 2001).

Measurement debates aside, studies of how American’s participate politically represent a very large literature. Power (1986) does a good job explaining the institutionalist concerns, and finds that restrictive voter registration laws and declining party affiliation caused a decline in voter turnout beginning in the 1960s. Registration laws have liberalized since Powell’s work, without the hoped-for return to earlier turnout levels, casting some doubt on that portion of his findings. Still, party identification continues to be a robust variable in voting choice models, particularly the historically dominant Michigan model established with the publication of The American Voter (Campbell, Converse, Miller, & Stokes, 1960). Abramson and Aldrich (1982) also identify weakening party affiliations among voters, as well as declining beliefs in external political efficacy, as correlated with the decline in turnout. They show that party affiliation matters, with strongly affiliated partisans voting at a higher rate than those without strong party affiliation. This helps explain the historical trend of a decline as well, as in the past the U.S. populace has had very high partisan affiliation among voters, and so the empirical decline in party affiliation makes sense theoretically as well. Many modern scholars see the sharp decline in turnout in the post-1964 era as rebounding to some degree, with some attributing these trends to inter-generational changes related to broad value changes across countries as overall standards of living have increased (Inglehart, 1971; 2018).
Theories of socialization in voting behavior as discussed earlier, such as those used by Beck and Jennings (1982) have tended to be less popular in the recent literature, but Eric Plutzer (2002) revitalized the basic theories, using the third iteration of the data used earlier by Beck and Jennings (1982) and Jennings and Niemi (1968). Plutzer brings sophisticated methodology to propose a new model of voting which takes into account the path dependence of voting, as he finds that once a citizen begins voting, they tend to vote in subsequent elections. Again, SES and education levels dominate the other variables in predicting whether someone will participate through voting, but Plutzer’s findings are compelling and offer an advancement of the basic turnout model.

The most obvious and common way to participate politically is by voting in elections, but other methods exist as well. Citizens express their opinions in political discussions, shaping and being shaped by other participants. They donate to political parties and individual politicians with whom they agree. They become political activists, join social groups, and canvass their neighborhoods in support of political candidates.  Alexis De Tocqueville (1835) famously commented on the willingness of Americans to associate themselves in voluntary organizations. While this willingness continues to be a mainstay of American political participation, scholars such as Robert Putnam (2000) have grown concerned that declines in participation in voluntary associations has led to a decay of “social capital,” a change that threatens the stability of American political life. However, other work finds that while some forms of associational participation have declined, it has been replaced with increased participation through other modes such as political donations and directly contacting elected officials (Verba, Schlozman, & Brady, 1995). 

Familiar scholar voices return to the discussion of differing participation among Americans in “Citizen Activity: Who Participates?” (Verba, Schlozman, Brady, & Nie, 1993). The authors are responding to other scholars’ findings that voters and non-voters are similar demographically. Using a dataset designed to oversample political activists, the paper seeks to find if activists and non-activists are also demographically similar. They find that the two groups have similar beliefs, but differ in how they participate, and that the differences matter. Activists tend to have higher SES, and far fewer activists come from the lowest SES groups. However, those activists who do come from lower SES backgrounds are more effective in communicating their personal stories of human need to politicians whom they contact. This finding has been widely cited in participation literature due to the basic finding that SES affects how someone participates through political communication. 

### The Electoral Connection and Democratic Effects

In Public Opinion and American Democracy, V.O. Key (1961, p. 7) emphasizes the important link between political participation and the ability to influence government action, insisting that “Unless mass views have some place in the shaping of policy, all the talk about democracy is nonsense.” To that end, scholars have gone to great effort in attempting to locate the effect of public participation and opinion on public policy. David Mayhew (1974) produced one of the most enduring theories in the field in his book The Electoral Connection, where he lays out a stark case that only re-election matters to members of Congress (and to some extent, other elected officials). Given that claim, which is still firmly attended to by most, the voting public’s opinions on policy ought to matter a great deal, and their political participation taken to express those opinions as well. 

Larry Bartels focuses a great deal of attention on the ability of the public to influence public policy in chapter eight of his book Unequal Democracy: The Political Economy of the New Gilded Age (2016). Bartels’ basic thesis is that economic inequality is growing, and that growth has political, not just economic effects. In chapter eight he presents his findings that economic inequality leads to a lack of representation of the views of low income Americans, and that senators “attach little or no weight to the preferences of low-income constituents” and that the political views of the poorest third of Americans receive “little or no weight in the policy making process” (Bartels, 2016, p. 259). One might be tempted to simply connect this lack of representativeness back to the discussion of SES earlier: if poorer people vote less, isn’t it sensical that elected politicians would tend to represent the views of poor people less? Bartels shows, however, that the magnitude in turnout differences between rich and poor groups are too small to correlate well with how senators vote against the policy preferences of the poor. In the end, he finds that while affluent Americans can influence the policy process through both direct and indirect processes, poorer Americans are really left only with indirect methods, such as when less-affluent Americans vote as a group in such a way that close elections are decided by those votes.  

For democratic theorists, there is a normative connection between the two – public opinion ought to affect policy if in fact the polity has control of government. Reflecting that ideal, scholars have attempted to connect Americans’ political participation to the policy outcomes of their government, with mixed success. For many years the literature failed to establish correlation, let alone causation, between public opinion and public policy. But beginning in the late 1980s and 1990s, two primary types of research in this area were established (Monroe, 1998) – studies of “congruency” and those of “consistency.” 

The classic congruency study is “Effects of Public Opinion on Policy” (Page & Shapiro, 1983), which uses survey data collected over multiple decades. The authors use similarly worded questions from across the time periods, and then attempt to correlate policy shifts from before the first time period a question was asked, and after the next time it was asked. Page and Shapiro are able to establish that when a shift in public opinion is detected (they look for shifts larger than 6% in aggregate opinion), there is a correlative policy switch approximately two-thirds of the time. This is a higher percentage than most expected, as most previous research had failed to find any connection between public opinion and public policy. However, given the lack of causal connection the authors were necessarily reserved in their claims. 

While Page and Shapiro represent the congruency approach, Monroe (1998) places himself in the consistency camp of researchers, which compares how the distribution of policy outcomes with the distribution of public opinion. Monroe compares two time periods, 1960-1979 and 1980-1993, and finds that in the later period public policy’s consistency with public opinion had dropped from 63% to just 55%. Monroe attributes this relatively low consistency to institutional reasons connected to the inherent bias against change in the American political system. Monroe goes further than Page and Shapiro (1983) by further stratifying his samples of opinion and policy into substantive policy area, allowing him to detect that, for example, in foreign policy, there is nearly 100% consistency between public opinion and policy change. Like Shapiro and Page, though, Monroe’s methods limit him to correlation claims rather than causal ones, and in the case of foreign policy, the nearly 100% correlation leads one to believe that at least a significant portion of the consistency is due to policy affecting public opinion, rather than the opposite. 

While the research considered here has so far been concerned with national-level issues, Erickson, Wright, and McIver (1989) offered an interesting variation, locating the discussion in state-level research. The most important finding to come out of this work is that state electoral forces are the most important factor in the correlation – or lack of it – between public opinion and public policy. Using complex path models, they demonstrate that ideological variation between states makes comparisons very difficult - i.e. a Democrat from Alabama is probably more conservative than most New York Republicans. State policy outcomes do represent state opinion preferences, but party control of a state legislature is not a good predictor of state policy, as the party activists and the centrist voter tend to be pulling politicians in different directions than the national party. 

### Conclusion

When observing American political participation, demographics matter. Wealth, income, class, and education measures continue to provide the greatest explanatory power for researchers interested in questions of “who participates?” E.E. Schattschneider famously made reference to the political bias for the wealthy in The Semisovereign People (1960), writing “The flaw in the pluralist heaven is that the heavenly chorus sings with a strong upper-class accent.” Socioeconomic status continues to be the most powerful explanatory factor for determining who participates, how they participate, and whether their participation is able to substantively influence political and policy decisions in their favor. American’s do participate differentially in politics, in voting as well as other modes, and that participation appears to have differing effects on politicians and the public policy they implement. 

## Political Participation 2

American political scientists have been addressing questions of political participation since the beginning of the field. Estimates of voting participation, which is the most commonly studied mode of participation, show that between 1860 and 1900, national voter turnout was between 70% and 80% (Gans & Mulling, 2011). In the modern era however, turnout has sharply declined (Abramson & Aldrich, 1982), particularly after the 1960 election cycle, and turnout levels of approximately 50% in most presidential elections are the new normal. 

Sidney Verba is perhaps the most influential scholar in the participation literature, as he centered SES measures in the calculus of participation. The predominant modern model of political participation is the socio-economic status (SES) model, which posits that a person’s income, education, and class status affect their likelihood of  “acts that aim at influencing the government, either by affecting the choice of government personnel or by affecting the choices made by government personnel” (Verba & Nie, 1972, p. 2). SES remains the most accepted, central, explanatory factor in political participation.

Socioeconomic measures remain central to understanding American political participation, but the research remains robust as scholars continue to seek a more complete model, expand what “participation” means, and make use of increasingly sophisticated research methods. Covering the whole of participation literature is an impossible task in any writing length short of a book, and so this essay will proceed in three main sections. First, a brief introduction to the literature looking at participation broadly, with a focus on the influential work asking questions about who participates, and the correlates associated with participation. The second section turns to the specific question of voting turnout, and presents five competing explanatory theories of institutions, attitudes, habits, pressure, and conformity. 

### Who Participates and Why?

Beck and Jennings (1982) provide an overview of the essential concerns of those political participation scholars. The authors use the second iteration of the data set used by Jennings in his earlier sociological work (Jennings & Niemi, 1968), with the continuing research forming panel data for the researchers to use. The authors attempt to rehabilitate (or perhaps simply extend the end-of-life of) political socialization in the face of growing theoretical reliance on rational choice explanations that were becoming popular (Aldrich, 1976). While not successful in that goal, they do nicely lay out the sequential argument, testing four separate path models, and then combining those into a final model. They give credence to the rising acceptance of the SES model of participation (Verba & Nie, 1972), including early work showing that economic adversity such as unemployment and poverty work to suppress political participation and turnout (Rosenstone, 1982). In addition to those effects in adulthood, Beck and Jennings (1982) find evidence that pre-adult forces also affect participation. They single out parental socioeconomic status, participation, and civic orientation, as well as youth school activity, as having statistically significant effects. Still, the traditional components of SES – education levels and income – have larger effects. Beck and Jenning’s weakest causal claim is related to the youth activity levels, as it is just as likely that young adults already interested in political participation are more likely to have more activity engagement, thus undermining the causal path suggested by the authors. Finally, the representativeness of the sample must be questioned, as the dataset does not include those young adults who have dropped out of high school. As high school completion is already highly correlated with socioeconomic factors (Mare, 1980; Toutkoushian & Curtis, 2005), it is likely that Beck and Jennings models would show even stronger SES effects if the sample were truly representative.

Narratives are an increasingly salient area of policy study (Epstein, Farina, & Heidt, 2014; McBeth, Jones, & Shanahan, 2014) and build upon early work on how personal narratives and participation intersect. Recall that Stanley Verba helped shaped the field of political participation research, and in “Citizen Activity: Who Participates?” he and his co-authors (Verba, Schlozman, Brady, & Nie, 1993) narrow in on political activism as a specific, non-voting form of political participation. While voting turnout is relatively low historically and comparatively, becoming an activist is even rarer. The authors are addressing earlier findings which showed that both voters and non-voters as groups are similar demographically and ask: Ddoes the same apply to activists and non-activists? To answer the question, they use national survey data designed to oversample political activists (ANES). They report that while the two groups have similar beliefs, there are SES differences within activists and non-activists, and further, that the differences matter. Activists with lower SES backgrounds tend to use more compelling, humanistic stories based in their own experience when they are speaking to policy and law makers. But because of their disadvantaged SES backgrounds, there are relatively fewer activists within that SES group. This finding has been replicated and widely cited due to the core finding that SES status affects the messages that someone communicates politically, which is a distinction worth noting. 

Reflective of the impact of Verba’s work, Lawrence Bobo and Franklin Gilliam (1990) test one of the most influential findings from Participation in America (Verba & Nie, 1972): that once socioeconomic controls are introduced, black American’s actually participate politically at a greater rate than their white counterparts. Verba famously theorized that this was due to a group consciousness, which was supported by findings from interviews that black respondents who talked about race more often were also more likely to politically participate. Bobo and Gilliam (1990) are motivated by that theory, and test their hypothesis that black citizens living in areas with black mayors are more likely to participate. They find empirical support for their hypothesis, but ultimately their findings are significantly undercut by both causality problems as well as a mismatch between theory and method. Causally, it is not clear that it is black political leadership that is causing increased black political participation – and in fact the reverse is quite more likely to be true, as the black political leadership must be campaigned for and voted for, and those actions are very likely to be undertaken by black supporters who are already participating politically. Further problematic is that the authors’ theory implies a durable time effect – that with black political leadership in place, black participation ought to increase over the term of that leadership. However, the authors use only cross-sectional data from a 1987 General Social Survey (GSS) dataset (National Opinion Research Center, 1941). This limitation is quite likely tied to limits on the dataset itself, as the 1987 GSS intentionally oversampled black respondents. Regardless of reason, however, the causal and methodological problems limit the confidence we can have in the findings.
Stanley Verba and Kay Schlozman return as co-authors in “The Public Consequences of Private Inequality: Family Life and Citizen Participation” (Burns, Schlozman, & Verba, 1997), reporting on a survey study of married couples and political participation. The authors begin their study with the accepted theory of the time that because women are oppressed within the family structure, it is likely their political participation is lowered. Despite this theoretical orientation, and to their credit as academics, they report findings not fully in-line with their prior expectations. The study started with examining a large national telephone survey, and then narrowed the scope to identifying a randomized sample (n=609, married n=380). Researchers then interviewed the smaller sample in detail by telephone regarding their household roles and political participation. Rather than finding that household roles suppress women, the authors contribution is that roles affect men. Specifically, free time and control of household resources matter for men, while women’s political participation is enhanced by beliefs in gender equality, both their own and their partners’. 

### Turnout: Institutions, Attitudes, Habit, Pressure, or Conformity?

While voter turnout – the proportion of citizen who do vote, compared to the population that are eligible to vote – is covered briefly by some of the authors covered in the first section of this essay, there is value in covering the different theoretical orientations that have developed around this most visible, and most heavily researched, aspect of political participation. Rational choice theories gradually replaced the sociological explanations for political behavior broadly beginning in the 1980s. The earliest rational choice and turnout work (Riker & Ordeshook, 1968) constructed a model in which the choice to vote was equal to the benefits times the costs, plus a duty measure, minus the costs of voting. Turnout was fit more specifically into rational choice theory over the next few decades, with mixed results (Aldrich, 1993). Overall the rational choice orientation has not proven well-suited to explaining why people choose to vote, or not (A. Blais, 2000), though there are some research agendas still focused on exploiting the theory in experimental conditions (Duffy & Tavits, 2008), or to explore informal social networks (Abrams, Iversen, & Soskice, 2011).

As noted in the beginning, most scholars accept the beginning premise that voting turnout has significantly declined, and their research reflects that broad research question. And while that research orientation is broadly true for the literature, it is important to note some scholars claim there are methodological problems with how turnout has been calculated that are responsible for creating the illusion of declining turnout (McDonald & Popkin, 2001). Once the calculation problem is corrected, the “true” historical turnout pattern is fairly stable, with a surge of voting in the 1950s. What factors led to this surge, they claim, should be the focus of turnout-related research. An example of such focus is research that shows lower participation in Canadian elections is due to life-cycle and generational effects, as post-baby boomers are less likely to turnout (A. E. G. Blais, Nevitte, & Nadeau, 2004).

The turnout measurement debate continues and researchers continue to propose more rigorous and accurate turnout metrics (Stockemer, 2017a), but there is still no single accepted model of turnout regardless of measurement preference. This remains one of the largest research literatures in political behavior, if not the largest, as comparative research continues to grow as well. Meta-analysis of the turnout literature finds that even the most accepted model of turnout may not be correct, and that turnout “might be more complex than the current theory suggests and is rather more context dependent” (Stockemer, 2017b, p. 698). In particular, comparative studies suggest that increased turnout is expected when there are 1). compulsory voting laws, 2). in small nations, 3). when an election is perceived to be important. Regional attachment and autonomy can increase the perceived importance of elections, leading to higher turnout (Henderson & McEwen, 2010). Perceptions are also important as they relate to how people perceive the integrity of elections, as they are less likely to vote when there are doubts about the fairness of the process (Birch, 2010).

So, what are the common theories of turnout? This essay covers five: theories of institutions, attitudes, habits, pressure, and conformity. Powell (1986) structures the major institutionalist concerns, many of which are still within the scope of modern scholarship. Powell lays out one of the fundamental questions of turnout: Why does the American populace, which has the political attitudes and educational levels that are associated with higher turnout, lag behind other institutionalized democracies in voting turnout? In this study two primary institutional factors are linked to turnout – voter registration laws and the U.S. party system. Overly onerous registration laws depress turnout by 14%, while a party system without clear class affiliations (such as those found in the UK and other European countries) depresses it by another 13%. With the benefit of retrospect, Powell’s findings on registration laws are not appear as robust as he hoped, as the US registration laws have largely liberalized since his research, without the hoped for significant increase in turnout. However, his party-linked findings are still useful in examining the US system. From a broad perspective, both registration laws and party linkages are of continuing concern in turnout research.

Abramson and Aldrich (1982) examine the effect of political attitudes, specifically attitudes towards political parties, on turnout.  Like Powell (1986), these authors were interested in what had caused the precipitous decline in voting turnout after the 1960 election. They identify weakening party affiliations among voters, as well as declining beliefs in external political efficacy as correlated with the decline in turnout. Political efficacy belief is when a person believes that government will be responsive to their political participation. This contributes to the theoretical link between declines in political efficacy belief and reduced turnout – if a voter does not think their political activity will result in government action to address that activity, they are less likely to engage in the activity in the first place. This is related to other research linking perceptions of unfair electoral practices and lower turnout (Birch, 2010). Abramson and Aldrich are able to show that party affiliation matters, with strongly affiliated partisans more likely to vote than those without strong party links. Historically the U.S. populace has had very high partisan affiliation among voters, and so the empirical decline in party affiliation matches the  theoretical predictions as well. 

Following the research of Abramson and Aldrich(1982) and Powell (1986), scholars have been able to identify some of the reasons behind the central puzzle of declining turnout post-1960. A generational effect may be responsible (Inglehart, 1971, 2018). The WWII post-war generation had unusually high beliefs in good civic values, while the generation that followed did not share in those same beliefs, leading to a decline in turnout. Similar to generational effects, further research has located the inertial effects of voting – that is, people who vote tend to continue to vote, while non-voters tend to continue to abstain. Eric Plutzer (2002) suggests a developmental theory of turnout. He applies modern, sophisticated methodology to relatively old data, using the third wave of the Student-Parent Socialization Study  (Jennings & Niemi, 1968). Plutzer’s research is the rare resurfacing of socialization theory, while using individual-level time-series analysis to identify the factors behind an individual’s decision to vote. Plutzer offers a new framework of voting habit, which includes a starting level (probability a person will vote in first eligible election) and measures of inertia (person continues voting). He identifies factors which influence both starting level and inertia. When first eligible to vote, parental SES and political resources largely define the probability of voting, and continues to be influential in subsequent elections, though that influence declines as the individual voter’s accomplishments increase. While Plutzer’s findings are not necessarily surprising, by identifying voting and not voting as habits he contributes compelling findings and theory that advance the overall model of turnout.

### Turnout, Social Pressure, and Field Experiments

One growing trend in turnout research is that of social pressure, and there has been robust field experiment work in this area, which will be the focus in this section of the essay. The foundational research in this line of research was a large-scale randomized field experiment that showed nonpartisan phone calls were ineffective at increasing turnout, while canvassing and direct mailers had small positive effects (A. S. Gerber & Green, 2000). The original results were replicated in part soon after (Green, Gerber, & Nickerson, 2001), with an experiment in six cities showing that door-to-door canvassing resulted in an approximately 7% increase in turnout. Nickerson (2007)  rehabilitats phone-calls a few years later by showing that they can increase turnout, but only when the quality is high, and in separate work (D. W. Nickerson, 2007) demonstrates that nonpartisan emails had no significant effect, though other research shows emails can be effective (Malhotra, Michelson, & Valenzuela, 2012).

The formative research is from some of the same authors (A. S. Gerber, Green, & Larimer, 2008). Using a sophisticated, large field experiment design, the authors sent single postcard sized mailers with one of four simple messages, using a stair-step approach of increasing social surveillance pressure: 1). civic duty (voting is your duty as a citizen, least pressure), 2). the Hawthorne effect (researchers are watching), 3). the self message (showing the recipients voting record), and 4). the neighbors message (most pressure, showing their neighbors’ voting record). While the magnitude of effect for the messages was relatively small, only a couple percentage points increase to turnout, the finding that every increased level of social pressure had a corresponding statistically significant amount of increased turnout was astounding. In the same year that Gerber, Green, and Larimer (2008) firmly established the effect of social pressure on turnout, Nickerson (2008) uses similar methods to demonstrate the strong effect of ‘spillover.’ That is, around 60% of the effect of canvassing directly on one member of a household is passed on to other members of the household. The spillover effect (Nickerson, 2008) has arguably become more important as time has increased research into the social effects of interventions on turnout (Sinclair, 2012).

Gerber, Green, and Larimer’s social pressure research has since been replicated and strengthened as researchers advance the field’s understanding of social network effects, with small but statistically significant effects from social pressure found across political participation contexts, including voting, political donations, candidate selection, and party identification (Sinclair, 2012). This moves the field beyond even pressure and begins to show how individuals conform to their social network. As shown by Sinclair (2012), individuals conform to their geographic and social network pressures in party identification. For example, Republican individuals who have close friends who are Democrats do not necessarily change party identification, but they do begin to retreat from public declarations of party affiliation. Similarly, small subsets of individuals who identify as having conservative beliefs vote for Democrats in presidential elections, and the same phenomenon occurs with liberals voting for Republican candidates. Sinclair shows that the strongest predictor of this cross-ideological voting is that the individual lives in an area with strong partisan leanings opposite their ideological beliefs. 

Social media is playing a larger role in turnout research. The largest election day study (Bond et al., 2012) of approximately 61 million Facebook users showed that simple “I Voted” messages combined with faces of their friends induced hundreds of thousands of users to vote. Teresi and Michelson (2015) show that Facebook mobilization efforts are successful in increasing turnout among college students, even with motivation from online ‘friends’ who are not well known. Finally, the social pressure from shaming and praising Facebook friends for not voting or voting can significantly increase turnout (Haenschen, 2016). 

There is much more to explore before the full scope of social effects on political participation is understood, but the beginnings of this area of research is very promising. There is growing evidence that the treatment effect of social pressure on turnout is heterogenous (Arceneaux & Nickerson, 2009; Coppock & Green, 2016), and that social pressure mobilization efforts are primarily having an effect on people who are already likely to vote, whereas there is little effect on l0w-propensity voters (Enos, Fowler, & Vavreck, 2014). Meta-analysis of research on social pressure and voter turnout has shown that the average treatment effect for all modes of contact increases when there is a personal element to the outreach, while those lacking the social pressure component generally have no effect (Green, McGrath, & Aronow, 2013). Nickerson has continued to contribute in this area, with his recent demonstration that canvassing for voter registration has an indirect but significant positive effect on turnout (D. W. Nickerson, 2015). 

### Conclusion

This essay has progressed from the broadest conceptions of participation, narrowed to turnout as one aspect of participation, and then narrowed further to look at one theory of turnout.  The selection of social network effects on participation as the most focused aspect of the participation literature was not accidental, but intended to reflect the intensity of research in that area going forward (Cox, 2015; Gerber & Green, 2017). It is not surprising that in a world ever-more increasingly connected through virtual social networks that academic research would tend to grant greater resources to exploring the impact of our social selves on traditional political science questions. There is growing acceptance that “social environment determines individuals’ political choices and participation,” (Sinclair, 2012, p. 153), and while there is not yet a widely accepted explanatory model of participation, it seems clear that at least marginal increases to the canonical SES model can be found in exploring our social selves rather than simply describing our demographic traits. 


## The Weak American Presidency

An enduring debate within American political science is whether the office of the presidency is fundamentally strong, or weak. In this paper I will argue that the presidency is indeed an institutionally weak office: It was created constitutionally weak, operates largely constrained, and even in those moments where a truly “strong” President appears, it is due to cyclical circumstances largely outside of their control (Skowronek, 1993). Many theorists have lined up on the ‘strong’ side of the debate, but in the end they focus to much on small slices of what presidents do (such as executive orders) and mistake the actions of a few skilled presidents who had the fortune to take the office in a fortuitous time as representative of an office which has the vast majority of the time operated in a context where it was constitutionally and politically constrained. 

### Presidents are strong

The American political system is based on the sharing of power between the executive, legislative, and judicial branches. In the pluralist tradition, the president is strong by virtue of being an equally balanced member of the American governmental triumvirate. In Federalist #51, James Madison (1787) lays out a theory of early pluralism, famously writing, “Ambition must be made to counteract ambition.” In constructing the office of the presidency, the founders sold the idea that the executive branch, would be just as “strong” as the other two branches. In creating three separate, independent, and conflictual branches of government, Madison argues, the country would avoid the tyrannies of both the majority and the despot. Pluralism dominated early American political science, yielding only slowly and never fully, and pluralist assumptions can still be found being usefully applied, albeit it in a substantially altered manner, such as in Neustadt’s (1991) work on presidential bargaining.  

Prior to the rise of “new” institutionalism (March & Olsen, 1983), political science primarily saw the presidency through contextual, utilitarian, reductionist, functionalist, and instrumentalist theories. Politics broadly, and the presidency specifically, were separate from larger society, and susceptible to exogenous events, primarily reactive. Political scientists searched for behavioral and economic theories to explain politics, which they believed could be understood as individuals acting out of self-interest. Most damningly these theories saw the American political system functionally, misappropriating evolutionary theory (1983, p. 735) “to see history as an efficient mechanism for reaching uniquely appropriate equilibria.” 

Early presidential political scholarship did not focus on the institutional factors of the office, and academic renderings of a “weak” or “strong” presidential office were rendered through psycho-biographical scholarship. In this tradition are authors like James Barber (2017) and Fred Greenstein (1994). These authors essentially argue that “good” presidents are inherently strong, brilliant men, and “bad” presidents are inherently weak men. Barber in particular argues that presidents fall into a two-by-two matrix: active positives, active negative, passive positives, and passive negatives. 

This tradition eventually provides unsatisfactory answers, due to the post-hoc nature of the examination. Critics rightly point out that Barber does not leave room for agency on the part of presidents, as essentially how they will act as presidents is pre-determined from an early age. Further, Barber’s “methodology” leads to surprising category assignments for some presidents, such as asserting that President Clinton was an “active positive” when his presidential performance would lead most to believe he was an “active negative,” at least under Barber’s assignations. 

William Howell and David Lewis (2002) write within the tradition of scholars who see the presidency as strong, particularly within a context of a weak (and weakening over time) Congress. They see presidents’ strength in unilaterally creating agencies outside of Congress' control (and often against their wishes), which in turn expands presidential power. The measure of congressional weakness continues to have significant and positive effects on the number of executive-created agencies each year. When Congress is weak, presidents create more agencies by executive action, and through path dependence (Pierson, 2000) those agencies accrue further power to the president.

Andrew Rudalevige joins the chorus of early 2000’s scholarship which saw a resurgent presidency, strong and growing stronger. The presidency (in this case President George W. Bush) was increasingly free to take unilateral action, particularly in foreign affairs. Democratic accountability weakened in a post 9/11 America, with increasing governmental secrecy and a Congress unwilling to confront the president, and Congress members were increasingly dependent on the executive branch for information, weakening the separation of powers. 
Slightly preceding Rudalevige but with findings that support his belief in a strong, expanding executive branch, Howell and Lewis (2002) test whether federal agencies created via the executive branch are less vulnerable to congressional oversight, and find that they are. “The ability to act unilaterally” to create administrative agencies “stands out as one of the most important characteristics of the modern presidency,” (p. 1113), they write, and indicates a strong presidency in the face of a Congress burdened by institutional factors which makes is weak in relation.

Bolton and Thrower (2015) pick up on Rudalevige’s assertion that increasing use of the executive order to bypass Congress is an example of a strengthening presidency and decide to test it. They test two related hypotheses, (1) that prior to the 1940s when Congress had low legislative capacity, President’s issued more executive orders, and (2) that post-1940’s, as Congress had increased capacity, Presidents issued fewer executive orders. Bolton and Thrower’s analysis finds that indeed, presidents used the executive order less in the post-war period, casting some doubt on Rudalevige’s theory of an expansive and strong presidency. In hindsight, Rudalevige’s conviction in a renascent “imperial presidency” may have been overly dependent on a single war-time president, George W. Bush.  Further, as Bolton and Thrower admit in their conclusion (2015, p. 661), conceptualizing a president’s power as being strongly related to the number of executive orders issued is problematic, as “these are by no means the only way Presidents exercise unilateral power. Memoranda, proclamations, signing statements, national security directives, and regulations all serve as vehicles for presidents to potentially circumvent the legislative process.” One could easily add more that Presidents do without Congressional approval. Presidents give speeches, conduct foreign travel and relations, conduct limited (many would argue the modifier here) military actions, direct covert intelligence activities, and on and on. Obsessing over the number of executive orders to prove the presidency is a fundamentally strong institution misses the forest for the trees. 

### Presidents are weak

James March and Johan Olsen set the stage for moving beyond the dominant political theory of the time, pluralism, in American political scholarship with the publication of (1983) “The new institutionalism: Organizational factors in political life.” March and Olsen’s insights allowed for presidential scholarship to advance beyond the personal, biographical, and “daddy issues” of authors like Barber and Greenstein. Instead of biographies of elites, the new institutionalism insisted that (1983, p. 747), “The organization of political life makes a difference.” Institutions, including the presidency, were endogenous, with their own history, culture, and practices which affected it. The state is not only affected by society, but society is also affected by the state; leaders do not simply affect followers, but are affected by followers as well. 

Richard Neustadt (1991) adopts the new institutionalist perspectives in his presidential scholarship. The power of the presidency is relatively weak, Neustadt believes, and therefore presidential power is dependent on individual skill of presidents interacting with context. How presidents behave is dictated by the weakness of the office, so president can’t rely on the power of the office, and has to rely on persuasion, negotiation, bargaining. 

The president’s resources include bargaining powers that come from the position, professional reputation, and public prestige. The president’s (p. 150) “power is a product of his vantage points in government, together with his reputation in the Washington community and his prestige outside.” Neustadt believes that professional reputation is the most important of these. 

Neustadt (1991) uses pluralism as a vehicle for his argument that presidents must bargain both externally and internally in order to wield the relatively weak power of the office. Presidents bargain internally with competing factions of his own executive branch. Heads of federal executive agencies, cabinet secretaries, and bureaucrats all have competing interests. The most important factor of these in Neustadt’s view is a president’s reputation inside Washington D.C. Neustadt’s work has proven to be a long lasting and powerful theory of presidential scholarship, as modern examples continue to be seen in President Obama’s use of the executive order to bypass Congress in the Deffered Action for Childhood Arrivals (DACA) and President Trumps Congress-bypassing travel ban on immigrants from specific countries.

Kernell (2007) agrees with Neustadt’s general premise that the institution of the presidency is weak, but challenges Neustadt’s pluralist, bargaining argument and instead makes the case that a president’s public prestige is more important to their power than their reputation in Washington, D.C. Neustadt was right about the presidents and Washington, D.C. of his own time, argues Kernell, but since that time presidents are increasingly taking their policy messages to the wider public, bypassing Congress. The modern Washington, D.C. looks very different than the city of the past, and has transformed into an “individual pluralism.” This is due to the sheer number of interest groups, legislative members and their staff, and executive agencies a president would be forced to bargain with. Skowronek (1993) agrees with Kernell narrowly here, in that he believes we are currently in a “Plebiscitary Era” with increasingly candidate-centered presidential campaigns, and greater reliance by presidents on direct appeals to the electorate. The presidency is an institutionally weak office, and so the president uses his plebiscitary power to appeal to the public, which can then influence members of Congress to give the president what he wants.

Other scholars have answered Kernell by asking for examples where this has occurred and was successful. While presidents have been more public in their appeals, finding times where a preferred presidential policy was implemented through strategies of public appeals, skipping the legislative bargaining piece, has proven difficult.

Canes-Wrone (2001) answer Kernell’s critics who question whether the plebiscitary power of the president are effective.  The authors make rational-choice assumptions about presidents, namely that presidents go public with an issue only when the public already supports the president’s view. If presidents go public when the public is not already supportive, the president risks alienating the legislative branch by raising the issue’s salience without the broader support necessary for passage. Canes-Wrone’s compare appropriations data and televised speeches by the president, and find general support for their hypotheses. “Going public” in Canes- Wrone’s analysis does help presidents overcome the weakness of the office to get what they want. 

Stephen Skowronek (1993) offers a different take on the presidency, one that is context dependent, and in the end the most convincing. Presidents are not individually great, or weak, or competent, or incompetent, but instead are in most ways defined by the presidential cycle they find themselves in. Skowronek classifies presidents in two dimensions. He asks if the current political regime is strong, or weak, and then asks is the president affiliated or opposed to that regime? 

These two dimensions give rise to four types of presidents:  reconstructive, articulative, pre-emptive, and disjunctive. Presidents of reconstruction are what history remembers as “strong” presidents, such as Thomas Jefferson, Andrew Jackson, Abraham Lincoln, Franklin D. Roosevelt, and Ronald Reagan. The weakest presidents are those that precede political presidential reconstruction, such as Jimmy Carter, Herbert Hoover, and I would argue our current president Donald Trump (a president affiliated with a weak or weakening political regime). 
In addition to regime affiliation and strength assessments, Skowronek adds a related distinction between presidential power and presidential authority. Presidential power remains fairly constant over time, fundamentally weak but probably growing slightly over time. The authority of the president on the other hand depends on where in political time the president takes office. Reconstructive presidents have the most authority, while disjunctive presidents have the least.

Skowronek more closely aligns with theorists who see the institutional powers of the presidency as weak, and presidents themselves are more likely to be prisoners to their own place in political and secular time than they are to be fully-realized agents of a powerful office. In addition to the political cycles (called “political time”), Skowronek theorizes there are also “eras” of presidential time (“secular time”). American history has had four eras, with the current era beginning in 1972 (the “Plebiscitary Era). Over time, these eras have gradually become ever-more “institutionally thick” and resistant to change. 

Skowronek and Neustadt provide the most compelling theoretical and empirical arguments of the presidential scholars, and seem to at least agree that modern presidents are operating in a different era of presidential politics compared with presidents of the past, although the two come to different conclusions. Where Neustadt divides between modern and pre-modern presidents, Skowronek would say there have been four presidential eras, and we are in the “Plebscitary Era.” Further Skowronek provides a compelling theory in showing the rise of the modern administrative state (Howell and Lewis, 2002) has brought with it an accretion of the political interests of those administrative agencies to the presidential status quo. Ultimately Neustadt’s conclusion that pre-modern presidents were clerks of the executive branch is less convincing and too simplistic compared to Skowronek’s. The modern president is both impeded by the thickening administrative barnacles on the ship of state, but also has the advantage of the weight of the administrative state added to his office when he is able turn it in the right direction.    Only when the context of political and secular cycles are serendipitously arranged at the same time a president of sufficient skill takes office does presidential power truly become strong. In those presidencies of reconstruction, the existing political regime dies giving birth to the next regime.

Skowrownek’s theoretical and empirical rendering of a fundamentally weak office which is at specific, very limited times filled by a skilled president is the most convincing. By removing the focus on the individual and concentrating his attention on the institutional factors of the office, he allows for a historical, analytical approach to the presidential scholarship which does not easily bog down in distance-psychodiagnosis or partisanship. 

### Conclusion

The presidency is an inherently weak institutional power. The mere fact that history has given us scattered, strong and successful presidents is not enough to counteract the institutional context which defines a weak presidency. The presidency is weak, but of course not powerless, allowing for skilled presidents in the right historical moment to rise above a constrained office and appear enormously strong. However, the strength of these outliers is remembered so well in large part due to the vast number of presidents who are largely feckless and prisoners of their own place in time. 

The primary constraints on the office of the presidency are constitutionally designed. While Madison (1787), writing under the pseudonym “Publius,” claims that the three branches of government are equally balanced, it is important to remember Madison was primarily writing in order to promote ratification of a Constitution. The balancing of the constitutional powers was important to both a public tired of kings and an aristocracy terrified of popular rule. The Federalist Papers can be thought of a partly propaganda, intended to “sell” the Constitution, and so it made sense to sell the normative view of how pluralism would prevent an imperialist president or mob rule, revolution, and guillotines. Yet while in #51 Madison sells a vision of equally balanced powers, Alexander Hamilton undermines the image of an equally strong presidency in Federalist 69, titled “The Real Character of the Executive. Hamilton (also working under the pseudonym “Publius) appears to be working to address the primary fear of the anti-federalists, who worried a president was just another name for a King, which they were devoted to preventing. The president’s authority, Hamilton wrote (1788), “would amount to nothing more than the supreme command and direction of the military and naval forces, as first general and admiral of the confederacy.” 

Pluralist beliefs of how things should work should not be confused with how they do work. The constitutional constraints on the presidency give really only one true strong power, that of the presidential veto, and even that can be and has been overridden with a super-majority of Congress. The president can propose a budget, but cannot pass one. He can ask for a declaration of war, bargain for preferred policy, appeal to the public, and yet in the end he must go to the legislature to take action. 

Those who argue that the president can use executive orders or limited military action to bypass Congress are right insofar as those powers exist, but wrong in translating those limited activities into an institutionally powerful office. In any case, as shown by Bolton and Thrower (2015), use of the executive order has in fact decreased in the post-war period. Even in cases where there seem to be examples of a strong presidency willing to use executive orders, as in the case of Presidents Obama and Trump, their use is more accurately seen as a symptom of a weak office, too unskilled to wield their own bargaining and plebiscitary powers to leverage Congress into their preferred action. 

An institutionally weak presidency is good news in an America dominated by partisan swings in the Presidency. Despite the incautious rhetoric of President Trump, and the utterly predictable reactions of horror from many, the institutional limits on the presidency have for the most part prevented President Trump from taking unilateral action. At every step he is confronted by an “institutionally thick” (Skowrownek, 1993) American government which prevents him from taking unilateral action, resulting in impasse. As much as the public hates impasse on their preferred policies, they should simultaneously recognize that the slow, grinding nature of the American political system also protects them against the fever dreams of would-be kings.  

## Trump as Skowronek’s “Disjunctive President”?

In The Politics President’s Make, Stephen Skowronek (1993) lays out the historical institutionalism case for understanding how and why US presidents are successful in remaking the American political order – or not. Skowronek sees the history of American presidents playing out in cycles comprised of four types of politics: the politics of reconstruction, articulation, pre-emption, and disjunction.  Although Skowronek makes room in his theory for “hard cases,” in general if his theory holds then one ought to be able to predict the broad strokes of a presidency. This paper will not engage Skowronek’s theory critically in the sense of attempting to challenge his theory. Rather, I will take the theory on its own best ground, granting its assumptions and conclusions, and attempt to follow the implications of Skowronekian thought through to its bitter, Trumpian ends.

In some future history, President Trump should fit into one of Skowronek’s categories, and in this paper, I will make the case that he will be seen as a disjunctive president. President Trump does not cleanly fit into either the categories of articulative or pre-emptive, and so the only alternative within Skowronek’s formulation would be President Trump as a reconstructive president. I grant at the outset that either are possible, and good arguments can be reasonably made for either. However, the weight of evidence seems to tilt towards Trump as disjunctive; therefore, I will also address why he is likely not a reconstructive president.

### Skowronek’s Historical Cycle

Where other presidential scholars tend to attribute success and failure to the personality characteristics and decisions of presidents (Neustadt, 1991) or their personal communication styles and talents (Kernell, 2007), Skowronek insists that presidents must be seen in relation to those who came before them, and to the politics of the time. Presidents exist both in time and act over time.

Skowronek’s presidential cycle always begins with a reconstruction, followed by a period where presidents of articulation and pre-emption exchange through time, before finally a president of disjunction takes office. The politics of disjunction proves the established order no longer has the answers to the questions faced by the nation, and is soon followed by the next president of reconstruction. Importantly, “disjunction” is not describing a situation where one major party or the other no longer has the answers – but the system itself, comprised of both parties, no longer does. When just one party (or president) is seen as failing, this is a sign of either a pre-emptive president or one of articulation – either attempting to reinvigorate the politics of the previous reconstructive president (articulation) or attempting to provide answers the politics of the reconstructive president could not (pre-emption).  

### Skowronek’s Modern Cycle

Skowronek places President Reagan at the beginning of the modern political cycle, as a president of reconstruction. Successive Presidents Bush (41, articulation), Clinton (pre-emptive, and Bush (43, articulation) traded offices between the poles. When President Obama took office in 2008, much was hoped for and predicted for his forthcoming transformational presidency. President Obama’s election, though doubtlessly inspirational for much of the world, did not translate to a repudiation of the established political order. Pragmatic problem solving, innovation within the boundaries set by the conservative regime ordered by President Reagan better characterize President Obama’s two terms than does reconstruction of the American political reality. In any case, President Obama’s failure to secure the presidency for his named successor, a hallmark of reconstructive presidents, should permanently secure his place as a pre-emptive president – at least under Skowronekian law.

### Trump’s Disjunction

Skowronek defines the politics of disjunction as those periods of time where the established political order is no longer capable of effectively addressing the nation’s problems. A time of disjunctive politics is required for there to be a time of reconstructive politics. The disjunctive period serves as “proof” to the voting public that the established political order is no longer a viable order – and the transformative period that follows requires that there be no viable alternative available to the established order. 
Some might be tempted to see Trump as a reconstructive president. I think this is a mistaken impression which is heavily influenced by what Trump (as both candidate and president) says rather than what he does. Trump at times speaks as if he were a reconstructionist president intent on remaking the system in a new image. “Drain the swamp,” his oft-heard quote and tweet, is intended to capture that timeless American hobby of disapproving of the “intrigue” of Washington D.C., or as James Madison (1787) wrote in Federalist 10: “Men of factious tempers, of local prejudices, or of sinister designs, may, by intrigue, by corruption, or by other means, first obtain the suffrages, and then betray the interests, of the people.” 

Yet, while Trump has rhetorically railed against the Washington D.C. political environment, there is little to suggest he has taken action to support that rhetoric. Whereas candidate Trump “routinely invoked Goldman Sachs as a bogeyman” (Coppins, 2018), President Trump appointed a Goldman Sachs alum, Gary Cohn, as director of the National Economic Council. Candidate Trump campaigned on a promise to seek legislation placing a five-year lobbying ban on former Congress members and staff; President Trump hasn’t sought any such legislation. And while candidate Trump promised to limit the influence of lobbyists, press reports show that twenty former lobbyists work in President Trumps executive office, and forty-nine former lobbyists now work for the agencies they previously lobbied (Freidersdorf, 2017). 

A longer list of how candidate Trump’s rhetoric conflicts with President Trump’s actions could be compiled, but is outside the scope and length of this paper. In any case, most if not all presidents could likely be accused of the same campaign versus administration conflicts. The value in examining these conflicts here is not simply to report that politicians campaign ambitions are not carried into administrative action, but to examine why President Trump is not building a politics of reconstruction. 

Trump often has the rhetoric of reconstruction politics, but his actions betray disjunctive hints. Skowronek (p. 38) adds this regarding reconstructionist presidents: “Reconstructing political order is a process that joins party building to an assault on the residual institutional infrastructure of the old order.” As I show above, President Trump’s actions have not assaulted the established political order, despite his rhetoric claiming otherwise. His reliance on Wall Street veterans to head economic councils, for instance, seems to preclude a Jacksonian reconstruction of the national banking system. Far from transformative or reconstructive, President Trump’s actions, and the response of both the Republican and Democrat national parties, point towards a time of disjunctive politics. 

Disjunctive politics are those times Skowronek defines as when the established order’s instinct to protect the status quo politics of the time, “becomes at these moments a threat to the vitality, if not survival, of the nation, and leadership collapses upon a dismal choice” (emphasis added, p. 39). In Skowronek’s formulation, while the office of the president is a “battering ram” (p. 28) in politics, only those presidents best situated in “political time” have been able to use that power. Skowronek’s “political time” refers to both the national political institutional environment prevailing at the time a president governs (weak or strong), and the president’s relationship to that environment (affiliated or non-affiliated). The politics of disjunction require a weak national political regime and an affiliated president; and so, the next section attempts to narrow those questions directly. 

### Weak or Strong; Affiliated or Unaffiliated?

The national regime established by the last transformative president, Ronald Reagan, is that of conservatism. The conservative regime is weak following the successive presidential tennis match seen in Bush/Clinton/Bush/Obama. All either failed to show the established regime was bankrupt of answers (pre-emptive cycles of Clinton and Obama) or attempted to re-establish the Reagan golden era (articulative cycles of both Bush presidents). President Obama’s failure to appoint a (winning) successor in Hilary Clinton only cemented his status as the latest pre-emptive president. 

The conservative regime is over forty-years-old now, and does not appear to be answering the questions faced by a United States of America of 2018 – a world vastly different than the one in which the conservative regime was borne. While a resurgent Russian bloc under Vladimir Putin may remind some of President Reagan’s political time, in fact he presided over the end of the USSR as a world power, not it’s birth. Trade, communications, technology, work force compositions – the list of the macro conditions which have changed both worldwide and nationally does not need full elucidation here for the point to hold. The conservative regime, accepted by both the Republicans and Democrats in the absence of a viable alternative, continues to hold together but not through its untapped potential for new answers. As Skowronek notes (p. 31), while the executive branch has vastly expanded in power, so too have the “institutions and interests surrounding it” as well. This makes even a “weak” constitutional regime difficult to reconstruct, as the “institutional thickening” makes the plans of even those presidents with the “most compelling warrants for independent action” difficult to achieve in the face of the equally thickened defenses of competing factions. 
So, if Reagan’s conservative regime is weak (though institutionally thick), then in Skowronek’s theory, President Trump could be either a reconstructive, if unaffiliated, or disjunctive, if affiliated with the conservative regime. This is not a question of President Trump’s ideological conservative bona fides, which have always been doubtful to many, given he travelled in liberal Democrat circles for most of his adult life. The question of his affiliation to the conservative regime asks a different question: Does he act in ways that would tend to support, or overthrow, Reagan’s conservative regime?

Both candidate and President Trump have argued that the modern regime is weak, though he would not say the “conservative regime” is weak. But that is, in fact, what he is implying when attacking the so-called “deep state” of the modern regime, as the modern American state has grown to the breadth and depth it is within the agar laid down by President Reagan. However, while Trump has argued, though not acted, against the modern political regime, he has absolutely affiliated himself with its founder. President Trump’s oval office is decorated with a large portrait of President Reagan, and he has affirmatively affiliated himself with the reconstructionist as both a candidate and a president. Candidate Trump has offered similarities including that both he and President Reagan are former Democrats (Richardson, 2015). President Trump, following the publication of a book critical of his presidency, tweeted that “Ronald Reagan had the same problem and handled it well. So will I!” (Schwab, 2018). 

### Conclusion

Some may counter that appeals to Reaganesque stature is simply a hallmark of Republican candidates seeking to establish themselves with conservative voters, and thus is a cynical political stance, not one truly held. This may be true, but in any case, ignores that every president since Reagan has also claimed to have affiliation with him.  Candidate Obama in 2008 made the comparison himself (Murray, 2008): 
"I don't want to present myself as some sort of singular figure...I think Ronald Reagan changed the trajectory of America in a way that Richard Nixon did not and in a way that Bill Clinton did not. He put us on a fundamentally different path because the country was ready for it."

As a Democrat candidate still engaged in a primary fight, Obama was not campaigning for conservative values. But Obama was a political actor in a still resilient political environment shaped by Reagan, and so while he opposed the political order, he was not able to cast aside the vision of leadership offered by Reagan, and still very much desired by the American electorate. Orthogonally, President Trump, though faced with a much weaker political regime following President Obama’s tenure, affiliates with the regime. No candidate – other than a reconstructive one – can afford to cast aside that vision of leadership.


## Public Opinion: Media Sources & Public Policy Impacts

How public opinion is formed, and its effects on public policy, has been an integral facet of American political since the 1940s, beginning with the famous Columbia school of American political behavioral scholarship. These studies pioneered survey research in America, and eventually produced two influential books – The People’s Choice: How the Voter Makes Up His Mind in a Presidential Campaign (Lazarsfeld, Berelson, & Gaudet, 1948), and Voting: A Study of Opinion Formation in a Presidential Campaign  (Berelson, Lazarsfeld, & McPhee, 1954). These two works were the first of the Columbia school of American political though, forerunners of a disillusioned view of the American electorate as irrational and not up to the task of running a democracy (Bartels, 2008), and have guided political research in public opinion for decades after (Bartels, 2010; Delli Carpini & Keeter, 1996; Zaller, 1992). 

The Columbia study proved the value of survey-based research, and soon after scholars at the University of Michigan began their own pioneering work. The national survey work in Michigan began in 1948, and was repeated every four years after, eventually producing “one of the longest-running research projects in the history of academic social science” (Bartels, 2010). The research produced what eventually became known as the “Michigan Model” and after the first decade of work resulted in the The American Voter  (Campbell, Converse, Miller, & Stokes, 1960). The book is the signature work of the Michigan Model, and has remained a hallmark American political study. The book’s classification of party identification and candidate attractiveness as primary motivations of the American voter have remained important through today.
While the Colombia School and the Michigan Model proved remarkably influential, it should not be surprising that their findings were not as consistently accurate as perhaps their methods were. The remainder of this essay will address later work in two specific areas. First, whether mass media affects public opinion, and to what extent, and second, how public opinion might affect public policy. 

### Media Effects

Like its methodological influence, the Columbia study’s findings that mass media had little effect on public opinion persisted for decades. The Colombia study used long-term panel studies to establish that even in the face of substantial mass media input, public opinion appeared to be remarkably stable, leading researchers to conclude there was no effect at all (Klapper, 1960), let alone the large effects predicted for mass media. The ‘no effects’ finding was accepted in the field for decades, and not until the late 1960s and 1970s did researchers began accepting there were probably small effects from media on public opinion, particularly with the agenda-setting function of the media (McCombs, 2004; McCombs & Shaw, 1972). Despite these early entries, the same period still saw strong arguments from some scholars that the growth of media did not mean there was a growth in media effects (Sears & Whitney, 1973). Though these arguments predate Zaller’s (1992) ‘Receive-Accept-Sample’ model of public opinion, they were important for their theorizing of selective exposure and perception, parts of theories of motivated reasoning that are central to understanding the effect of media on public opinion (Bolsen, Druckman, & Cook, 2014). Selective exposure effects were likely to cause the public to not consume information that conflicted with prior belief, and selective perception effects would result in the public dismissing conflicting information even if exposed. This effect is a strong feature in recent research into the increasingly partisan and polarized American electorate (Iyengar, Lelkes, Levendusky, Malhotra, & Westwood, 2019). For example, a positive frame in media can cause the public to dismiss negative information (Druckman & Bolsen, 2011), though members of the public with strong prior beliefs appear less susceptible to media framing effects (Lecheler & de Vreese, 2012, 2018; Lecheler, de Vreese, & Slothuus, 2009).

As rational-choice theories in political science more broadly began to gain adoption (Aldrich, 1976), the question of media effects was still outstanding, as it did not appear that there was a rational connection between media consumption and public opinion. Into that frame, Page and Shapiro (1987) offered new methodology and direction for researchers with their research presented in “What Moves Public Opinion.” The authors improved methodology in the field by pairing specific survey questions asked at different points, and selecting the questions where there was a significant (> 6%) difference overall public opinion. Instead of simply measuring self-reported media consumption, they looked at media stories which occurred between the two points in survey questions. They also coded the media for the source quoted in the media, and how prominent the story was by looking at how early in the news broadcast the story was featured. 

Page and Shapiro (1987) argue that media does have an effect on public opinion, but that the information must be received, understood, relevant, credible, and at odds with their previously held beliefs. While their findings were somewhat limited by a small sample of approximately 80 cases, the paper was significant for finally locating substantial media effects on public opinion, after four-decades of researchers failing to do so. The findings are difficult to assign causation to given the nature of the survey research, and the authors stretch too far in drawing some causal relationships where they are not present, such as finding that news commentators drive public opinion, when it seems more likely the news commentators are following broad public opinion changes. Page and Shapiro followed up their 1987 work with a more full rational choice accounting of public opinion (1992). Using data that speaks to a wide range of policy domains, the authors hold a focus on aggregate public opinion, which appears to be rationally connected to important events. On the whole, public opinion is stable and the change that does occur is generally rational, even when the individuals in the system do not necessarily appear to be so. 

More so than any other piece, “Experimental Demonstrations of the 'Not-So-Minimal' Consequences of Television News Programs” (Iyengar, Peters, & Kinder, 1982) revitalized academic interest in media effects on public opinion. Using relatively complex experimental methods, the authors exposed their treatment group to ‘doctored’ news broadcasts which changed the topics covered, and how early they were broadcast in the segment, when compared to the control group. Experimental methods were not well accepted at the time in political science, a fact well documented by the amount of time the authors spend justifying their methodological choices. The experiment was successful in showing strong agenda setting effects in the media – or the ability for stories earlier in the broadcast to convince viewers of the importance of those stories. The further finding that viewers who were less sophisticated in their political knowledge were more susceptible to the agenda setting effect was to be important to later researchers examining differences in how opinion shifts in the public. The findings were not perfect however, and in particular their argument for media priming the audience, or how the media might “alter the standards by which people evaluate government” was not as convincing in the long run, and the logic was substantially improved on later by Zaller (1992) with his “receive – accept – sample” model, particularly in how people process conflicting messages in the media. Iyengar continued to make important strides in this theme of research (Iyengar, 1990, 1991; Iyengar & Kinder, 1987) for several years, generally finding that the media (particularly television media) has agenda setting and priming capacities.

Later work (Baum, 2002) builds on these findings, and demonstrates that consumption of ‘soft news’ and entertainment television manages to transmit political information - but typically of foreign crises, and not the less dramatic national politics. This suggests that barriers to political information for low-engaged citizens may be falling in an era where infotainment is increasing. ‘Soft’ news and entertainment transmits some information about high profile events, particularly international/security information, to the low-information, low-engagement voters that have concerned some scholars for decades (Converse, 1964; Delli Carpini & Keeter, 1996), and is further evidence that there are alternate methods and heuristics that the public is able to use to come to opinion sets that closely match those of more highly engaged political consumers (Lupia, 1994). 

Though unable to overcome the identification problem, Page and Shapiro (1987) were important for providing a basis to understand that the media interacts with other political actors to influence public option. They presented at least some evidence that presidents are able to harness the media to persuade the public, at least when presidential job approval ratings are high. Later scholars suggested that the presidential ability to move public opinion was curtailed by the rise of cable television news (Baum & Kernell, 1999), but on balance the research showed that the effects were more complicated. While the president has been a reliable source of cueing public opinion (Mondak, 1993), news media can direct public evaluation of presidents when the news source is trusted (Miller & Krosnick, 2000). 

With the theory that media has more than negligible effects on public opinion well established by the 1990s, researchers began to investigate how the tone of media messages might affect public opinion (Ansolabehere, Iyengar, Simon, & Valentino, 1994). This work heeds the call of earlier research (Iyengar et al., 1982) for methodological pluralism. First in the experimental stage, they showed that a treatment group exposed to negative political advertising were less likely to intend to vote. In the second phase of their research, they took that model and texted it against voting and political advertising data from the 1992 senate elections. The experimental results were validated empirically, with the second phase finding that turnout in senate races with more negative political ads was by approximately the same amount (~4%) as the intent to vote in the experimental phase participants. In using field data to validate their experimental findings, the researchers were successful in making a convincing argument that voter demobilization is due to general cynicism - "as campaigns become more negative and cynical, so does the electorate." That finding has proved controversial, however, as replicating the findings has had only mixed success, with the result that while it is probably true that negative campaigning can depress voter turnout, the amount of depression is likely not as large as the original findings suggest.

Barabas and Jerit (2009) show how far the literature has changed from the days of the ‘no effect’ findings of the Columbia study. The study uses sophisticated Bayesian analysis to estimate effects of the media on public opinion, but more importantly what political information is transmitted to the public through media. The authors add to the ‘media has effects’ literature, with additional findings related to how the media can transmit political information to voters. What would seem to be the most obvious determinant of media effects-the volume of coverage-is not the only or even the most important predictor of knowledge. The breadth of coverage and the prominence of a story are equally powerful predictors of knowledge and are more important than demographic characteristics or indicators of socioeconomic status. These findings continue to be impactful, though with more time to study these effects, they appear to be weaker than the authors determine here.

Another line in the literature of media and public opinion seeks to understand the media’s role in the polarization of the public. Before reviewing the contesting claims, however, it should be noted that whether there is polarization at the mass public level is a still contested question. Polarization among elites and highly partisan members of the public is a well demonstrated phenomenon in modern American politics (Poole & Rosenthal, 1984). However, other scholars argue forcefully that polarization at the elite level is not new and has had a consistent presence in American politics (Lee, 2009; Nivola & Brady, 2008), and that it is the relative balance of power in the two parties that is responsible for how the institution of Congress acts, not ideology or polarization (Lee, 2016). The focus on income inequality and its link to political outcomes has a strong research tradition (Bartels, 2010; Schattschneider, 1960). McCarty, Poole, and Rosenthal (2006) extends earlier work by some of the same authors (Poole & Rosenthal, 1984) by causally linking increasing income inequality and immigration to increasing polarization. Still, some of the most recent review of the scholarship finds that mass polarization is occurring, and that it is affecting how American’s behave in non-political realms (Iyengar et al., 2019).

So, setting aside the question of whether polarization is a widely observed public phenomenon, assuming it exists, is the media responsible for it? With the existence and extent of mass polarization still contested, it is not surprising that the mechanism explaining the proposed relationship would be controversial as well. The early entry in this area of research was not on polarization exactly, but on the broader social division that could be exacerbated by narrowcast media (Mendelson & Nadeau, 1996), which was just beginning to emerge in the forms we recognize to day as overtly partisan media outlets such as Fox and MSNBC in cable news, as well as the many forms of small media firms that take partisan sides. Hacker and Pierson (2005) note that overall mass partisanship had not changed much, although conservative policy had moved far more right than the median conservative voter or lawmaker. Most American are politically moderate, even on highly salient political issues such as abortion, gun control, and gay marriage (Fiorina, 2017), and the conception of America as highly polarized is primarily driven by media reinforcing messages of the political elite. This may show the primary effect of media on polarization is still limited to political elites (Prior, 2013), who tend to be more ideological than the typical voter from the same partisan family. 

In any case, the evidence that the media is responsible for what some see as a dangerously increasing polarization in the public (Iyengar et al., 2019) is quite thin, and measurement issues continue to hold back our understanding of the relationship, should it exist. The most likely explanation for the correlation between increased partisan media is that technology made it easy to enter small, discrete media formats in order to monetize the strong partisans already in existence who were generally unhappy with the more moderate news media that dominated in the 1970s. Selective exposure then occurs, as strong partisans increasingly turned to media channels and sites that were congruous with their pre-existing beliefs. In other words, if anything, media followed partisan members of the public, not the other way around, and at this point (Prior, 2013, p. 119), “Research to date does not offer compelling evidence that partisan media have made Americans more partisan.”

### Public Opinion and Public Policy

Whereas the scholarship discussed in the previous section was concerned with the effects on public opinion, the following section is primarily focused on the connection between public opinion and public policy. For many democratic theorists, there is a normative connection between the two – public opinion ought to affect policy if in fact the polity has control of government . Like the connection between mass media and public opinion, for many years the literature failed to establish correlation, let alone causation, between public opinion and public policy.
In the late 1980s and 1990s, two primary types of research in this area were established (Monroe, 1998) –  studies of “congruency” and those of “consistency.” In the congruency camp, the classic study comes from Benjamin Page and Robert Shapiro (1983). Using trimmed survey data culled from multiple decades, the authors used similar methodology to their research on media and opinion (1987), looking for similarly worded public survey questions on policy, and then correlating policy shifts from before the first survey and after the second survey. In this way they showed that indeed, in the aggregate at least, there were relatively higher correlations (around 64%) than existing theory would expect between the public opinion and public policy shifts. The authors were appropriately reserved in their own judgement, given the lack of any causal connection. Further, in approximately one-quarter to one-third of the cases, public policy shifted in a direction opposite the public opinion direction, leaving a great deal still to be explained.

In contrast to the congruency approach of Page and Shapiro (1983), research by Alan D. Monroe (1998) highlights the consistency research in public opinion’s connection to public policy. For Monroe, consistency research compares the “distribution of public opinion with the policy outcome,” and compares two periods, 1960-1979 and 1980-1993, and finds that in the later period public policy’s consistency with public opinion had dropped from 63% to just 55%. He attributes this relatively low number to the inherent bias against change in the complex American political system. Monroe added further to this literature by examining consistency not just in the aggregate, but also stratified by substantive policy area. This allowed him to show very interesting results, such as his report that in foreign policy, there is 100% consistency between opinion and policy in the latter period studied. Again, like Page and Shapiro, Monroe is not able to establish a complete causal connection, and given the nature of the research problem, modern research has still failed to convincingly do so. 

The study of state electoral politics in order to better understand how public policy forms is a central lesson throughout political science (Key, 1949, 1963). Erikson’s early work (1976) evaluates data from the 1930’s and showed that public opinion and state policy were strongly correlated on death penalty and child labor law issues of the day. Erikson, Wright, and McIver (1989) expand that early work with an interesting variation on research into public opinion and public policy, locating the discussion in state-level research.  Their basic finding is that state electoral politics is the most important factor in correlation between state opinion and state policy. The authors propose a new model of electoral representation, in which centrist parties are rewarded. The authors are rehabilitating and improving on Downs (1957) original argument that political parties will eventually migrate to the center of public opinion. But Down’s original model was too simple, and the actual electoral politics make the path between public opinion and policy outcomes more complex. Erickson, Wright, and McIver demonstrate this with their own complex path model. They demonstrate that ideological variation between states makes comparisons very difficult - i.e. a Mississippi Democrat is probably more conservative than a New York Republican. State policy outcomes do represent state opinion preferences, but party control of a state legislature is not a good predictor of state policy, as the party activists and the centrist voter tend to be pulling politicians in different directions than the national party. This connection is an electoral one (Mayhew, 1974), as voters reward legislative control by selecting party control based on ideological position, leading politicians to pay close attention to shifts in public opinion (Erikson, McIver, & Wright, 1993). 

Showing there is still considerable research and debate in the public opinion and public policy field, Jason Barabas (2004) uses Bayesian theories of information updating to propose a model where "citizens who deliberate revise their prior beliefs, particularly when they encounter consensual messages." This work builds upon the ‘receive-accept-sample’ model (Zaller, 1992), but uses thoroughly different methodology to do so, and attempts to answer some of the outstanding questions in the literature. One finding in the deliberation literature is that on the individual level, people do have opinion shifts following deliberation, and “deliberation increases knowledge and alters opinions, but it does so selectively based on the quality and diversity of the messages as well as the willingness of participants to keep an open mind." 
Barabas’ (2004) findings are in line with earlier meta-analysis that shows media messaging is translated into measurable learning, influences attitudes, and shapes behavior in the public (Emmers-Sommer & Allen, 1999). It is further supported in more recent research (Coppock, 2016) that finds that persuasive political messages elicit small, homogenous changes in beliefs. But in the aggregate, as Barabas (2004) shows, there is not much change, and even the individual shifts tend to be short-term. ‘Short-term’ is still contested and relatively poorly defined, as experimental work Coppock (2016) shows that the changes persist for at least some period that could conceivably effect policy. Despite Barabas’ careful methodology and analysis, in the end his findings run into the same problems as previous deliberation research. 

### Conclusion

The consistency in public opinion with public policy matches long-standing findings from participation research (Nie, Verba, & Petrocik, 1976; Verba & Nie, 1972) that American voters hold consistent attitudes, understand and identify with the ideological frameworks in operation nationally, and vote according to those ideological preferences. In other words, there is a consistent thread connecting how the public votes, how they understand public policy, and how they affect that policy through political participation. At the aggregate level, public opinion is stable, and when shifts do occur they are met by lawmaker response through legislation that matches the new public policy preference (Page & Shapiro, 1983; Shapiro, 2011). This connecting thread is a public policy variant of the electoral connection (Mayhew, 1974). The causal pathway is not always clear however, as the concept of low-information rationality shows voters also rely on framing messages from politicians (Popkin, 1991) and other sources (Lupia, 1994) as heuristics for forming their own opinion (Lupia, McCubbins, & Popkin, 2000).

The study of public opinion and its hypothesized effect on public policy continues to be a substantial research topic in American political behavior. On the whole, there is enough evidence to believe that there is indeed a connection between public opinion and the formation of public policy. However, future research is still left with huge questions, including the primary one, which is investigating the causal connection. While Mayhew’s (1974) “electoral connection” can be relied on to a certain point, it is not clear past election how exactly policy shifts in response to changes in public opinion. Even more strikingly, in an America where so much policy work is done outside any direct electoral connection – in courts, administrative organizations, and in public-private and public-non-profit contexts – there is even less obvious causal link to explain policy shifts. 

## Voting and Elections

The ability of citizens to influence, or even control, their form of government is at the heart of democratic theory, and explaining how people decide who, or what choices, to vote for forms perhaps the largest body of research literature in the American behavior field. Because the body of work is so large, this essay cannot hope to cover it in detail. Instead, it will proceed in three sections. First, the three largest academic traditions which have shaped the field will be briefly considered. Second, the role of partisan identification in vote choice will be reviewed through three important works. Finally, the role of political campaigns and theories of decision making in vote choice are reviewed through a brief examination of Lau and Redlawsk’s (2006) novel experimental approach to “correct” and “incorrect” voting.

### Traditions of Vote Choice Scholarship

Three broad traditions have emerged out of political scholarship to explain how voters choose in elections – the Columbia school, the Michigan model, and rational choice. All three theories of voter choice offer competing explanations, and to a great degree have shaped how scholars approach vote choice research questions. 

#### The Columbia School

The Columbia school studies were based in sociological traditions. Research coming out of this school predated the ability to conduct large telephonic or web-based data collection. Instead, researchers delved deeply into the community in which voters lived. Not surprisingly, then, researchers found that voting is a group act – that is, it was formed and influenced by the social groups in which people lived. Columbia school studies tended to be long-term longitudinal studies over the space of a single election, based in very specific geographic locations. The studies tended to discount the role of media, election campaigns, or other correlates which occurred outside the very concrete, observable social groups to which their subjects belonged. The explanations fit well within the design constraints these researchers operated within, but tended to be myopic and non-representative, as the findings didn’t travel well to other locales, or to the nation as a whole. 

#### The Michigan Model

The sociological approach of the Columbia school was to be quickly supplanted by the social-psychological approach of what came to be known as the Michigan model. This literature was begun by a group of political scientists and one social psychologist (Campbell, Converse, Miller, & Stokes, 1960).  These scholars centered their research quite differently compared to their Columbia school colleagues. Whereas a Columbia school researcher might ask “which groups do you belong to?”, Michigan research was more interested in “which groups do you think you belong to?” This slightly different formulation was forced somewhat by choice of method, and allowed for very different answers, and different models. Michigan model research was based on large telephonic survey methods, so researchers could not directly observe the local social groups a respondent might participate in, and so had to rely on which groups a respondent reported they belonged to. The community context that was so important to the Columbia school was stripped away, and group identity emerged instead of group membership. The Michigan model benefitted from the very large samples they were able to produce, and their findings tended to be more nationally and regionally representative than the smaller Columbia studies. 

While the Michigan model covers decades of work, the basic model which came out of The American Voter (Campbell, Converse, Miller, & Stokes, 1960) can be summarized as three main components which have a direct effect on voter choice. The single largest factor is partisan identification, defined as which political party a voter says they more closely identify with. The next largest component of vote choice is candidate image, or how favorably or unfavorably a voter considers a political candidate. The third, and much smaller facet of vote choice is a voter’s reported position on the issues which are in play during the election.

The Michigan model was not a complete break from the Columbia scholarship, and the move from sociology to social-psychology is better thought of a refinement than a reinvention. For example, while Columbia scholars never directly named “partisan identification” they talked about various aspects of it. Still, if forced to choose, the Michigan model has proven the more lasting tradition and the dominant model of vote choice. Even today political scientists still measure partisan identification and candidate image variables in the same ways that Michigan scholars were over fifty years ago. The Michigan studies of national voting behavior have continued uninterrupted and are now known as the American National Election Study (ANES), which continues to be an important source of data for modern vote choice scholars. 

While dominant, the Michigan model is not without critics. The largest critique is the model’s treatment of issues, which end up as a distant third in importance compared to partisan identification and candidate image. In fact, issues don’t matter much at all, but this has been critiqued because of the way Michigan researchers conceptualized what an “issue voter” is. Their method was overly rigorous, as it required a respondent to know where the political parties stood on an issue, where the respondent themselves stood, and had to be able to accurately differentiate these stances across a wide variety of issues. The issues themselves were selected by researchers, which led to a secondary critique that the Michigan model lacks any measurement of issue salience – what is important to the voters themselves as they consider their vote? Later scholarship has shown that once people are asked about the issues that are important to them, the issue of issues in voting looks very different. “Single issue” voters can be identified, and for these voters, issue stance is the most important determinant of vote choice, which is a finding the Michigan model overlooked due to their research design. Still, most research has consistently found that the “typical” American voter is not an issue voter, lending further credence to the Michigan model as a whole. 

#### Rational Voter Choice Models 

While rational choice theory was adopted throughout other areas of political behavior study, it wasn’t initially a popular theoretical structure to explain voter choice. Initially, the conclusion of many attempts to explain voting behavior such as turnout with rational choice (Aldrich, 1993) led to the unhelpful conclusion that voting wasn’t in the voter’s interest. Morris Fiorina (1978) caused some controversy when he claimed he could use rational choice methods to predict voter choice just as well as the Michigan model. Fiorina’s essential claim was that when faced with a choice in an election, the voter essentially asks “who will benefit me the most,” and was forming that belief based on which candidate or party had most benefitted them in the recent past. Critics pointed out that what Fiorina had done was essentially take partisan identification – the taproot of Michigan model theory – and renamed it in a rational choice friendly way. In the end, rational choice still struggles to explain vote choice in a compelling way, due to its roots as an economic model. Attempting to explain voting in utils is always going to be at a disadvantage in explaining behavior with social and psychological roots.

### Partisanship and Vote Choice

The 1979 article “A Dynamic Simultaneous Equation Model of Electoral Choice” by Gregory Markus and Phillip Converse is an excellent example of how the Michigan model conceives of vote choice. The authors lay out a very complex series of regressions (a path model) among the most typical correlates, including the “big three” of partisan identification, candidate image, and issues. The study uses panel ANES data from the 1972 to 1976 presidential election, and results of the model claim to be able to correctly predict 90% of the vote choice by respondents. The authors are responding to other critics, including Fiorina (1978) who claim that a respondent’s vote in the previous election is enough to predict a large proportion of votes in the current election. Markus and Phillip (1979) include the respondent’s previous election vote into their model and show that it isn’t as predictive as the other scholars believe. Voters occasionally switch party in their votes, but a Democrat voting for Regan in 1984 is (usually) still a Democrat in 1988. In addition to the normal focus on partisan identification as the predominant causal mechanism for vote choice, Mark and Phillip’s model also concentrates heavily on candidate image and does a better job than previous Michigan model work with issue saliency. While a good example of the Michigan model, this paper has been criticized later for the relatively small sample (n=884), but this is primarily an artifact of the panel data used by the authors. Further critiqued is the authors’ choice of regression modeling, as they use ordinary least squares (OLS) regressions to model a dichotomous vote outcome. This choice was popular though for some time, and even replicating the study with the more appropriate probit or logistic regressions would be unlikely to change the direction of effect between independent and dependent variables. 

Another preeminent American political scholar is Larry Bartels (2000), and in “Partisanship and Voting Behavior, 1952-1996” he analyzes partisan voting in American elections. Bartels uses probit regressions of ANES data to address what some scholars believed was a trend in American’s voting less frequently on a partisan basis. 1972 proved to be a low-point in partisan voting, as popular Republican President Nixon handily won re-election in a country that was still safely Democrat. Bartels finds that “strong party identifiers” decreased for a short period between 1964 to 1976 but rebounded strongly since then. Partisan “identifiers” decreased as a proportion of Americans overall since 1964, but have increased as a proportion of voting Americans, and Bartels shows that partisanship among the voting public is at a level similar to those found in the immediate post-war period. Bartels contributes as well with his discussion on how to measure partisan identification, and points out that while a sizeable percentage of respondents will answer questions about their partisan leaning, many will not answer corresponding ideological questions. This may be at least partly due to the trend in ANES data noted in other work (Sinclair, 2012, p. 78), which finds that “11 percent of self-identified liberals voted for the Republican presidential candidate, and 29 percent of self-identified conservatives votes for the Democratic presidential candidate.”

While the data Bartels uses is mostly focused on presidential elections, he finds similar increases in party-basis voting in Congressional races as well. Bartels theorizes that the increases in partisanship-based voting (77% higher in 1996 than in 1972, and up to 20% higher than in the post-war period) is due to increasing differentiation between the political parties, particularly as the Democratic party’s dominance in the southern United States degraded. Bartels is successful in rehabilitating partisanship, as he shows that once models correctly take into account who is voting, not just who is eligible to vote, party identification remains a primary motivator of vote choice.

The early Columbia studies found that election campaigns don’t matter much at all, while later scholarship has found that negative campaign messages can depress voter turnout (Ansolabehere, Iyengar, Simon, & Valentino, 1994), that direct mailers from campaigns could theoretically increase turnout (Gerber, Green, and Larimer, 2008), and that door-to-door canvassing by campaign workers from the local community could increase it even more (Sinclair, 2012). But one ongoing and current debate in vote choice studies is what effect, if any, election campaigns have on the choice a voter makes once they actually turnout. Hillygus and Jackman (2003) address this question in “Voter Decision Making in Election 2000: Campaign Effects, Partisan Activation, and the Clinton Legacy.” 

Partisanship tends to matter a great deal at election time, but typically has less salience during non-election years. Hillygus and Jackman try to measure how election campaign events such as party conventions and political debates affect how voters feel about candidates George Bush and Al Gore in the 2000 presidential campaign. This study contributes usefully in its method. The authors “piggyback” on a private marketing survey which samples web-based respondents multiple times per day. This allows the authors to conduct a time-series study of candidate image throughout the campaign, measuring aggregate differences before and after the campaign events they are interested in. The study finds, for instance, that Al Gore “won” the conventions and George Bush “won” the debates. While this is useful, in the end this study ends up reinforcing earlier findings that election campaigns just don’t seem to make a big difference. Voters make up their minds about candidates very early, and while there are small aggregate shifts throughout the campaign, in the end partisan identification and candidate views don’t change substantially. 

### Campaigns, Information, and Vote Choice

In their book “How Voters Decide: Information Processing during Election Campaigns,” Richard Lau and David Redlawsk (2006) usefully provide an overview of the predominant models of voter choice, and then compare those models using novel experimental treatments to see how voters process information as they make election vote choices. In addition to the rational choice model, the authors compare Herbert Simon’s model of bounded rationality and satisficing; a “cognitive consistency” model loosely based on the sociological Columbia model, although the theorists never directly use that phrase; and finally, a heuristics short-cuts model. The Columbia model has enough overlap with the Michigan model that some comparison can be made in the results presented by the authors.

Lau and Redlawsk want to test the four models of vote choice, using a unique definition of “correct voting” tested with experimental methods. For the authors, a “correct vote” is one in which the study subject votes for a hypothetical candidate in a time-constrained environment with too much information (modeling a campaign where voters are inundated with election information), and then votes again for that candidate later, in a non-time bounded environment, where the subject can study a candidate at great length.  In those cases where a study subject switches their vote in the second vote, they are considered to have cast an “incorrect vote” in the first vote. In this study, up to 30% of voters cast “incorrect votes.” 

The experiment’s findings reveal that rational choice decision models don’t appear to help the voters in decision making. The psychological strategies, Simon’s satisficing and the heuristics or shortcut model, improve voter decision making – that is, improve “correct voting,” while the sociological model is only useful in the most simplistic scenarios, which are unlikely to be found in national elections. In elections with more than one candidate, which are presumed to be more complex, the effect of partisan identification becomes even more important as a shortcut, lending credence to the earliest Michigan model findings.

Lau and Redlawsk’s work is not without limitations. Their findings are subject to the usual limits of experimental research design, in that it is not clear to what degree their findings are portable to the “real world” of political campaigns and voter decisions among candidates. However, the strength of their experimental methods allows scholars to make useful distinctions between the different models, which often have overlapping theoretical edges. Lau and Redlawsk are able to claim strong causal links, make useful distinctions between the models, and reject at least one in rational choice. While no single model dominates vote choice, the experimental results are most clear in rejecting the rational choice model as useful in explaining vote choice, and voters are likely using some combination of the other three, with psychological aspects playing a dominant role.

### Conclusion

How a citizen decides who to vote for at election time remains a stubborn question. There are now decades of evidence that partisan identification plays a central role in vote choice, and the Michigan model remains the best theoretical structure to understand it. It is somewhat surprising just how robust the Michigan model has proved, given its relatively advanced age. Advancing experimental techniques, such as those used by Lau and Redlawsk (2006), point the way towards more refined theoretical sensitivity which may help with greater understanding of vote choice, and the psychological aspects of decision making offer promising future refinement as well. Given the ever-escalating political stakes, and the saliency of partisanship even in off-election years, the subfield of vote choice is likely to remain a core research agenda within American political studies.

## What do Americans Know about Politics?

In response to: 

> Do average Americans really know enough about politics to participate effectively in the process of democratic decision making? A not unreasonable assumption about democratic governments is that citizens need to be sufficiently informed about political institutions, important political actors, and the substance of public policy to be able to understand and evaluate both their own interests as well as the interests of others. Based on the research that you have read about what Americans know about politics and how citizens make political decisions, how would you evaluate whether most Americans are sufficiently informed about politics to participate effective?

Questions about how citizens develop their basic political beliefs, perceive political issues, and participate in the political process are at the heart of the study of American political behavior. In the democratic ideal, the citizen is a well-informed participant in the political realm, able to effectively consider a wide-range of information about a wide-range of topics and distill that information into a well-reasoned act of political participation. However, this normative ideal is loaded with assumptions that are worth testing empirically. This essay will consider a wide range of political science literature which examines the components of the ideal political citizen. Broadly, it will consider whether most Americans are sufficiently well-informed about politics in order to participate effectively. The essay will progress in three broad parts, first examining how Americans learn about politics, and continuing then to discuss what Americans know about politics, and finally, whether American’s know enough about politics to perform meaningfully in political processes. 

### How Americans Learn Political Information

Prior to developing a political opinion, value, or belief, at least some information must be gathered by the individual. The earliest political science inquiries into how Americans learn about politics were developed in the sociological tradition in the years after World War II. Scholars were focused on trying to understand what had made America and Great Britain different from the Axis power countries. How had Americans and the British managed to resist totalitarian and fascist takeovers of their governments, while Germans and Italians had not? For political scientists of that era, the “obvious” answer was that there must be something about how America and Great Britain were raising their children that imbued them with a natural political resistance to despots.  Political researchers, borrowing from sociological theory popular at the time, attempted to test that theory.

Two of the earliest examples of the socialization approach to political knowledge are Fred Greenstein’s (1960) “The Benevolent Leader” along with “Transmission of Political Values from Parent to Child," by Kent Jennings and Richard Niemi (1968). Still researching in the era before broad national telephone surveys were a realistic data gathering method, Greenstein directly interviewed school-age children, and found they have more positive views of politicians, what Greenstein described as “unqualified sympathy,” than their parents did. Though still cited in modern work, Greenstein’s early attempt to locate political information as springing directly from parental political views was not very successful. Kent and Jennings (1968) were more successful in showing that at least some political knowledge in children was correlated with parental political belief, but really only in terms of partisan identification. 

These early sociological attempts, while influential, were not convincing on the whole, as there was a clear gap between theoretical expectations and empirical evidence: children did not simply adopt their parent’s political views.  In addition to these empirical problems, the theory of socialization had a substantial problem in that it expects that by adulthood our preferences (in this case political preferences) are fixed. This expectation was undermined however, with increasing evidence that people maintained a life-long openness, and socialization continued throughout adulthood, as individuals continued to try to fit-in with their work, neighborhood, and other social environments. 
Into the empirical and theoretical gap left by the sociological scholars, the growing popularity of theories of rational choice were being used profitably across much of political science, but did not prove as useful in studies of the formation of political knowledge. While socialization would for the most part fade as a favored theory of political knowledge, there are occasional attempts to resurrect at least pieces of it. Sears and Valentino (1997) use elements of socialization to examine how exogenous political events such as presidential campaigns help form persistent political attitudes. This type of socialization is taking place outside the family, and comports more easily with other scholarship (Inglehart, 1971) which finds generational shifts in political beliefs and values. Inglehart and Abramson (1994) provide more evidence of this type of generation shift having a formative impact on political knowledge, which still has a socialization root. The authors find there is a cohort effect within generations which is affected by the environmental conditions the cohort experienced as children, and that as a society becomes more prosperous, and thus less likely to be worried about base survival needs, their political views as adults will tend to be post-materialist. While not technically a socialization theory, but uses theories of the socialization of information profitably in a political context, in that they allow for a theory of political knowledge that allows for events outside of the immediate choices in front of a person, as expected by rational choice theory. 

A common, and contemporaneously relevant, explanation for how people learn about political information is that they do so from what they are exposed to by varying media sources. The earliest models of political knowledge from the Columbia school studies discounted the effect of mass media on political knowledge. However, by the late 1980s and early 1990’s scholars were successful in locating at least modest effects of media on the political beliefs of Americans. By this time, rational choice theory was increasingly adopted by political scientists, but with relatively less success in the specific question of political knowledge formation. Page, Shapiro, and Dempsey (1987) examine a small sample (n = 80) of pairs of public opinion polls over time and compare them to the media coverage of issues covered in those polls. They find that the views of political elites, (such as experts, popular presidents, media commentators), transmitted through mass media, can affect public opinion. While influential, this study is hampered by the small sample and relatively short time-frames used by the authors.

One of the most comprehensive models comes from John Zaller (1992) in his book The Nature and Origins of Mass Opinion. Zaller gives an excellent frame for understanding the effects of media on not only what Americans know about politics, but how that knowledge is learned. Zaller’s model of how political opinions are formed starts with receiving information, which is often from political elites through mass media. People must then decide to accept or reject that information, and the final step before forming a political opinion is then to sample from the most recent information that person is aware of. This final sampling step is often a decision by the person made by reflecting on the information learned from mass media sources. This is a dynamic process, and recency matters a great deal. Zaller’s insights and modeling have had a broad, long-lasting impact on how we understand the formation of political knowledge, belief, and values. 

### What Americans Know About Political Information

Quite separate from the question of how we learn political information is the question of what Americans know about politics. In this area normative democracy theorists will not find much peace of mind. Some of the original research in this area from Converse (1962) showed that American’s frequently have little political information with which to base their partisan identification, and lack coherence when asked open-ended political questions (Converse, 1964). 

In What Americans Know about Politics and Why it Matters, Michael X. Delli Carpini and Scott Keeter (1996) follow Converse, and carefully and thoroughly undermine the idea that the average American knows much at all about politics. The crux of the book is that the distribution of political knowledge is very uneven. Americans from higher socio-economic backgrounds, older Americans, white and male Americans, in part or in sum, all have a fairly good basis of political information. However, if Americans who are young, from the inner city, black, or female,  tend to fare worse in tests of political knowledge. These differences matter not only for knowledge, but for rates of participation, and even the effectiveness of that participation. One area where the authors have been critiqued is that they do not distinguish what types of knowledge might be more useful for different demographic groups. For instance, while a white, middle aged male might have his political needs served by certain information (the type the authors test for), a young black woman may have very different political information needs. Delli Carpini and Keeter do an excellent job documenting their research process, and the methods used to come to their conclusions. While their argument stretches in some areas (see Baum, 2002, below), overall they make a good case for why we should be concerned about what Americans know about politics.

One example of how John Zaller’s (1992) work has influenced later research is Matthew Baum’s (2002) “Sex, Lies, and War: How Soft News Brings Foreign Policy to the Inattentive Public,” which helps explain how the media affects what Americans know about politics. Baum shows that “soft” infotainment shows, which regularly cover the big political foreign crises of the day, help shape Americans’ views of foreign policy. This helps explain one of the counter-intuitive findings from Delli Carpini and Keeter (1996) that Americans possess more information about foreign policy than domestic policy. 

### Political Knowledge and Meaningful Participation

Do Americans have enough factual information to be able to participate meaningfully in their American democracy? The findings of Delli Carpini and Keeter (1996) along with others would argue that Americans do not have the political knowledge to participate fully. Because political information is distributed unevenly across demographic groups, groups with less access to political information participate less, and when they do participate, they are less likely to attain the political results they seek. This is reminiscent of the findings of Larry Bartels in his book Unequal Democracy (2016), which shows that while lower-income Americans tend to vote at similar rates as higher income Americans, their vote “counts” for less because it is given less preference by politicians. The finding that socio-economic status influences the impact of political participation is found across modes of participation, as shown in “Citizen Activity: Who Participates?” (Verba, Schlozman, Brady, & Nie, 1993), which found that lower income Americans were less likely to become political activists, and even when they did they used different (although sometimes more effective) communication methods than did activists with more financial resources. 

However, some scholars would disagree with the stark findings of the scholarship above. Lupia (1994) argues that voters don’t need to be fully informed “encyclopedias” in order to vote, and that less-informed voters use information shortcuts to vote in ways very similar to their better-informed counterparts. Lupia uses empirical evidence from a California initiative on insurance reform, a topic that most typical voters will have very little information about. Low-information voters participate meaningfully by relying to cues from third-parties such as advocacy groups and political parties to form their political opinion, which is then translated into voting that is not dissimilar to those who spend much more time delving deeply into political issues. However, this kind of “partisan motivated reasoning” has been found by other scholars to reduce the quality of political opinions (Bolsen, Druckman, & Cook, 2014), and to be more shaped by the power of prior belief than the accuracy of new information (Taber & Lodge, 2006). 

Americans must make political decisions in a complex, fluid environment, and much of the scholarship which finds lackluster participation and lower political efficacy is based in a somewhat elitist rational choice belief that “correct” participation or “incorrect” voting exists in the first place. Lau and Redlawsk (2006) undermine this assumption in How voters decide: Information Processing in Election Campaigns. The authors use clever experimental methods to test four models of how voters process information and make voting decisions. They have subjects make voting decisions on hypothetical candidates in a time-pressured environment, and then in the second phase allow those same respondents to collect information for as long as they need before making a voting decision. Subjects who change their vote between the first and second phases are considered to have made an “incorrect vote” in the first phase, while those whose vote is consistent between phases made “correct votes.” They find that in up to three-quarters of the time, their subjects were able to make “correct votes” despite not knowing enough. 

The methods used by Lau and Redlawsk are subject to critiques that their experiments are artificial environments, and so don’t necessarily tell us exactly how voters make decisions in real elections. However, the value of these experimental findings is that they closely mimic the frantic, bounded rationality (Simon 1972; 2000) and heuristic shortcuts (Lodge & Hamill, 1986; Taber & Lodge, 2006) real Americans in the real world must contend with when making political decisions. In many ways Lau and Redlawsk’s findings are supportive of Lupia (1994) and others who reject the belief that democracy can only function properly when American’s are participating with rational, well-informed political knowledge. 
### Conclusion

Americans are remarkable political creatures. They form political knowledge along socioeconomic cleavages, vote at low rates and participate in declining numbers in other participation modes, possess little accurate political information, and their political participation tends to have little impact. Yet, their political behavior is redeemed when the context of that political behavior is more fully considered. It is a privilege to have the time, resources, and desire to fully pursue political information, and that privilege is extended to relatively few in American society. Still, despite the privations of political knowledge, expertise, and participation, most Americans manage to “get it right” most of the time – they operate in political spheres remarkably well in the face of factors which in a more rational approach should convince them to not participate at all. That is not to attempt to dispose of the normative democratic ideal, but perhaps to soften the approach of its most ardent believers. We should still heed the warnings of scholars such as Bartels (2016), who rightly worry that growing economic inequality threatens democracy. While not yet a thousand-year proven success, the American democratic experiment has so far succeeded despite the flaws of its citizens’ political knowledge. Short of the ideal lies a political persistence in the American citizen which manages to know enough, participate enough, and succeed politically enough. 

## What is Political Science For?

> “Those who have handled sciences have been either men of experiment or men of dogmas. But the bee takes a middle course: it gathers its material from the flowers of the garden and of the field, but transforms and digests it by a power of its own.”
> Sir Francis Bacon, The New Organon, 1620

### Assessing the Discipline & Profession of Political Science

The authors in this reading selection seem to agree that political science scholarship is unbalanced on the whole, though the measures differ for each. All seek to redraw the contours of the scholarship landscape, and in doing so believe political science scholarship will better serve the public, and democratic ideals. Though their prescriptions (where existent) differ, the authors desire a more impactful political science scholarship – through methodological diversity, an embracing of teaching responsibilities, or new paths to addressing free-access goods. In looking backwards to identify gaps, and attempting to describe a path forward, all four authors ignore the paradigmatic shift coming to all social sciences through the development of deep-learning neural networks and artificial intelligence. In a future where a century of empirical data is reworked in relative moments by machine learning capable of descriptive inference at a resolution unobtainable by a human mind – what is a political scientist?

### Conversations

The four authors all trace selected histories of the political science field, and describe dialogic forms they believe are unbalanced. Robert Putnam (2003) seeks to hold quantitative methods in high esteem while raising the qualitative ones alongside. Robert Keohane (2008) argues for a value-laden political science, and a recommitment from scholars to teaching undergraduate and graduate students. Finally, Jane Mansbridge (2014) seeks to turn a field too focused on preventing tyranny towards discovering better paths towards “legitimate coercion” in order to find solutions for the free-access problems facing humanity. In a way, Rogers Smith (2015) goes further than the others and sees a political science too enamored of science, producing research without impact, and failing to serve democracy or the public.

Smith sees political scientists serving the goal of intellectual rigor at the expense of what is supposed to be an equally important goal – serving democracy. In chasing quantitative methods and rational choice theory, he argues, we have neglected “important political questions not amenable to such methods” (Smith, 2015, p. 368). Smith continues his analysis by equating this neglect with a failure to enhance democracy, and linking both to low public confidence in political science research. How then have the other sciences had success in both? Smith doesn’t appear to contemplate other social science scholarship which, while often being rigorously quantitative, still surfaces in every news cycle with findings that appear to interest the general public (assuming the preferences of the news directors who continue to select such stories are an appropriate proxy for “public interest”). 

Smith complains the sentinel works by Elinor Ostrom (1990) and Robert Putnam (2001), the two most influential political scholars in recent decades, “tend to celebrate many traditional American values and institutions, to express concern about loss of older civic virtues, and most importantly, to favor decentralized, civil society solutions to common problems – not big government, certainly not any radical egalitarian reform agendas” (Smith 2015, p. 369). This is a strange complaint, as Smith later essentially acknowledges the American public is correct in dismissing the research arm of political science, “given the limited public impact of most late twentieth-century and twenty-first century American political science scholarship” (Smith, 2015, p. 374). It’s not clear which Smith wants more: public impact or radical reform agendas? The two are unlikely to be found in the same book on the same bestseller list. Rogers Smith does not offer prescriptions alongside his diagnosis – the disease is too far along for remission: “The trends I have sketched,” he writes, “are largely not reversible, nor would many types of reversal be desirable.” 

Putman (2003) does not appear concerned about appealing to one partisan enclave or another. Rather, he sees a professional responsibility to “engage with our fellow citizens in deliberation about their political concerns, broadly defined.” Like Smith, Keohane sees a need for a recommitment to teaching of students, which will in turn strengthen the research goals of the field and its scholars. Keohane (2009, p. 363) sees most value in teaching students to question the taken-for-granted, to ask simple questions and “on the critical imagination, conceptual boldness, and intellectual rigor of successive cohorts of newly trained scientists.” 

Big Data, Big Problems

Reviewing an entire field and categorizing it along two-dimensional scales – tyranny or legitimate coercion? Teaching or research? Quantitative or qualitative? Rigor or applicability? – is at once too broad a demand and too simple a remedy. All of them are “right,” and none of the authors are “wrong”- at least not for a retrospective look at political science scholarship. Their collected decades of thinking about political science scholarship has taken place in a context where the giants who came before, and their contributions, are intensely valued. Yet, of the four authors here, perhaps only Keohane appears comfortable with the knowledge that what comes next – artificial intelligence – could render “what came before” as interesting historical context. 

In a recent research note, Kriegeskorte (2015, p. 2) reviews how quickly advances in neural networks are approaching human-level performance in brain and vision tasks: “Now deep neural networks provide a framework for engaging complex cognitive tasks and predicting both brain and behavioural responses.”  Kriegeskorte continues (2015, p.1): “With human-level performance no longer out of reach, we are entering an exciting new era, in which we will be able to build biologically faithful feedforward and recurrent computational models of how biological brains perform high-level feats of intelligence.” While other fields are excitedly moving forward with incorporating machine learning into their scholarship, what of political science? While not comprehensive, a simple Google Scholar query for “political science and neural network” and “political science and artificial intelligence” had no relevant results. 

Gary King is clear in his assessment (Lohr, 2012, p. 1): “It’s a revolution. We’re really just getting underway. But the march of quantification, made possible by enormous new sources of data, will sweep through academia, business, and government.” As political science scholars are well aware, revolutions tend to have some bloodletting, at least metaphorically, and our field, concerned with both government and academia, should not consider itself “safe” from such a revolution. Within the next several decades, deep learning neural networks and artificial intelligence will begin to replace two of the four pillars of what Keohane (2008) identifies as the core functions of political science: descriptive inference and causal inference. Retooling political science through old ontological debates will not answer the question faced by political scientists in the coming years: what are we for? To reframe Smith (2015, pp. 366-367): perhaps political science is not so exceptional. Political science is not alone in facing profound change in how scholarship will be done. Already, health care is reckoning with a future in which expert systems and deep-learning neural networks will soon outperform classical diagnosis in terms of recall and prediction. Does the field of political science fancy itself so different in scope and complexity that it can escape what other human-centered fields have already begun to cede core functions to? No matter ones’ preferred -tatives or -isms, it seems an unlikely outcome.  

In recent years Russian President Vladimir Putin predicted (emphasis supplied) “artificial intelligence is the future, not only for Russia, but for all humankind…It comes with colossal opportunities, but also threats that are difficult to predict. Whoever becomes the leader in this sphere will become the ruler of the world” (Hern, 2017). In the same week, and in response to Putin, technological wunderkind Elon Musk predicted the next world war will be started by competing artificial intelligence networks controlled by states (Hern, 2017). 

One would be hard pressed to understate the salience for political science or the threat to democratic values presented by a government (whether autocratic, tyrannical, or democratic) ceding strategic and tactical control of their military to an artificial intelligence. International power, regime studies, war theory, genocide, administrative evil, and ethics – nothing is off limits. Jane Mansbridge (2014), in attempting to locate paths to legitimate coercion, may be most well positioned to contribute to a future political science in an era where political actors have been replaced by political artificialities unconnected to human experience. Political science, with its history of studying human conflict writ large, can help guide the ethical considerations and responses of inhuman, or at least non-human, politics of the future.  Sarah Jordan (2014, p. 375), writing in Public Integrity, predicted “The future of public administration lies in its ethical knowledge work and expertise.” Sounding similarly alarmed, Donald Menzel, (2015, p. 363) noted that the ethical implications of government use of information technology – which is the basest level of technological complications on the horizon – is “one sketchily researched area” deserving more scholarship (Menzel, p. 363).

If machinated intelligence comes to dominate descriptive and causal inference, as appears likely, what political scientists should get better at is the “puzzling” (Keohane, 2008, p. 360) – observing what does not fit with theory - and “conceptualizing,” particularly as it relates to translating scholarly arcana to ordinary usage. We must become better data scientists and qualitative researchers and puzzlers and conceptualizers – all at once. Alongside those, society will be in need of intense scholarly focus on instilling our ethical knowledge work into the machine learning, neural networks, and artificial intelligences that are the future of both governance, and the study of governance. In her address to the Southern Political Association, Mary Guy (2003, p. 651) stated: “The field is served best when we remain self-consciously aware of the implications of our models and the information they produce.” Two decades hence, the best human political scientists will be those that who are able to be aware of the human and ethical implications of models and information produced by inhuman “minds.” The big question, as seen by Sara Jordan (2014, p. 276): How ought government protect the public against the use of their data but also protect the public through the use of their data?” The field of political science should be prepared to begin answering that question – that is what we will be for. 

## Partisan Hearts & Minds: Political Parties and the Social Identities of Voters

In Partisan Hearts and Minds (Green, Palmquist, & Schickler, 2002) partisanship is defined as a type of social identification: a “psychological process of self-categorization and group evaluation” (p. 13). People identify as a Democrat or a Republican in the same way they identify as belonging to a religious denomination or ethnic group. Instead of the “warmth” indicators used by many partisanship researchers in surveys, Green, Palmquist and Schickler argue that “people ask themselves two questions: What kinds of social groups come to mind as I think about Democrats, Republicans, and Independents? What assemblage of groups (if any) best describe me?” (2002, p. 8). This a rejection of the rational choice version of partisanship which dominated the academic view of partisanship at the time (Green & Shapiro, 1996).

How a person answers this question to themselves produces a very stable partisanship. The authors see partisanship as a relatively non-dynamic phenomenon. That is, rather than elections producing a great deal of party choice change among voters, most people already have a partisan identification (at least to themselves) and that identification is rather unlikely to change in the short-term span of an election cycle. Elections are more of a cause for cheerleading one’s own team, and less a competition between two (or more) choices of individual politicians: “Elections are also forums for intergroup competition. Individuals who identify with these groups are drawn into this competition. Their interest and level of emotional engagement increase as they embrace the team as their own. Although not irresistible, the desire to see one’s team prevail powerfully influences the probability of casting a vote for the candidate of one’s party” (Green et al., 2002, p. 202).

This willingness to see past rivaling individual politicians and engage with politics as social identification is why partisanship matters – it matters because it affects electoral politics. The authors seek to provide empirical support for their theory not just in American politics through the case of the 2000 presidential election, but in comparative international contexts as well, with evidence from the United Kingdom, Canada, and Germany.  This is an important claim from the authors, who position their theory of partisanship not as an American phenomenon, but a human one.

### Methodological Contribution

The book makes a clear methodological contribution as well, despite containing relatively sparse statistical analysis than other texts in the genre. Because of their underlying critique that party identification has been poorly measured in much of the partisanship research, the authors present and defend a way of accounting for measurement error (p. 231-234). Once models of partisanship allow for measurement error, the authors argue, party identification is revealed as a very stable pattern over multiple decades.

While in this work they use mean-corrected panel analysis, this is just one example of how to correct for measurement error. Psychologists have long recognized that measurement error interferes with causal inference (Baltes & Nesselroade, 1979). Green, Palmquist, and Schickler make good use of this insight to address pooled measurement error, which still remains relatively unaddressed in much of political science research. The next steps needed can already be seen in modern techniques and rapid development still taking place in developmental psychology (Deboeck, Nicholson, Kouros, Little, & Garber, 2015), which uses derivatives of both inter- and intra-measurement error. Political science has not yet developed even the tools to collect the tools needed to build datasets capable of being tested in this way. Once (if) they are, however, further insights into the stability of partisanship over time, and its vulnerability to political events including elections, could be gleaned. As it is, the authors make a good case for including measurement error in partisanship studies, and more broadly political science.

## *Southern politics in state and nation* – VO Key

VO Key’s *Southern Politics in State and Nation* (1949) is a central text in American political studies, and the remains “the single most important work in understanding the role of the South in American politics” (Bateman, Katznelson, & Lapinski, 2015, p. 154). VO Key’s central insight that drives the analysis in the rest of the book is captured in his concept of “black belts.” These are the areas within Southern states with much higher proportions of black Americans. The dominance of black belt whites explains a tremendous amount of Southern politics, particularly the dominance of the Democratic party, which in many places effectively established one-party rule. Whites in the black belts have been the predominant shapers of politics in the South since even before the Civil War. Black belt whites successfully pushed for secession, despite opposition from many of the whites in non-black belt areas. West Virginia split from Virginia over this dispute. Though the secession of the South was defeated, the political power of whites in the black belt soon reasserted itself. In the 1890s a series of uprisings re-entrenched Southern political power in the hands of black belt whites.

To varying degrees, black belt whites sought to establish one-party rule in order to prevent partisan competition for the black vote and to present unified political resistance to attempts from the Federal government to remove the white supremacist policies of the South. Key argues that in those southern states where there is more partisan competition, such as Florida, North Carolina, Texas, and Tennessee, the competition is a direct result of having fewer and smaller black belts. Where multiple factions exist, due to small black belts, more progressive politics emerge. As a result of greater electoral competition, rare Republican victories at the state level occasionally emerged. The increased partisan threat served to keep Democrats more organized and cohesive. Party discipline was needed to win elections, and the party was motivated to work for redistributive policies that benefitted more than just black belt whites.

However, the cases of partisan competition are relatively limited in the South. In most states, one-party domination by Democrats was the rule. More than any other, Arkansas was the example of a pure one-party state, though the same general pattern could be found in most of the South. Extreme political conformity meant that no consistent factions arose, as political hopefuls were basing success on friends and neighbors in the areas they were lived. This reified the power of black belt whites, who in turn were motivated to rule through non-redistributive policy. After all, if winning elections means keeping your (white) neighbors happy, and politicians are at least strongly motivated to win re-election (Mayhew, 1974), then making sure those neighbors are well taken care of by the party machine is a rational strategy.

Key’s study of the South generated broader implications for political study of states and parties. His genius was in deriving more general political hypotheses from close study of Southern state cases. The primary insight is that the closer a state comes to pure one-party rule, the more multi-factionalism will result within those parties. In states with more (though not equal) partisan competition, both parties will become more cohesive. This begs the question: What is the difference between party and faction?

Key sees the differences as a continuum, from the most extreme one-party rule in Arkansas and South Carolina to states where the minority Republicans came close to establishing two-party rule, most identifiably in North Carolina. Key identifies at least five differential effects – things parties do that factions do not. First and primarily, when there is a unified and persistent minority party, the majority party rules more responsibly, redistributes benefits to compete for votes among blacks and minority party members, and is more disciplined. Fractured factional leaders do not do this. Second, parties can organize voters, and sustain those coalitions of voters around shared interests, whereas factions are too locally focused on merging interests with other outside factions. Thus, factional politicians end up having to rebuild any coalition around a new identity each election cycle, without continuity of effort or dedication. The third effect is that states with more than single-party rule, the minority party can sustain their opposition, whereas factional opposition cannot. Fourth, the sustained nature of parties means that leaders are vetted and experienced over time, whereas in factions, charisma becomes more critical as they attempt to build coalitions each cycle quickly. Fifth, corruption, nepotism, and political favoritism are more endemic to factional rule in one-party states because factional politicians must use “whatever means are available” (Key, 1949, p. 305) as they attempt to bribe friends and neighbors to stay loyal. In a cohesive party, leaders are still likely to rewards friends with a place in the unified party government, but again this a done in a more distributed manner. Also, because there is a unified opposition party with at least some governmental levers to pull, the risk of discovery of criminal behavior in the majority party serves to keep corruption relatively lower.  The differences between parties and factions show how complex results can spring from the relatively simple difference that parties have internal cohesion and discipline, while factions do not.

With some limited exceptions, the South was a political body comprised of factions. Democrat rule was the rule. Whereas Key generally expects national politics to seep into local politics, if not dominate them, the South remained politically isolated from the nation. The South’s atomized factionalism meant the governments were generally unable to “provide the political leadership necessary to cope well with the governmental problems of the South” (Key, 1949, p. 310). The South was primarily organized politically around the interests of black belt whites, who were primarily interested in preserving their power at the expense of the blacks who far outnumbered them. This was a political division that did not hold at the federal level and reinforced the political isolation of the South, which remained insular and overly focused on localism and racist traditions.
 

## More? Stay tuned!



 








